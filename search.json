[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Researcher’s Guide to N3C",
    "section": "",
    "text": "Welcome\nWelcome to The Researcher’s Guide to N3C: A National Resource for Analyzing Real-World Health Data. Here you will find guidance, information, and pointers to resources for conducting research with the National COVID Cohort Collaborative (N3C).\nDuring a time of isolation and unanswered questions in an emerging pandemic, N3C was born of an urgent need to save lives. N3C was created “to acquire and harmonize data across organizations and [provide] a secure data environment to enable transparent and reproducible collaborative research.” 1 Dozens of technical, clinical, and regulatory experts from government agencies, healthcare organizations, academic institutions, and the private sector worked together to build the nation’s largest and most diverse set of de-identified health records to date. Today, thousands of researchers have actively engaged with N3C resulting in hundreds of scholarly products informing public policy and patient care.\nN3C emphasizes community and team science values which are embodied by the structure of the organization and reflected in the following chapters. This guide originated as a project of the Education and Training Domain Team, one of the many self-organizing teams within N3C. An editorial committee was formed to engage with principal investigators, clinician-scientists, informaticians, data scientists, and others to condense the collective knowledge amassed in N3C. Chapters are authored and peer reviewed by a diverse group of contributors with both broad and deep domain expertise. This has been a work of heart and soul, designed to make this ground-breaking work accessible without obscuring its complexity.\nN3C is continuously evolving, and we expect this book will as well. We invite you to join us in continually expanding and improving this guide, but most importantly, we invite you to get involved in one of the many avenues for impactful research that N3C provides.\nSincerely,\nThe G2N3C Editorial Committee"
  },
  {
    "objectID": "index.html#sec-welcome-contributors",
    "href": "index.html#sec-welcome-contributors",
    "title": "The Researcher’s Guide to N3C",
    "section": "Contributors",
    "text": "Contributors\nIndividual chapter lead and contributor affiliations are also provided in each chapter.\nEditorial Committee Shawn T. O’Neil, Will Beasley, Johanna Loomba, Sharon Patrick, Kenneth J. Wilkins, Karen M. Crowley\nChapter 1 – Introduction Karen M. Crowley, Shawn T. O’Neil\nChapter 2 – A Research Story Will Beasley, A. Jerrod Anzalone, Sharon Patrick\nChapter 3 – Data Lifecycle - From Patients to N3C Researchers Stephanie Hong, Bryan Laraway\nChapter 4 – Governance, Leadership, and Operations Structures Christine Suver, Johanna Loomba\nChapter 5 – Onboarding, Enclave Access, N3C Team Science Sharon Patrick, Jonathan F. Emery, Suzanne McCahan, Mary Helen Mays\nChapter 6 – Getting & Managing Data Access Shawn T. O’Neil, Mariam Deacy\nChapter 7 – Understanding the Data Harold P. Lehmann, Lisa Eskanazi, Sigfried Gold, Shawn T. O’Neil, Thomas Richards, Kristin Kostka\nChapter 8 – Introducing Enclave Analysis Tools Amy Olex, Andrea G. Zhou, Johanna Loomba, Evan French, Shawn T. O’Neil, Steven G. Johnson\nChapter 9 – Best Practices for Research Lifecycle Harold P. Lehmann, Hytham Sidky, Jimmy Phuong, Kate Bradwell, Kenneth J. Wilkins, Andrea G. Zhou, David Sahner\nChapter 10 – Publishing and Sharing Your Work Julie A. McMurry, Jeremy R. Harper, Christine Suver, Carolyn T. Bramante, Mary K. Emmett, Amit K. Saha, Farrukh M. Koraishy, A. Jerrod Anzalone, Shawn T. O’Neil\nChapter 11 – Special Topic: Help and Support Shawn T. O’Neil, Saad Ljazouli, Johanna Loomba, Lisa Eskanazi"
  },
  {
    "objectID": "index.html#sec-welcome-contribute",
    "href": "index.html#sec-welcome-contribute",
    "title": "The Researcher’s Guide to N3C",
    "section": "How to Contribute",
    "text": "How to Contribute\nWe welcome suggestions, edits, and larger contributions to this guide. This book is typeset in Markdown with the Quarto package for R and hosted on GitHub. For errors or requests, please submit an Issue to the Issues tracker. To make larger or direct contributions, please make a pull request using the standard GitHub workflow. If you would like to contribute but are unfamiliar with any of these technologies, please contact any of the editorial committee members or submit an Issue to request help."
  },
  {
    "objectID": "index.html#sec-welcome-cite",
    "href": "index.html#sec-welcome-cite",
    "title": "The Researcher’s Guide to N3C",
    "section": "How to Cite this Work",
    "text": "How to Cite this Work\nO’Neil ST, Beasley W, Loomba J, Patrick S, Wilkins KJ, Crowley KM. (2023). The Researcher’s Guide to N3C: A National Resource for Analyzing Real-World Health Data. DOI: 10.5281/zenodo.7749367"
  },
  {
    "objectID": "index.html#sec-welcome-licensing",
    "href": "index.html#sec-welcome-licensing",
    "title": "The Researcher’s Guide to N3C",
    "section": "Licensing",
    "text": "Licensing\nThis book is licensed under the Creative Commons Attribution-NoDerivatives 4.0. Individual chapters are as well, unless otherwise noted."
  },
  {
    "objectID": "index.html#sec-welcome-funding",
    "href": "index.html#sec-welcome-funding",
    "title": "The Researcher’s Guide to N3C",
    "section": "Funding",
    "text": "Funding\nThis content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the N3C program.\nAny analyses herein were conducted using the NCATS N3C Data Enclave supported by NCATS U24 TR002306, Axel Informatics Subcontract Number: NCATS-P00438-B and made possible because of the patients whose data was contributed by partner organizations. We gratefully acknowledge the scientists who have contributed to the on-going development of this community resource (Haendel et al. 2020).\nFunding and support for individual authors is listed in Chapter 15.\n\n\n\n\nHaendel, Melissa A, Christopher G Chute, Tellen D Bennett, David A Eichmann, Justin Guinney, Warren A Kibbe, Philip R O Payne, et al. 2020. “The National COVID Cohort Collaborative (N3C): Rationale, design, infrastructure, and deployment.” Journal of the American Medical Informatics Association 28 (3): 427–43. https://doi.org/10.1093/jamia/ocaa196."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Researcher’s Guide to N3C",
    "section": "",
    "text": "See https://covid.cd2h.org/about.↩︎"
  },
  {
    "objectID": "chapters/intro.html#sec-intro-mission",
    "href": "chapters/intro.html#sec-intro-mission",
    "title": "1  Introduction",
    "section": "1.1 Mission",
    "text": "1.1 Mission\nThe National COVID Cohort Collaborative, or N3C, is an open-science community stewarded by the National Center for Data To Health (CD2H) and the NIH National Center for Advancing Translational Sciences (NCATS), with significant contributions from many partners including the Clinical and Translational Science Awards (CTSA) program, Centers for Translational Research (CTRs), and thousands of researchers from hundreds of participating institutions in the US and abroad.\nFaced with the COVID-19 pandemic, the issue addressed by N3C is clear and direct: the US has no centralized data repository for health records and related information, hindering the response of the scientific community.1 Although healthcare providers are mandated by law to utilize electronic health records (EHR), little guidance coordinates how or exactly what information to collect and store. Commercial data-collection suites (e.g. Epic) are widely used in clinical settings, and controlled vocabularies (e.g. ICD10 and SNOMED) provide standards for representing medical information, but there are many such standards in use and software is highly configurable to the needs of individual organizations. As a result, databases of EHR information across the US are largely non-interoperable, presenting challenges to researchers hoping to use this vast national store of information in practice."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-cdm",
    "href": "chapters/intro.html#sec-intro-cdm",
    "title": "1  Introduction",
    "section": "1.2 Common Data Models and N3C",
    "text": "1.2 Common Data Models and N3C\nIn recent years, the common solution to these issues have been the creation of Common Data Models (CDMs). A Common Data Model is an agreed-upon structure and format for databases containing clinical information, into which diverse organizational EHR databases can be standardized for research purposes. Even so, healthcare organizations are reluctant to share their data directly, because the risks associated with data breaches for protected health information (PHI) defined by Health Information Portability and Accountability Act (HIPAA) are high. As a result, several groups of healthcare and research organizations have formed federated research networks, where each organization in a network translates some subset of its data into an agreed-upon common data model, and affiliated researchers can then write queries intended for data in that format. Examples of such federated networks include PCORNet, i2b2, and OHDSI, each of which utilize their own common data model. In a federated model, data queries are generated by researchers, but executed locally by the data owners and only summarized or specifically requested results are sent back to researchers. This ensures protection for the patient data (which never leaves the boundaries of any individual organization), but prevents exploratory data investigation and other techniques (like many machine-learning methods) which require direct access to the totality of the data.\n\n\n\nFigure 1.1: A visual representation of disparate, non-compatible EHR databases across the United States.\n\n\nDriven by the imperative of addressing COVID-19, in concert with the use of a FedRAMP-certified cloud-based analysis ecosystem we call the N3C Data Enclave,2 N3C is partnering with EHR data providers across the nation to collect billions of EHR data points for millions of patients with and without COVID-19 in a single, secure, accessible database for research use. In this centralized model, researchers have direct access to the entirety of the data to support complex analyses including AI and other machine learning techniques. N3C simultaneously moderates controlled access to these data by research teams from across the country (and beyond), including from private companies, community colleges, universities, medical schools, and government entities. While data cannot be exported from the Enclave, published results can be after an export review (see Chapter 10).\nLike federated research networks, N3C also uses a common data model, known as OMOP, chosen for its strong community support and open nature, support of scientific use cases, and availability of tools for translating and working with data (Chapter 7 and Chapter 8 discuss OMOP in more detail).3 To rapidly collect data from around the country, N3C leverages the existing work data owners have already done to convert their organization-unique data to one of a handful of N3C-supported “source” common data models: PCORNet, i2b2, TriNetX, ACT, and OMOP. A potential data partner with data in PCORNet format, for example, will locally run a set of N3C-generated “PCORNet to OMOP” translation scripts prior to transferring the result to N3C via a secure channel. The process of coalescing multiple such data payloads into a unified whole is known as harmonization, and is a complex task even after everything has been mapped to OMOP initially. Two overlapping teams of EHR data experts participate in this process: one works closely with data partners to make it as easy as possible to contribute data to N3C, and another handles the post-ingestion harmonization and comprehensive quality checks of the incoming data."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-enclave",
    "href": "chapters/intro.html#sec-intro-enclave",
    "title": "1  Introduction",
    "section": "1.3 The N3C “Enclave” and Data Access",
    "text": "1.3 The N3C “Enclave” and Data Access\nOnce harmonized and stored in the secure Enclave, the data are made available via a web-based interface to research teams using common tools such as SQL, Python, and R, as well as a number of code-light graphical user interfaces.\n\n\n\nFigure 1.2: A visual representation of N3C’s data harmonization from source-CDM model, and team-based access to these data in a secure cloud-based enclave.\n\n\nMere access to the Enclave, however, doesn’t automatically provide access to any of the protected data itself (although we do make other, publicly-available datasets available with minimal restriction for practice and learning purposes). Multiple “levels” of the data are available with different anonymization techniques applied, facilitating “just enough” access to research teams depending on their needs and ability to access protected health information. Accessing the most secure level, for example, requires obtaining approval by an Institutional Review Board (IRB) who validates the appropriateness of human subjects research, while the lowest level is heavily anonymized and accessible by private individuals (citizen scientists) with only certain legal and training requirements.\nBecause effective analysis of EHR data requires a diverse set of skills–especially clinical and data science/statistical expertise–N3C provides organizational structures and resources to rapidly create and support multidisciplinary research teams, many of which are geographically diverse as well. As of February 2023, dozens of these “Domain Teams” have supported over 400 research projects, contributed to by over 3,300 researchers hailing from 350+ different institutions and organizations. Over seventy data partners provide EHR data for 17 million patients (a third of whom have had COVID-19), representing 10.5 billion lab results, 3.5 billion medication records, 2 billion clinical observations, and 1 billion clinical visits. For up-to-date information on these numbers and more, visit our dashboard at https://covid.cd2h.org/dashboard.\n\n\n\nFigure 1.3: Summary statistics for N3C patients as of Aug, 2022. Confirmed COVID-19 patients are those with a known positive PCR or Antigen lab test, possible patients are those with likely symptomatology."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-next",
    "href": "chapters/intro.html#sec-intro-next",
    "title": "1  Introduction",
    "section": "1.4 Where to Go Next",
    "text": "1.4 Where to Go Next\n\n1.4.1 For Researchers\nSo, why should you get involved with N3C? First and foremost, N3C provides an opportunity to participate in impactful team science. Investigators with expertise in multiple domains come together across organizational boundaries to understand and address the impact of COVID-19 across the United States. Dozens of N3C-supported publications span the gamut of research: Pfaff, Girvin, Bennett, et al. (2022) applied machine-learning methods to understand important predictive factors for Long-COVID, and Sharafeldin et al. (2021) identified demographic and clinical factors contributing to mortality risk in cancer patients. Mehta et al. (2021) studied the use of hydroxychloroquine, remdesivir, and dexamethasone over time at multiple sites, revealing how treatment guidelines evolve in response to updated information over time. Sun et al. (2022) studied breakthrough infections after vaccination, Yang et al. (2021) evaluated COVID-19 outcomes in HIV patients, Reese et al. (2023) clustered patients to reveal sub-types of Long-COVID, and Anzalone et al. (2023) found higher hospitalization and mortality in rural communities. These are but a small sample of work produced by researchers participating in N3C.\nThis range of work is only possible by the diversity of interests and expertise researchers bring. Practicing clinicians, biostatisticians, machine-learning researchers, and others collaborate on projects inside the secure data enclave. N3C supports team science in a variety of ways. Domain Teams, for example, serve to connect groups with similar interests for peer support, research coordination, and collaboration building. While most Domain Teams are clinically oriented (e.g. the Pregnancy Domain Team), others are more general (e.g. the Machine Learning Domain Team). Chapter 5 covers Domain Teams in more detail. N3C provides a number of training and support venues, including regular office hours, training modules, and of course this book. See Chapter 11 for more information on these topics.\nOf course, N3C brings significant value as one of the largest databases of de-identified patient records in the US, covering drug prescriptions, conditions, procedures, and more, each associated with a corresponding visit and other information. Data are extensively quality-checked and harmonized for consistency to the OMOP common data model (see Chapter 3), which supports sophisticated filtering and querying (Chapters 7 and 8). Other data are available as well, including publicly-available datasets (e.g. from the US Census) and, for some patients, additional mortality, viral variant, or billing data from non-EHR sources (Chapter 3).\nBig data is of little value without powerful analysis tools. Fortunately, N3C’s enclave supports analyses with SQL, Python, and R, including thousands of popular libraries for the latter two. Backed by the high-performance distributed-computing framework Apache Spark, researchers can include billions of rows of data in a single analysis. Graphical tools are also available for those without coding expertise, and the N3C community generates reusable code and datasets to pave the way for others. All of these tools are cloud-hosted, so researchers only need to bring a web browser. For information on these topics, see Chapter 8.\nFinally, N3C has worked hard to make these resources secure and accessible. Enclave access requires coverage by a Data Use Agreement, which hundreds of institutions across the US and beyond have signed on behalf of all their employees and students. Data itself is accessed via a guided Data Use Request form in the enclave, and researchers can invite others to their projects at any time. See Chapters 5 and 6 for details.\n\n\n1.4.2 For Institutions\nThere are two primary ways that institutions can participate in N3C: (1) by signing an institutional Data Use Agreement; and (2) by contributing data.\nSigning an institutional Data Use Agreement provides access to N3C’s data enclave to all employees and students at your institution. Hundreds of institutions have done so, connecting their research community to a vast network of data, tools, and expertise. For more information about Data Use Agreements, see Chapter 5.\nInstitutions that contribute data to N3C gain more than recognition–contributing sites get early access to new features and pilot programs. More importantly, N3C provides data partners feedback on their data quality. While contributing sites implement their own data quality checks, N3C has discovered a number of issues that are only apparent in an environment with multiple organizations’ data (Pfaff, Girvin, Gabriel, et al., 2022).\nFinally, N3C’s governance structures may be of interest to other organizations embarking on large-scale, team-science efforts. Chapter 4 introduces these perspectives.\nRegardless of how you think your institution can work with N3C, getting started is as simple as reaching out–either by contacting N3C leadership, submitting a ticket to our enclave-external help desk, or just stopping by office hours. More information on these latter two options may be found in Chapter 11.\n\n\n\n\nAnzalone, A. J., Horswell, R., Hendricks, B. M., Chu, S., Hillegass, W. B., Beasley, W. H., Harper, J. R., Kimble, W., Rosen, C. J., Miele, L., et al. (2023). Higher hospitalization and mortality rates among SARS-CoV-2-infected persons in rural america. The Journal of Rural Health, 39(1), 39–54. https://doi.org/10.1111/jrh.12689\n\n\nMehta, H. B., An, H., Andersen, K. M., Mansour, O., Madhira, V., Rashidi, E. S., Bates, B., Setoguchi, S., Joseph, C., Kocis, P. T., Moffitt, R., Bennett, T. D., Chute, C. G., Garibaldi, B. T., & Caleb Alexander, G. (2021). Use of hydroxychloroquine, remdesivir, and dexamethasone among adults hospitalized with covid-19 in the united states: A retrospective cohort study. Annals of Internal Medicine, 174(10), 1395–1403. https://doi.org/10.7326/M21-0857\n\n\nPfaff, E. R., Girvin, A. T., Bennett, T. D., Bhatia, A., Brooks, I. M., Deer, R. R., Dekermanjian, J. P., Jolley, S. E., Kahn, M. G., Kostka, K., McMurry, J. A., Moffitt, R., Walden, A., Chute, C. G., Haendel, M. A., Bramante, C., Dorr, D., Morris, M., Parker, A. M., … Niehaus, E. (2022). Identifying who has long COVID in the USA: A machine learning approach using N3C data. Lancet Digit Health, 4(7), e532–e541. https://doi.org/10.1016/S2589-7500(22)00048-6\n\n\nPfaff, E. R., Girvin, A. T., Gabriel, D. L., Kostka, K., Morris, M., Palchuk, M. B., Lehmann, H. P., Amor, B., Bissell, M., Bradwell, K. R., et al. (2022). Synergies between centralized and federated approaches to data quality: A report from the national COVID cohort collaborative. Journal of the American Medical Informatics Association, 29(4), 609–618. https://doi.org/10.1093/jamia/ocab217\n\n\nReese, J. T., Blau, H., Casiraghi, E., Bergquist, T., Loomba, J. J., Callahan, T. J., Laraway, B., Antonescu, C., Coleman, B., Gargano, M., et al. (2023). Generalisable long COVID subtypes: Findings from the NIH N3C and RECOVER programmes. EBioMedicine, 87. https://doi.org/10.1016/j.ebiom.2022.104413\n\n\nSharafeldin, N., Bates, B., Song, Q., Madhira, V., Yan, Y., Dong, S., Lee, E., Kuhrt, N., Shao, Y. R., Liu, F., Bergquist, T., Guinney, J., Su, J., & Topaloglu, U. (2021). Outcomes of COVID-19 in patients with cancer: Report from the national COVID cohort collaborative (N3C). Journal of Clinical Oncology, 39(20), 2232–2246. https://doi.org/10.1200/JCO.21.01074\n\n\nSun, J., Zheng, Q., Madhira, V., Olex, A. L., Anzalone, A. J., Vinson, A., Singh, J. A., French, E., Abraham, A. G., Mathew, J., Safdar, N., Agarwal, G., Fitzgerald, K. C., Singh, N., Topaloglu, U., Chute, C. G., Mannon, R. B., Kirk, G. D., & Patel, R. C. (2022). Association between immune dysfunction and COVID-19 breakthrough infection after SARS-CoV-2 vaccination in the US. Archives of Internal Medicine (Chicago, Ill. : 1908), 182(2), 153–162. https://doi.org/10.1001/jamainternmed.2021.7024\n\n\nYang, X., Sun, J., Patel, R. C., Zhang, J., Guo, S., Zheng, Q., Olex, A. L., Olatosi, B., Weissman, S. B., Islam, J. Y., et al. (2021). Associations between HIV infection and clinical spectrum of COVID-19: A population level analysis based on US national COVID cohort collaborative (N3C) data. The Lancet HIV, 8(11), 690–700. https://doi.org/10.1016/S2352-3018(21)00239-3"
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "These articles provide good overviews of N3C and the surrounding landscape: It took a pandemic, but the US finally has (some) centralized medical data (MIT Technology Review), and The ambitious effort to piece together America’s fragmented health data (The Verge)↩︎\nThe Federal Risk and Authorization Management Program (FedRAMP) is a rigorous, standardized certification program with an emphasis on security and information protection. The Enclave is an installation of Palantir Technologies’ Foundry platform, a FedRAMP-certified data analytics suite.↩︎\nOMOP was originally developed by its namesake, the Observational Medical Outcomes Partnership, but is now stewarded by the Observational Health Data Sciences and Informatics (OHDSI, pronounced “odyssey”) program, an international group of researchers and clinicians. For complete information about OHDSI and OMOP, see the Book of OHDSI.↩︎"
  },
  {
    "objectID": "chapters/story.html#sec-story-onboarding",
    "href": "chapters/story.html#sec-story-onboarding",
    "title": "2  A Research Story",
    "section": "2.1 Onboarding",
    "text": "2.1 Onboarding\nAs is typical for access to patient data outside your own institution, institutional participation in N3C requires legal be put in place. Even with strong institutional support, execution of the university’s Data Use Agreement with the NIH may take several months in legal and administrative channels. Yet after clearing that first (tall) hurdle for your site, each specific project takes only a week or two to be processed by the N3C staff. That’s a remarkably short time considering the scale of available data. It’s likely quicker than initiating a project based on a single EMR from your site –much quicker than EMRs from 70+ sites.\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nThe next afternoon you are chatting with your institution’s Navigator.1 She organized the local N3C presentation and invited any interested attendees to contact her.\n\n\n\n\n\n\n\n\nHover over a footnote to see the popup, without jumping to the bottom of the page.\n\n\n\n\nNavigator: I’m glad you think the N3C might help your research. As I wrote in this morning’s email, the agreement between the university and the NIH was established last year, so don’t worry about that.2 There are two remaining steps. First, complete your personal paperwork.3 Second, submit a DUR tailored to your hypotheses.4\nInvestigator: Remind me what a DUR is?\nN: A Data Use Request describes your upcoming project. Once an NIH committee approves your proposal, your project’s code and data are protected in this workspace allotted on the NIH cloud.5 Everyone on your project uses this dedicated workspace too. But they don’t have to submit additional DURs –you grant them permission to join yours.6\nI: Umm, I think I got it.\nN: It will make sense once you get it into it. Skim the example DUR proposals I’m sending now. Then start filling out this online form. Get as far as you can, and then I’ll help with the rest. If there’s something I don’t know, I’ll ask a friend. The DUR application process will take about an hour. Then the proposal will likely be approved within a week or two. In the meantime, we can talk about potential collaborators.\nI: Is there anything else I need to know?\nN: The only other consideration at this point is whether your want your study to use Level 2 or Level 3 data. Level 2 data is fully de-identified, meaning all the dates are shifted by up to 180 days. Also, all location identifiers are obfuscated to allow potential reidentification, so you won’t have access to 5-digit ZIP Codes or other granular location measures. If you need either of those, you have to submit Level 3 data, which is a limited data set containing these data elements. Some institutions requires an IRB protocol to submit a Level 3 DUR, so keep that in mind. Our institutional IRB has provided a letter covering most Level 3 DURs in N3C, so I can provide that to you.\nI: Ok, that’s very helpful. I may need real dates as I anticipate differences in treatment deliveries based on real-world availability. Am I stuck with Level 2 data if I start there?\nN: No. If needed, you can request a workspace to be updated from Level 2 to Level 3 and receive IRB approval.\nI: That’s great. I think we’ll start with Level 2 and go from there. I’ll work on the research protocol.\n\nAfter some thought, the investigator puts together a research protocol and submits a DUR in N3C. They submit the DUR, shown in Figure 2.2.\n\n\n\nFigure 2.2: Submitted Data Use Request"
  },
  {
    "objectID": "chapters/story.html#sec-story-team",
    "href": "chapters/story.html#sec-story-team",
    "title": "2  A Research Story",
    "section": "2.2 Team building & collaborating",
    "text": "2.2 Team building & collaborating\nThe next step is to build a team to leverage retrospective medical records. Like most contemporary research teams, heterogenous skills are important. Ideally a team has at least, as shown in Figure 2.3, members who can fill the roles below:\n\na navigator who has learned the administrative and IRB requirements and can facilitate the investigation,\na subject matter expert (SME) who has clinical experience with the disease of interest and can inform decisions with EHR variables,\na statistician or data scientist who understands the limitations of observational collection and can model retrospective data,\na logic liaison or informaticist who understands the challenges of EHRs and can extract and transform information (also known as a data engineer),\na data liaison who has expertise in medical terminology to help develop concept sets or identify previously validated concept sets, and\na principal investigator who knows the literature and can testable hypotheses and write the manuscript.\n\n\n\n\nFigure 2.3: Typical N3C Team Composition\n\n\nN3C teams have some differences from conventional research teams at single sites. Some trends we have noticed are:\n\nMost N3C teams have researchers from multiple institutions. In the experience of the authors and editors, this encourages more diverse opinions and more willingness to express constructive criticism. Researchers from a single institution/lab are sometimes more reluctant to generate contrary views.\nThe role of the navigator is often the most important member of a successful team. Your local investigations are likely guided by someone with years of experience with institutional safeguards and the personnel who can help when something stalls. N3C is bigger and younger than your site’s EHR research team, so an N3C project will benefit when guided by a bright, patient, and persistent navigator.\n\nIf your team needs someone, consider asking a relevant domain team for help identifying and approaching potential collaborators. Note that community-wide data and logic liaisons are available for consultation during regular office hours.\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nRecruiting your crew…"
  },
  {
    "objectID": "chapters/story.html#research-teams-first-meeting",
    "href": "chapters/story.html#research-teams-first-meeting",
    "title": "2  A Research Story",
    "section": "2.3 Research Team’s First Meeting",
    "text": "2.3 Research Team’s First Meeting\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nThree weeks later…\nOnce the team is assembled, the first discussion is usually a variation of this exchange, with the goal to move toward competition of a research protocol:\n\n\n\nInvestigator: Welcome everyone. We’d like to know if Drug A or Drug B is associated with better outcomes.\nStatistician: No problem. I can longitudinally model the type and amount of each medication received by each patient, relative to their intake date.\nLogic Liaison: Hmmm. I’m happy to produce a dataset with the dose and frequency columns7, but you may not find it useful. Those two columns are sparsely populated and they look inconsistent across sites.8\nI: Bummer. Then what’s realistic or feasible?\nSubject Matter Expert: Maybe this simplifies the picture… In my clinical experience, a patient rarely switches between Drugs A & B. Based on the initial presentation, their provider will pick A or B, and complete the regimen unless there’s an adverse event.\nSt: In that case, should my initial model have three levels for treatment: A, B, and A+B?\nI: Probably. In the N3C database, can someone tell me how many patients get both during the same visit?\nLL: I’m already logged into the Enclave9. Give me 2 minutes to add this drug to our templated fact tables which show same day events and also summarize events at the patient level.10\nI: Oh my goodness, is that your cat? What a cutie!11\nLL after a few minutes: Ok, I got it. [Unmutes himself.] Ok, I got it. 40% of patients are Drug A only, 52% are Drug B only, while 8% have at least one administration of both Drug A & B in the same visit.\nSME: Weird. 8% is a lot more than I expected. I was thinking around 1%.\nLL: Hmm, let me check. Give me another minute.12\nLL after a few minutes: I see what you mean. It looks like the bulk of the combo patients were admitted in the spring of 2020. After Jan 2021, only 3% of patients have both Drug A & B.\nSt: I was already planning to model the phase of the pandemic. I’ll test if there’s a significant interaction between time and treatment.\nI: I like that as a starting point. Regarding the question about dose and frequency… For now let’s assume the providers were following the current dosing guidelines. Therefore the dose and frequency variables can be dropped from the analyses.\nSt: Phew. I didn’t want to admit this. But I skimmed the dosing guidelines you emailed yesterday. It looked complicated. I wasn’t sure if I could appropriately incorporate those variables in the model.\nI: Well, that’s everything I wanted to cover today. See you in two weeks. Wait. I can’t believe I forgot. Sorry -our Navigator is sick this week and I’m almost worthless in her absence. Is everyone still on the call? For our secondary hypothesis, we want everything to connect to a patient’s diagnoses. …before, during, and after their covid hospitalization.\nLL: Bad news. This is kinda like the dose and frequency situation a few minutes ago. The structure of the OMOP diagnosis table theoretically can connect a patient’s diagnoses across different locations. But the quality of the historical records really depends on the site. Some places like Rhode Island leverage their state’s HIE13 to populate their N3C dataset. However other places are not as well connected. If a patient doesn’t have diagnosis records, it’s tough to determine if they are healthy, or if their primary care provider uses a siloed EMR.14\nI: Ugh. Good point.\nLL: But I’ve got good news. All the N3C contributors comprehensively capture all conditions diagnosed during the visit. Furthermore the diagnosis codes are standardized really well across sites. That’s because all the providers enter ICD codes into the EMR, which eventually can be cleanly mapped to OMOP’s standard concepts.15\nI: Well, that’s fine for this paper. Maybe our next manuscript will follow up with N3C’s death records.16\nSME: Sorry everybody, I have clinic this week, and they’re calling me. I need to drop.17\nSt: Can I go back and ask a question about medications? I see that Drug A has 15 different brand names. I don’t recognize half of them. How should I classify them?\nLL: It’s actually worse than that. Sorry I’m a downer today. Can you see my screen? Drug A has 15 brand names and 200 different RxNorm codes; each package is uniquely identified by the NIH’s NLM. SME and I started on a concept set Thursday. We’re operationalizing the drug classes by their RxNorm ingredient. There are five ingredients that are conceptualized as Drug A. A friend showed me how she used the OMOP tables in a different project.18 I’ll roll up the meds into the patient-level dataset. It will have one integer for the number of medication records tied to a Drug A ingredient and another integer for Drug B records. You’ll probably want to transform the two counts into two booleans.\nSt: And if I change my mind and decide to use the counts, then at least I’ll know.\nShoreleave: and knowing is half the battle."
  },
  {
    "objectID": "chapters/story.html#protocol-variables-definitions",
    "href": "chapters/story.html#protocol-variables-definitions",
    "title": "2  A Research Story",
    "section": "2.4 Protocol, variables, & definitions",
    "text": "2.4 Protocol, variables, & definitions\nDeveloping a research protocol is both familiar and vague for most research teams in the context of EHR studies. Most researchers have several years of graduate-level courses and real-world experience.\n\nTradeoffs are inevitable when selecting variables. Rarely will an investigator’s first choice be available.\nRetrospective medical records are extracted from a larger dataset. An investigation can use only a fraction of the terabytes in an EMR. Many decisions are involved to include only the relevant variables among the qualifying patients.\n\nWhile there are different approaches to developing a Research Protocol, the general steps involved include:\n\nDefine the research question: The first step is to clearly define the research question and the patient population of interest. This will guide the selection of relevant clinical concepts and data sources.\nDetermine the study design: The research team should choose an appropriate study design, such as a cohort study or case-control study, and determine the inclusion and exclusion criteria for the patient population.\nDevelop the analytic plan: The research team should prospectively specify the statistical methods to be used to analyze the data, including any adjustments for confounding variables and any sensitivity analyses.\nDocument the study protocol: The research team should document a detailed study protocol that includes all of the above information, as well as any other relevant information, such as the ethical and regulatory considerations, data privacy and security measures, and data sharing agreements. N3C contains a Protocol Pad that supports the development and documentation of detailed study protocols.\n\nAfter creating a research protocol, variable definitions are decided using OMOP and N3C tools and deployed in the Enclave."
  },
  {
    "objectID": "chapters/story.html#learning-and-using-omop-and-ohdsi-tooling",
    "href": "chapters/story.html#learning-and-using-omop-and-ohdsi-tooling",
    "title": "2  A Research Story",
    "section": "2.5 Learning and using OMOP and OHDSI Tooling",
    "text": "2.5 Learning and using OMOP and OHDSI Tooling\nThe Observational Health Data Sciences and Informatics (OHDSI) Observational Health Data Sciences and Informatics (OHDSI) program maintains the Observational Medical Outcomes Partnership (OMOP) common data model (CDM). OMOP was funded in 2008 by the US Food and Drug Administration, primarily for adverse drug events surveillance, but it has since expanded to become the de-facto global research CDM. Detecting a small signal requires a large datasets –larger than any single health care database (Sciences and Informatics 2019, chap. 1). Given its ubiquity and active research community, OMOP is well suited for N3C. OMOP has extensive tooling to support researchers, including two that directly support the curation of concept sets from the OHDSI program and one that is specific to N3C:\n\nAtlas provides a user-friendly interface for querying and analyzing data in the OMOP CDM. In the context of N3C, it supports browsing medical terminology and supports the development of concept sets.\nAthena is a centralized repository of standardized clinical vocabularies.\nThe N3C Concept Set Browser is an N3C specific tool that allows you to explore and modify existing concept sets as well as create new concept sets to fit your exact study needs.\n\nConcept sets, described in detail in Chapter 7 and Chapter 8, are the basic building blocks of an analytic dataset. The contain lists of medical codes, usually restricted to very specific definition or computable phenotype. In N3C, they are used to identify cohorts or exposures to answer a research question. They point to standardized vocabularies and clinically organized domains in the OMOP CDM (e.g., drug, condition, measurement).\nIn general, the overall process involved in developing concepts is as follows:\n\nDefine the research question: The first step is for the research team to clearly define the research question and population of interest, which will guide the selection of relevant clinical concepts.\nExplore the data: Using Atlas or the N3C Concept Set Browser, the team member filling as the data liaison will explore the data available or existing concept sets to identify relevant clinical concepts.\nRefine the concept set: The data liaison works with the SME to refine the concept set, which is usually an iterative process, to include clinically relevant and exclude clinically irrelevant concepts.\nValidate the concept set: Once the concept set has been defined, the SME and logic liaison validate the concept set and publish it in the N3C Concept Set Browser, which allows for reuse across the N3C community.\n\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAfter determining the need for a concept set defining anemia, which is a common symptom of scurvy, the subject matter expert and data liaison meet to refine the concept set.\n\n\n\nData Liaison: Good morning! I’ve started digging into existing concept sets for anemia and found one potential option.\nSubject Matter Expert: Good morning! That’s great. Let’s take a look at it.\nDL pulls up concept set for anemia: This is one that’s out there. It uses the parent SNOMED CT Code 271737000.\nSME: Hmm…this isn’t quite right. We need anemia caused by blood loss. A lot of these are unrelated.\nDL: Gotcha. Let’s take a look at the hierarchy and see if we can refine it. We can look at the descendants and go from there.\nThree hours later\nDL: Ok, great. I think we have a working example. I’m going to extract all of these into a spreadsheet. Please go through this one more time and then I’ll share with the group to review."
  },
  {
    "objectID": "chapters/story.html#creating-an-analysis-ready-dataset",
    "href": "chapters/story.html#creating-an-analysis-ready-dataset",
    "title": "2  A Research Story",
    "section": "2.6 Creating an analysis-ready dataset",
    "text": "2.6 Creating an analysis-ready dataset\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nOnce the project team has outlined the study protocol, key definitions, and timing of study elements, the next phase is the curation of a dataset for analysis. This is typically within the purview of the team informaticist or logic liaison. Let’s take a peak into the process…\n\n\nAfter clarifying all the data elements and study protocol, the next step is to curate an an analytic dataset. Depending on the study design, this can be organized in multiple ways. In general, analytic datasets are organized at either the person or encounter level. This means that you’ll have a single analytic dataset that contains one row with facts about either an individual patient or an individual encounter. This topic is covered in greater detail in the Tools and Best Practices chapters.\nUsing the previously defined protocol, the informaticist will use the [concept set browser] to identify or create concept sets of relevance to the study and begin to pull together the analytic dataset in a code workbook or code repository in the N3C Enclave. This will be a time-intensive process that, once completed, will result in a dataset that is ready for analysis, which can be handed off to the team’s statistician for analysis. N3C Logic Liaison templates are available to help build these datasets more quickly using defined pipelines where the custom concept sets can quickly be added as inputs. They also provide templates to assess the quality of the derived dataset which can provide insights into data density and completeness and help support decisions about dropping certain sites for the analysis (for example, if they appear to be systematically missing key variables).\n\n\n\nTable 2.1: Scurvy Analytic Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nSex\nRace Ethnicity\nQuarter of COVID dx\nSmoking Status\nScurvy pre-COVID\nMed A\nMed B\nDiabetes pre-COVID\nCOPD pre-COVID\nLiver Disease pre-COVID\nCancer pre-COVID\nRenal Disease pre-COVID\nCOVID Hospitalization\nECMO IMV\nTime to ECMO IMV\nDeath\nTime to Death\n\n\n\n\n27\nM\nNon-Hispanic White\n2022Q2\nCurrent or Former Smoker\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n28\n0\n28\n\n\n75\nF\nHispanic or Latinx\n2021Q1\nNo Documented History of Smoking\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n28\n0\n28\n\n\n54\nM\nNon-Hispanic Black or African American\n2020Q4\nNo Documented History of Smoking\n1\n1\n0\n1\n0\n0\n1\n0\n1\n0\n1\n1\n1\n\n\n34\nF\nNon-Hispanic White\n2023Q1\nCurrent or Former Smoker\n1\n0\n1\n1\n0\n1\n0\n0\n1\n1\n3\n1\n5\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…"
  },
  {
    "objectID": "chapters/story.html#analyses",
    "href": "chapters/story.html#analyses",
    "title": "2  A Research Story",
    "section": "2.7 Analyses",
    "text": "2.7 Analyses\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nOnce the analytic dataset is finalized, the handoff between the informaticist and statistician takes place. This often requires a discussion or ongoing dialogue to understand the data structure and definitions.\n\n\n\nNavigator: Welcome everyone. Today we’d like to review the analytic dataset and next steps in moving the analysis forward.\nInformaticist: Yep. I’ve made a new code workbook in your folder in the project workspace. Can you pull it up and we can take a look?\nStatistician: Sure. Did you send me the dataset? I don’t remember seeing an email.\nN: Remember, everything has to be done within the secure N3C platform. So you won’t be doing anything on your local machine. It all has to be done within the platform. Do you know how to get to the project workspace?\nS: …remind me.\nA few minutes later after a refresher.\nS: Ok, great. I think I get it. So it’s not that different from using R in RStudio as long as I follow the documentation you sent me. I think I can figure that out. What happens when I want to share results with the rest of the team? Can I send them out?\nN: You can add them to a report and share them within the N3C Enclave to those who have access to our project workspace. If you want to share results outside the platform, you can request any summary results, model output, or plots using the download review process. N3C leadership reviews any results before they can be extracted from the Enclave to ensure compliance with the N3C privacy and security expectations.\nS: Gotcha. I’ll reach out if I have any questions once I get to that point in the process. Thanks!\n\nFrom here, the statistician can work directly in N3C using R or Python for data analysis. After some work figuring out the platform, the statistician has some simple summary statistics to add to a report. Within a code workbook, they develop the following code and output to add to a report.\ndescriptive_statistics &lt;- function(analytic_dataset) {\n  library(gtsummary)\n  library(dplyr)\n\n  patient_characteristics &lt;-\n    analytic_dataset %&gt;%\n    dplyr::select(\n      Scurvy_Indicator_Before_COVID,\n      Age,\n      Race_Ethnicity,\n      Medication_A,\n      Medication_B,\n    ) %&gt;%\n    gtsummary::tbl_summary(\n      by = Scurvy_Indicator_Before_COVID\n    )\n\n  table1 &lt;- patient_characteristics &lt;- as_tibble(table1, col_labels = FALSE)\n\n  return(table1)\n}\nWhich produces the following Table 2.2.\n\n\nTable 2.2: Characteristics of Patients with and without Scurvy\n\n\n\n\n\n\n\nCharacteristic\nNo History of Scurvy\nHistory of Scurvy\n\n\n\n\nPatient Count\n19,871\n2,199\n\n\nAge, Median(IQR)\n50(25, 75)\n51(27, 75)\n\n\nRace/Ethnicity\nNA\nNA\n\n\nNon-HispanicWhite\n7,834(39%)\n883(40%)\n\n\nNon-HispanicBlack or African American\n3,970(20%)\n431(20%)\n\n\nHispanic or Latinx\n3,986(20%)\n430(20%)\n\n\nMissing/Other\n4,081(21%)\n455(21%)\n\n\nMedication A\n3,017(15%)\n318(14%)\n\n\nMedication B\n1,661(8%)\n173(8%)\n\n\n\n\n\nAfter repeating this for the remaining analyses for the project, the statistician creates a report and requests it be reviewed by the N3C Download Review Committee\n\nStatistician: Good afternoon! I just received notification from N3C that my download review request was approved. What happens now?\nNavigator: That’s great! Now that it’s been approved, you can go into the Download Review Dashboard and there’s an area where you can safely download the results. The review comes with a DRR #, which is required to download the associated results.\nS: Ok. And then I can send it out to the rest of the team?\nN: Yep! Once it’s been reviewed, you can share it with the rest of the team outside of the Enclave.\nS: Great. Thank you! I’ll set up a time to review the results with the team next week."
  },
  {
    "objectID": "chapters/story.html#draft-paper-pub-committee",
    "href": "chapters/story.html#draft-paper-pub-committee",
    "title": "2  A Research Story",
    "section": "2.8 Draft paper, pub committee",
    "text": "2.8 Draft paper, pub committee\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nNearing the trail head…\n\n\n\nInvestigator: Thanks everyone for making this meeting. I hope you’ve had a chance to review the results that were circulated last week.\nSubject Matter Expert: They looked great. Very unexpected, but intriguing results. I was surprised that outcomes were so much better in those exposed Medication B than Medication A.\nTeam reviews the results in detail. Manuscript write-up divided amongst the study team.\nI: Ok, so we should all know what the next steps are in terms of the write-up. Are there any other requirements from N3C before we can submit the paper to our target journal?\nNavigator: Good question. The manuscript draft has to be reviewed by the N3C Publication Committee. They meet weekly and will review the drat to make sure it’s compliant with N3C policies and adheres to the original research question proposed by us.\nI: Ok, that makes sense. How long does it usually take?\nN: In my experience, it’s a pretty quick turnaround. Usually 1-2 weeks unless there are serious issues with the draft. They are primarily concerned about compliance with N3C policies, such as not having small cell counts and having gone through the download review process. Since we did this, it should be a quick review.\nI: Awesome. I hope you’ll be able to help us when we get to the step. Thank you!\nN: Of course. Let’s plan on meeting after the draft is closer to being finished.\nI: Sounds great. See you all in two weeks!\n\nAt this point, the team divides up the remaining work and gets to writing. After completing a draft that’s almost ready to be submitted for publication, they submit their draft to the N3C Publication Committee. After a week, they hear back from the N3C Publication Committee.\n\n2.8.1 Sample Email from N3C Publication Committee\n\nTitle: Approved to submit MSID:383.23 | Investigator | The Impact of Drug A and Drug B on Patient with Scurvy Post-Acute COVID-19\nDear Investigator,\nThe publication committee has reviewed and conditionally approved your manuscript for submission. The following must be addressed: Please add “on behalf of the N3C consortium” to the end of the author list with an asterisk that says consortial contributors are in the process of being documented.\nSincerely, The N3C Publication Committee\n\n\n\n\n\nSciences, Observational Health Data, and Informatics. 2019. The Book of OHDSI: Observational Health Data Sciences and Informatics. United States: Independent. https://ohdsi.github.io/TheBookOfOhdsi/."
  },
  {
    "objectID": "chapters/story.html#footnotes",
    "href": "chapters/story.html#footnotes",
    "title": "2  A Research Story",
    "section": "",
    "text": "This role may be called something differently at your Institution; the roles are defined below in Section 2.2.↩︎\nRead about the institutional-level DUA in Chapter 5.↩︎\nSee Chapter 5.↩︎\nProject-level paperwork are discussed in Chapter 5.↩︎\nThe NIH “Enclave” is detailed in Chapter 8.↩︎\nDURs are the topic of Chapter 6.↩︎\nRead about the OMOP Standard Tables in Chapter 7, specifically the medications are in the drug_exposure table.↩︎\nConformance is a topic in Chapter 3.↩︎\nSee Chapter 6 for accessing the N3C Enclave.↩︎\nRead about SQL, Python, and R transforms in Code Workbooks in Chapter 8.↩︎\nThere is a brief discussion of SME’s cat.↩︎\nThere is a brief discussion of S’s daughter strutting in the background wearing a cowboy hat and waiving a fairy wand.↩︎\nAn HIE is a health information exchange.↩︎\nThe benefits and caveats of real-world data are a theme throughout the book, particularly in the best practices discussed in Chapter 9.↩︎\nAuthoring and using concept sets are described in Chapter 7. Mapping an ICD to SNOMED diagnosis code is an example of mapping a “non-standard” to a “standard” concept, discussed in Chapter 7.↩︎\nTODO: is the book planning to have a section on the CMS & death records?↩︎\nEveryone says goodbye to the cat.↩︎\nThe concept_relationship table is discussed with the OMOP concept hierarchy in Chapter 7.↩︎"
  },
  {
    "objectID": "chapters/lifecycle.html",
    "href": "chapters/lifecycle.html",
    "title": "3  Data Lifecycle - From Patients to N3C Researchers",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1ZjnFezddZk0YllqIDZN1mgexn1yM51tH\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-preamble",
    "href": "chapters/governance.html#sec-governance-preamble",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.1 Preamble",
    "text": "4.1 Preamble\nN3C is more than simply a data enclave. It is a research community dedicated to facilitating collaborations and the rapid generation and dissemination of knowledge to help combat COVID-19. There were numerous innovations along the pathway to N3C. The novel governance procedures and structures described here may benefit other collaborative research efforts beyond the pandemic emergency response."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-leadership",
    "href": "chapters/governance.html#sec-governance-leadership",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.2 N3C Leadership",
    "text": "4.2 N3C Leadership\nN3C is a public-private-government partnership among the Clinical and Translational Science Awards (CTSA) Program hubs supported by the National Center for Advancing Translational Sciences (NCATS) as the overall data steward; the National Center for Data to Health (CD2H); the Institutional Development Award Networks for Clinical and Translational Research (IdeA-CTR); data contributing organizations (e.g., public health Health Information Exchanges, hospitals/health systems, academic medicine, and clinical research); distributed clinical data networks (PCORnet, OHDSI, Act, TriNetX); multiple commercial partners; and a large community of researchers.\nFour co-leads (two from the researcher community and two from NCATS) developed the vision for N3C and led its implementation with extensive community consultation. Their vision was to bridge the traditional silos of biomedical research to rapidly enable team science while respecting the rights of data subjects, institutions, and researchers. To accomplish this at record speed, the N3C leadership established five community workstreams. Five Community workstreams were rapidly organized: Data Partnership & Governance, Phenotype & Data Acquisition, Data Ingestion & Harmonization, Collaborative Analytics, and Synthetic Data. focusing on specific N3C components. The workstreams are supported by dedicated administrative staff, with virtual workspaces and communication channels (Slack) to share materials and reduce barriers to participation. The workstreams meet regularly, and meetings are open to anyone interested in participating or monitoring activities. Early on, the N3C conducted general webinars and Questions and Answers sessions to inform potential partners and created a Welcome Partnership Packet to help initiate participation."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-culture",
    "href": "chapters/governance.html#sec-governance-culture",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.3 Establishing a collaborative governance culture- The Partnership and Governance Workstream",
    "text": "4.3 Establishing a collaborative governance culture- The Partnership and Governance Workstream\nAchieving the goals of a shared data resource involves deciding how the resource may be produced, operated, used, and sustained. Critical considerations for establishing a data sharing governance framework include:\n\nDeveloping a data-sharing strategy (i.e., deciding what data will be shared and whether sharing will be bilateral or multilateral, whether the data will be centralized or distributed /decentralized),\nUnderstanding the legal, regulatory and organizational compliance requirements and establishing the data sharing agreements (i.e. identifying potential restrictions on data sharing and use due to laws, regulations or ancillary agreements, and understanding party’s legitimate interests)\nSelecting the technical environment where sharing will occur (i.e. selecting the technical infrastructure and the security measures to be implemented to protect data integrity, assessing risks and managing data access)\nPreparing the data for sharing, retention and disposition (i.e. optimizing data usability, selecting data processing methods and adopting FAIR principles)\nMonitoring data sharing (i.e. establishing a monitoring and reporting plan, enforcing policies and procedures, and resolving disputes)\n\nThe Data Partnership and Governance Workstream established by the N3C PIs with participation from NCATS and the scientific and ethics community, is responsible for (1) establishing and refining the principles, policies, and procedures to support N3C, (2) determining the roles and functions to support the governance strategy, and (3) making recommendations to NIH for their implementation. The workstream convenes representatives of contributing organizations, researchers, and institutions that support them, NCATS, ethicists, and anyone interested in advancing N3C.\nAt the start, the Partnership and Governance workstream met twice weekly to rapidly align on an N3C governance vision and establish the universal conditions governing the use of N3C data. These conditions are that data protection is adopted by default, contributing sites are not identified, and workspaces must be provisioned on a per-project basis only for COVID-19-related research. Also, data cannot be extracted or downloaded from the data enclave, with the exception of publishable summary results or figures, each of which must be reviewed prior to export (see Chapter 10 for details on this process).\nThe workstream split into a smaller subgroup to draft the supporting governance documents while continuing to meet with the whole Workstream for ideation, context, and feedback weekly. Onward, the workstream adjusted the frequency of the meetings to the work cadence."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-bridges",
    "href": "chapters/governance.html#sec-governance-bridges",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.4 N3C Governance Bridges Individual Oversight Responsibilities",
    "text": "4.4 N3C Governance Bridges Individual Oversight Responsibilities\nThe N3C Governance is a set of behavioral norms, policies, and procedures supported by technology/security measures and oversight mechanisms. The N3C Governance structure relies on a variety of collaborative approaches, from hierarchical to consultative to consensus-based. The Data Partnership and Governance Workstream advocates a partnership approach that builds on collective competencies where governance aligns with community values, and final sign-off is obtained according to leadership roles and responsibilities. This governance approach values input from stakeholders and helps develop a shared understanding while recognizing the different risks and obligations of the scientific community and NCATS. Governance policies are posted on Zenodo to promote transparency and elicit community feedback. This also provides a resource for other researchers facing similar governance needs.\nThe N3C Governance framework was developed with guidance from the NIH Office of the General Counsel. As data steward, NCATS has fiduciary responsibility for the data enclave and adjudicating access. As such, NCATS is responsible for establishing the terms for data contribution and use, user training requirements, and enclave usage accountability mechanisms. Participating in N3C necessitates both organization-level agreement(s) and user commitments to the N3C User Code of Conduct and other N3C governance requirements.\nAdditionally, the scientific community representatives in the Participation and Governance Workstream, set the behavioral and ethical expectations and norms, including the N3C Community Guiding Principles to support collaborative research and the Attribution and Publication Policy to clarify the acknowledgment and authorship expectations for disseminating N3C-enabled research. The N3C Community Guiding Principles include a Diversity and Inclusion statement highlighting the commitment to create a safe and welcoming environment for all participants to N3C. It also has an Ethics statement highlighting the community’s core values. The scientific community takes a holistic approach to enforcing the Community Guiding Principles. A Community Response Team follows a conflict resolution process emphasizing dialog and mutual respect to adjudicate complaints and solve disagreements.\nSeven principles summarize the Community Guiding Principles:\n\nPartnership: N3C community members are trusted partners committed to honoring the N3C community guiding principles and N3C User Code of Conduct\nInclusivity: N3C is open to any organization that wishes to contribute data, code, and ideas, as well as anyone who registers to use N3C data to conduct COVID-19 related research, including citizen/community scientists\nTransparency: Open processes and reproducible research is the hallmark of N3C and good scientific practice. Access to data is project-based and focused on COVID-19 research questions. Descriptions of projects are posted publicly and are searchable to promote collaborations.\nReciprocity: Contributions are acknowledged and results from analyses, including provenance and attribution, are expected to be shared with the N3C community.\nAccountability: N3C community members take responsibility for their activities and hold each other accountable for achieving the N3C objectives and acting through good scientific practices.\nSecurity: All activities are conducted in a secure, controlled access cloud-based environment and are recorded for auditing and attribution purposes.\nMutual respect: Communications should be professional, concise, clear, and relevant. Follow proper communication etiquette. Avoid excessive conflict, unprofessional arguments, ad hominem attacks, and/or ridicule over chat and in messaging.\n\n\n\n\nFigure 4.1: N3C shared Governance initiatives with sign-off responsibility represented"
  },
  {
    "objectID": "chapters/governance.html#sec-governance-ethics",
    "href": "chapters/governance.html#sec-governance-ethics",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.5 Ethical oversight",
    "text": "4.5 Ethical oversight\nThe ethical oversight of N3C is two-fold. First, the IRB at John Hopkins University (JHU’s IRB) approved the N3C protocol and methodologies employed to create N3C data resources. Institutions must obtain relevant authorizations, including applicable IRB approvals or waivers, before contributing data to N3C. Institutions can select their own IRB or rely on JHU’s IRB as the Central IRB for N3C. Relying institutions must use the SMART-IRB online reliance system and execute a Master Reliance Agreement to delegate IRB oversight to JHU’s IRB for their contribution to N3C. While it is not necessary to use JHU’s IRB, the reliance process is simple and efficient, and relying on a single IRB reduces the burden associated with contributing data and speeds up IRB review.\nSecond, NCATS obtained IRB approval for the Data Enclave from the NIH IRB since NIH houses the Enclave under NCATS oversight. NCATS also received a Certificate of Confidentiality to resist legal requests to disclose identifying information from the people represented in N3C data."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-contractual",
    "href": "chapters/governance.html#sec-governance-contractual",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.6 Contractual Agreements: Separating Data Transfer and Data Use",
    "text": "4.6 Contractual Agreements: Separating Data Transfer and Data Use\nThe N3C Community contributed to the two types of contractual agreements: ones that delineate data contribution (or data transfer) and others that define data access (or data use) conditions. Both types of agreements clarify confidentiality, intellectual property, warranties and liabilities, conflict resolution, and expiration or termination. The data transfer and use agreements are intentionally separated since contributing data is not required to be eligible to access the data. All contractual agreements are made between NCATS and signing officials from home institutions\nAn N3C Data Transfer Agreement (DTA) must be executed between NCATS and a contributing organization before they can submit health record data.\nN3C leaders saw the value of enhancing the N3C research capability and developing the governance to enable privacy-preserving record linkage (PPRL) between N3C data and other data sources. PPRL enables linking records across multiple datasets without revealing the identity of the data subject. Data contributing organizations willing to participate in this data linkage pilot must execute a separate Linkage Honest Data Broker Agreement(LHBA) with NCATS and the Linkage Honest Broker. Again, separating the DTA and LHBA provides the flexibility to contribute data without enrolling in the PPRL.\nA separate Data Use Agreement (DUA) must be executed between NCATS and signing officials from an institution whose investigators wish to access N3C data. To improve efficiency, instead of executing a traditional pair-wise agreement each time a researcher needs access to N3C, a single DUA is executed between NCATS and an organization to render individual researchers eligible to request access to N3C content."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-access",
    "href": "chapters/governance.html#sec-governance-access",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.7 Data Access by Researchers",
    "text": "4.7 Data Access by Researchers\nFirst-time users wishing to access N3C must verify that their institution has executed a DUA with NCATS. Then users must register an account with N3C, including agreeing to the Community Guiding Principles and complete the Information Security and Management Refresher course of the NIH Information Security and Information Management Training series. Only then can users submit a Data Use Request (DUR). NCATS established a Data Access Committee (DAC) responsible for reviewing and approving DURs and addressing user questions. Users must submit a new DUR for every separate study. (See Chapters 5 and 6 for details.)\nThere are three levels of data access commensurate with the sensitivity of the data. See Chapter 6 for more details about these levels.\n\n\n\nFigure 4.2: Data Tiers\n\n\nAccess to each data level requires approval from the Data Access Committee. In addition, investigators requesting access to level 3 data must submit proof of IRB approval from their institution or an independent accredited IRB for their intended use. Note that PPRL data is only available by special request in level 3 projects with appropriate additional IRB approval.\nN3C governance includes considerations for protecting special populations and communities, and respect for tribal sovereignty. N3C contains no Tribal affiliation. Using ZIP code information to make assumptions about Tribal affiliation is not valid or appropriate.\n\n\n\nFigure 4.3: Data Access Governance Process. To lower the burden on participation, user authentication and eligibility are established once during account registration. Eligible users can then submit their Data Use Request for evaluation by the Data Access Committee. A new request must be submitted for each specific project."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-incident",
    "href": "chapters/governance.html#sec-governance-incident",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.8 Incident Notification and Escalation procedure",
    "text": "4.8 Incident Notification and Escalation procedure\nIt is essential to create an Incident Notification Policy to ensure that the right people are notified about incidents at the right time and that problems can be addressed rapidly. Incidents include interpersonal interactions or disputes, suspected terms/policies violations, technical issues such as systems interruption or process delays, a potential data breach, and attribution or publication situations. For each incident type, an escalation path clarifies the first line of response and any further process or action for escalating it based on severity.\nUsers can report interpersonal conflicts to the “Report Conduct Concerns” page on the website. All concerns reported are taken seriously and handled with discretion. A Community Response Team of 2-3 individuals nominated by the community follows a conflict resolution process to help parties solve disagreements in good faith with constructive dialog and escalate to NIH if relevant."
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-eligibility",
    "href": "chapters/onboarding.html#sec-onboarding-eligibility",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.1 Researcher Eligibility",
    "text": "5.1 Researcher Eligibility\nCitizen scientists, researchers from foreign institutions and researchers from U.S.-based institutions are all eligible to have access to the N3C Data Enclave. Everyone with an N3C Data Enclave account has access to the tools and public datasets that are available in the Enclave.\nThere are several levels of Electronic Health Record (EHR) data that are available within the N3C Data Enclave. (For more information about the levels of data, see the section ‘Description of Levels 1, 2, 3’ in the ‘Getting & Managing Data Access’ chapter. LINK NEEDS TO BE ADDED HERE)\nCitizen scientists are only eligible to access synthetic data (Level 1). This data is artificial but statistically-comparable to, and computationally derived from, the original EHR data.\nResearchers from foreign institutions are eligible to access synthetic data (Level 1) and patient data that has been deidentified by removal of protected health information (PHI) (Level 2). (PHI includes 18 elements defined by the Health Insurance Portability and Accountability Act (HIPAA).)\nResearchers from U.S.-based institutions are eligible to access synthetic data (Level 1), deidentified patient data (Level 2) and patient data that includes dates of service and patient zip code (Level 3). (The latter data set is referred to as a limited dataset because it contains only 2 or the 18 PHI elements.)"
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-registration",
    "href": "chapters/onboarding.html#sec-onboarding-registration",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.2 Registration",
    "text": "5.2 Registration\n\n5.2.1 ORCiD; InCommon vs Login.gov\n\n\n5.2.2 NIH IT security training\n\n\n5.2.3 Human Subjects Training\nDue to the secure nature of the data that is available in the N3C Enclave registration of users is key. There are two options in which users can log in and create an account, InCOmmon or Login.gov. The InCommon pathway is available to select institutions that participate in that identity management service. You can clock of the link to confirm if your organization participates. If your institution does not participate with InCOmmon, you will need to create a login.gov account. Use the link to Login.gov and complete the required fields to create an account. Once you know which pathway you will use to create an enclave account there are other security measures that are put in place, you will need to have a ORCiD, complete NIH security Training, and also human subjects training.\nORCiD, which stands for Open Researcher and Contributor ID, is a unique identifier free of charge to researchers.\nThe N3C Data enclave is hosted by National Center for Advancing Translational Sciences and all researchers must complete the “Informational Security, Counterintelligence, Privacy Awareness, Records Management Refresher, Emergency Preparedness Refresher” course. The course can be accessed at https://irtsectraining.nih.gov/public.aspx. The course take approximately 60-90 minutes to complete and you should print your certificate of completion. Users need to complete Human Subjects training that aligns with their institution’s guidelines. You will need to provide the date of completion as part of enclave creation.\nOverall, users will need to confirm if they use the InCOmmon or Login.gov pathway, register for an ORCiD, have completed NIH Security Training, and completed institution human subjects training."
  },
  {
    "objectID": "chapters/onboarding.html#data-use-agreements",
    "href": "chapters/onboarding.html#data-use-agreements",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.3 Data Use Agreements",
    "text": "5.3 Data Use Agreements\nThe data use agreement (DUA) establishes the permitted uses of the data in the N3C Data Enclave. By signing the agreement, an institutional official is assuring that users from their institution will abide by the terms defined in the agreement.\nA DUA must be executed by National Center for Advancing Translational Science (NCATS) and a research institution. The DUA must be signed by authorized institutional officials who have the authority to bind all users at their institution to the terms of the DUA. (A citizen scientist who is not affiliated with an institution must execute a data use agreement with NCATS.) A DUA will be in effect for five years from the DUA Effective Date.\nEvery individual who has access to the N3C Data Enclave must be covered by a DUA. This DUA must be in place before an account for the N3C Enclave is requested. If your institution has an active DUA, there is no additional action required with regards to the DUA. A list of institutions with DUAs in place can be found at List of DUA Signatories: (https://covid.cd2h.org/duas).\nThe Institutional Data Use Agreement form is available at:\nhttps://ncats.nih.gov/files/NCATS_N3C_Data_Use_Agreement.pdf\nFor more information see:\nhttps://ncats.nih.gov/n3c/resources/data-access"
  },
  {
    "objectID": "chapters/onboarding.html#enclave-access",
    "href": "chapters/onboarding.html#enclave-access",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.4 Enclave Access",
    "text": "5.4 Enclave Access"
  },
  {
    "objectID": "chapters/onboarding.html#research-project-teams",
    "href": "chapters/onboarding.html#research-project-teams",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.5 Research Project Teams",
    "text": "5.5 Research Project Teams\n\n5.5.1 Project Lead vs Collaborations\n\n\n5.5.2 Common roles and expectations (PIs, PMs, SMEs, Analysts, …)\n\n5.5.2.1 Expertise needed"
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-dt",
    "href": "chapters/onboarding.html#sec-onboarding-dt",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.6 Domain Teams",
    "text": "5.6 Domain Teams\nThe N3C Data enclave is built for multi-site collaboration, and aims to bring together researchers of different backgrounds with similar questions using domain teams. Because N3C is multi-site, it can be difficult to collaborate with researchers of different backgrounds from different sites. Domain Teams exist to alleviate this difficulty. Some collaboration examples could be collecting pilot data for grant submission, sharing methodology and cohort logic, or learning how to use tools for large-scale data like machine learning.\nFor example, let’s say your institution just signed the DUA and you have some questions about the relationship between rurality and COVID treatments. You can look at the list of domain teams to see rural health. Then you can get in contact and go to the next upcoming meeting. At the meeting you can find out whether your questions are already part of an existing project within the domain team, or if a new project should be created.\nIf you don’t see your type of questions belonging to any existing domain teams you can create a new one here:\nhttps://n3c-help.atlassian.net/servicedesk/customer/portal/2/group/3/create/58\nSee here for a list of existing domain teams:\nhttps://covid.cd2h.org/domain-teams"
  },
  {
    "objectID": "chapters/onboarding.html#browsing-researchersprojectsinstitutions",
    "href": "chapters/onboarding.html#browsing-researchersprojectsinstitutions",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.7 Browsing Researchers/Projects/Institutions",
    "text": "5.7 Browsing Researchers/Projects/Institutions\n\n5.7.1 Object Explorer, Public Dashboard\nOnce you have an enclave account you can log in and use the object explorer to browse researchers and research projects. The object explorer can be found on the left hand side of your view on the enclave homepage. Click on Object Explorer and there are several object-type groups, to search researchers and projects, click on N3C Admin, from there are you can search Data Use Requests, N3C Researchers, and Research Projects. If you are looking for a particular N3C researcher, you can click on that box and in the search bar type in their name and hit enter. A new box will be displayed and you can clock on that researcher’s name and from there you can see the research projects that are lead of or a collaborator on. You can go back to the Object Explorer, object type groups, click N3C Admin again, and search research projects by clicking that box. Using the search bar at the top of the page you can search by key word. Type in the key word and click enter and a results box will be displayed. You can view all results to find the project in which you are interested in joining. From that screen, you can select the title of the project that you are interested in joining or reading about. There is a public-facing version of searching projects in addition to using the enclave as a search method. Users can search https://covid.cd2h.org/projects or https://covid.cd2h.org/dashboard/exploration#projects and search for title, lead investigator name and also the institution. There are many features that are available to search using the public-facing dashboard. There are four categories of."
  },
  {
    "objectID": "chapters/access.html#sec-access-background",
    "href": "chapters/access.html#sec-access-background",
    "title": "6  Getting & Managing Data Access",
    "section": "6.1 Background: N3C Protected Data Levels",
    "text": "6.1 Background: N3C Protected Data Levels\nNot all data in the enclave requires an approved DUR to access–mock datasets and publicly-available data (e.g., US census data) are accessible by everyone with enclave access. These low-risk data are covered more in Chapters 7 and 10.\nThe harmonized EHR data that do require an approved DUR to access are made available in three different “levels,” each with different amounts of data obfuscation, and correspondingly different access requirements. Deciding which level of data is appropriate for your study is important, because accessing Level 3 data is more work and restrictive than accessing Level 2 data. On the other hand, there are some studies that can be accomplished only with Level 3 data. Note that if you start with a lower level of data, it is possible to “upgrade” a project’s access level, though all participants in the project will need to complete another DUR for the new level.\nIn addition to the primary Level 1, 2, and 3 datasets is “PPRL” data. PPRL data includes extra non-EHR sources of information such as obituary-based mortality records and viral variant sequencing information. These are available alongside only Level 3 data as an optional add-on; we’ll discuss PPRL in more detail below.\n\n6.1.1 Level 3, Limited Data Set (LDS)\nLevel 3, or LDS data is the most complete and protected (the term “limited data set” is defined by HIPAA and may contain a limited set of potentially identifying information). This dataset contains two pieces of Protected Health Information (PHI) defined by HIPAA: full, 5-digit patient zip codes,1 and accurate dates of events and services (except for dates of birth which are limited to month and year).2 Level 3 data are in the OMOP common data model, with some N3C-specific additions and conveniences, and are versioned as releases as described above. (OMOP and N3C-specific additions are covered in Chapter 7.)\n\n\n6.1.2 Level 2, De-Identified\nLevel 2 data, also known as De-Identified data, contains nearly the same information as the Level 3 data, but the two PHI-containing fields are further anonymized. Zip codes available in Level 3 are truncated to just the first 3 digits, and all recorded dates are shifted randomly, where the range of the random shift is +/- 180 days. This is not as dramatic a research limitation as it may seem, because the random shift amount is determined per patient: all dates for a given patient are shifted by the same (unknown, random) amount, allowing identifying sets of patients who had, for example, a positive COVID-19 PCR test and within 14 days received a given drug treatment.3 Level 2 data would not be appropriate for studies considering absolute timing, such as whether a patients’ primary COVID infection occurred during the Delta wave. Such questions are best answered by the LDS data.\nThe Level 2 data are also in OMOP format and versioned as releases. We’ll forgo examples of notional data because the format is exactly the same as for Level 3, LDS data.\n\n\n6.1.3 Level 1, Synthetic\nLevel 1, or Synthetic,4 data provide the most anonymous view of the harmonized data, and are quite different from the Level 2 and 3 datasets in both format and content. Rather, Level 1 data were generated from a statistical model of a researcher-defined subset of the Level 3 data. This means Level 1 data contain no real patient records at all, but only a synthetic derivative designed to be statistically similar. The generation process is handled by a private company, MDClone, whose proprietary algorithms also look for resulting information that are too similar to real patient information, potentially resulting in a loss of patient privacy. Such records are masked with “censored” values in the resulting dataset.\nBecause this statistical-modeling approach doesn’t scale to the entirety of an EHR database, it is generated on subsets of data of interest to researchers, with the assistance of MDClone representatives. To take an example, consider a research team interested in outcomes of COVID-19 in diabetic vs. non-diabetic individuals. Inside the enclave, the team initiates a request with an MDClone liaison, suggesting that they would like to collect a set of patient information (with one row per patient in the resulting table), with columns for patient_age (at the date of their first COVID-positive PCR test result), has_diabetes (indicating if their record contains a diabetes diagnosis), days_until_ventilation (number of days elapsed between their COVID-positive test and subsequent mechanical ventilation, or null if they were not ventilated within 30 days), and several other potential confounding variables. The MDClone liaison then collects this requested information from the Level 3 data, develops a representative statistical model, and delivers a table of synthesized rows from the model with the columns requested to the team.\nLevel 1 datasets are generated from the most recent Level 3 release at the time of generation, and not automatically updated as new primary N3C data arrives. Notes for each Level 1 dataset describe the date of generation and information included for future reference.\n\n\n\n\n\n\nUpdate: As of mid-2022, N3C is no longer able to generate new synthetic datasets at researcher request. The previously-generated datasets are still available for use however, and gaining access to Level 1 data provides access to all generated synthetic datasets.\n\n\n\n\n6.1.3.1 Example N3C OMOP Data Format\nTo give a sneak preview of the primary N3C data format, here’s a snapshot of a few columns of notional (fake) data in OMOP format from the condition_occurrence table:\n\n\n\nFigure 6.1: A subset of rows and columns for mock condition_occurrence data. Notice the N3C-added data_partner_id, these are anonymized identifiers for sites contributing data, labeling each record with a source.\n\n\nNotice in particular the columns for data_partner_id (a pseudo-random identifier assigned to each data partner), condition_concept_id (the OMOP identifier uniquely describing the condition used for data filtering and analysis), and condition_concept_name (the human-readable version of condition_concept_id, used for convenience but not for data filtering). Other OMOP tables link records by shared columns; here’s a few columns from the person table describing basic demographics of patients:\n\n\n\nFigure 6.2: Example data for the OMOP person table.\n\n\nSome other tables available with Level 3 data are specific to N3C; here’s a few columns from a notional manifest table providing information about data partners, including the source CDM used by the data partner, the dates of data extraction and submission to N3C, and whether the data partner performs random date shifting prior to submission to N3C:\n\n\n\nFigure 6.3: An example of the N3C-specific manifest table. This table provides information about data partners, such as their source common data model (cdm_name), whether they pre-shift dates (see Chapter 3), and when their last submission was.\n\n\n\n\n\n6.1.4 PPRL Data\nPPRL, short for “Privacy-Preserving Record Linkage,” is a strong cryptographic data handling technique allowing for the matching of records about individuals from different data sources, without revealing to any party except the data sources themselves any identifying information about the individual. While we won’t describe how the process works here, researchers with enclave access can read more about it in Chapter 7 and the Introduction section of the PPRL training module  in the Training Portal (see Section 11.4)."
  },
  {
    "objectID": "chapters/access.html#sec-access-availability",
    "href": "chapters/access.html#sec-access-availability",
    "title": "6  Getting & Managing Data Access",
    "section": "6.2 Level 1/2/3 (and PPRL) Availability",
    "text": "6.2 Level 1/2/3 (and PPRL) Availability\nLevel 3, LDS data are available only to researchers affiliated with US-based organizations such as universities and medical schools. As we’ll discuss below, the most significant access requirement for Level 3 data is a letter of determination from researchers’ local Institutional Review Board (IRB), so this level of data is also available to other US-based organizations with the ability to work with an Institutional Review Board, or IRB (e.g., pharmaceutical companies).\nLevel 2, De-Identified data are available to researchers at both US and approved foreign organizations (those that can sign an institutional Data Use Agreement (DUA), see Chapter 3).\nLevel 1 Synthetic data are available to researchers at US and approved foreign organizations (though note the update about Level 1 data generation above), as well as individuals without any affiliation with a research organization, i.e., “citizen scientists.” This level of data thus provides opportunities for public outreach and engagement with topics in clinical science. Citizen scientists must nevertheless complete some training requirements and sign the legal Data Use Agreement document. Minors may apply for access, but only via their parents’ or legal guardians’ consent and with explicit N3C consent.\nSince the registration process is non-trivial for those not affiliated with a research institution, citizen scientists hoping to work with Level 1 Synthetic data should reach out to one of the N3C support venues to get started (see Chapter 11).\nPPRL data can be accessed alongside only Level 3 (LDS) data, and as such has the same access availability. As we’ll discuss below, there are special procedures for gaining access to PPRL data as part of a Level 3 access request."
  },
  {
    "objectID": "chapters/access.html#sec-access-workspaces",
    "href": "chapters/access.html#sec-access-workspaces",
    "title": "6  Getting & Managing Data Access",
    "section": "6.3 Workspaces, Permissions, and the Data Catalog",
    "text": "6.3 Workspaces, Permissions, and the Data Catalog\nThe N3C Data Enclave uses a fine-grained permissions model to manage researcher access to protected data. While researchers are not able to modify these permissions themselves, understanding them will help in navigating the DUR process and subsequent work.\nData management in the Enclave is centered around “project workspaces” which act like folders–workspaces are indicated by a small filing-drawer icon and are listed under the “Projects & files” link in the left navigation menu.\n\n\n\nFigure 6.4: Project workspace browser. Note that the “Request access” buttons are not active and clicking one will suggest visiting the DUR dashboards described below.\n\n\nWorkspaces are used for multiple purposes. Some store the Level 1/2/3 data; the “LDS Release” workspace for example stores the tables for the Level 3 LDS data. (The Data Catalog provides a more efficient way to access these tables however, more on that below.) Some are used to store “external” datasets such as publicly-available US Census data (see Chapter 7). The “N3C Training Area” workspace can be accessed by anyone as a place to practice on notional data (see Chapter 10). Most, however, house research projects, and these are indicated with an RP-XXXXXX prefix.\nWork in one project workspace can access only data or files from another project workspace if a “reference” has been added from the former to the latter. Researchers do not have permission to add such references. Thus, access from a research project workspace to protected datasets is possible only if the appropriate references have been added by administrators, which is done after a corresponding Data Use Request has been approved. Said another way, rather than providing researchers access to data, researchers are provided access to project workspaces, and these are in turn provided access to data.\nThis reference-based permission scheme supports a number of useful features for N3C. Naturally, restricting workspace access to a subset of data affords the possibility of different levels of data access with correspondingly different access requirements. A single researcher may be involved with multiple research projects of different levels but cannot share data or files across them, thus it is impossible for a researcher with access to Level 3 data to share it with their colleagues in another project with Level 2 data."
  },
  {
    "objectID": "chapters/access.html#sec-access-dur",
    "href": "chapters/access.html#sec-access-dur",
    "title": "6  Getting & Managing Data Access",
    "section": "6.4 The DUR - Data Use Request",
    "text": "6.4 The DUR - Data Use Request\n\n6.4.1 Project Roles and DUR Types\nIn the simplest view, each research project is associated with a single ‘DUR’ listing the project title and abstract, and governing the level of data accessible to the project workspace (and thus also to all researchers with access to that workspace). DURs are reviewed by the N3C Data Access Committee (DAC), and when approved administrators configure the workspace and researcher access to it.\nIn practice the situation is a bit more complicated. The researcher who submits the initial DUR defining the title, abstract, and data access level is known as the project lead. Other researchers who sign on to the project submit “collaborator DURs.” A collaborator DUR is a copy of the original DUR submitted by the lead, but the title, abstract, and data access level are fixed. Both leads and collaborators must enter other requirements depending on the data access level (e.g., date of most recent Human Subjects Research Protection training or local IRB approval), and both lead and collaborator DURs are reviewed by the DAC before access is granted. If a lead’s DUR is rejected by the DAC, they will have the opportunity to appeal by updating the information provided.\nThe lead has certain abilities and responsibilities, most importantly configuring whether the project is open so that other researchers may request to join as collaborators, and if so periodically reviewing those requests. These cannot be delegated, so the lead should be someone with that decision-making authority. Aside from these aspects however, leads and collaborators all have the same permissions in the project workspace and can review and share work.\nSince DURs expire after one year, there is also a “renewal” DUR type, which both leads and collaborators are prompted to fill in the weeks prior to their DUR expiry via email. Renewal DURs don’t allow editing the title, abstract, or data level, and so are usually expedited by the DAC.\nLastly, there are “revision” DURs, where the lead requests to change some major feature of the DUR (most commonly to upgrade the data access level) for an existing project workspace. Once approved by the DAC, administrators reach out to the lead to plan the revision, because collaborators will also need to submit new collaborator DURs prior to the change or they will lose access to the workspace.\nNote that while we’ve described these different types of DURs, they all use the same DUR form, with the differences boiling down to what fields can be changed by the researcher filling it out.\nTheoretically a single researcher may have multiple outstanding DURs for the same project. For example, the lead for a project nearing its expiry may submit a renewal DUR, and while that renewal is pending DAC approval the lead may also decide to submit a revision DUR to access a higher level of data. Most likely the renewal will be approved first, followed by the revision. If it so happens that the revision is approved before the renewal, the renewal request will then be moot as revision also resets the expiry date. Such cases may require intervention by administrative support, so it is wise to be judicious about DUR submissions to expedite the process."
  },
  {
    "objectID": "chapters/access.html#sec-access-dashboards",
    "href": "chapters/access.html#sec-access-dashboards",
    "title": "6  Getting & Managing Data Access",
    "section": "6.5 DUR Dashboards",
    "text": "6.5 DUR Dashboards\nThere are three primary dashboards used to create and manage DURs; all three are linked from the Enclave homepage via buttons titled “Data Use Request (DUR)”, “My Projects (DURs)”, and “Explore Projects (DURs).”\n\n\n\nFigure 6.5: Enclave homepage. Highlighted are the three DUR-related dashboards.\n\n\nAlthough there is some functionality overlap, the primary uses for these three dashboards are:\n\nData Use Request (DUR) is used to submit a new Data Use Request for a new project. The submitter provides the project abstract and title and selects the data level needed, and will become the lead for the project.\nMy Projects (DURs) shows the status of your submitted requests and projects. This interface also provides some project management functionality for leads, such as linking a project to an N3C domain team, reviewing requests from potential collaborators to join the project, appealing rejected DUR submissions, and accessing the download request dashboard. \nExplore Projects (DURs) lists all N3C research projects, and for those that allow it, provides a link to request to join a project by submitting a collaborator DUR."
  },
  {
    "objectID": "chapters/access.html#sec-access-request",
    "href": "chapters/access.html#sec-access-request",
    "title": "6  Getting & Managing Data Access",
    "section": "6.6 Data Use Request (DUR): Initiate a New Research Project",
    "text": "6.6 Data Use Request (DUR): Initiate a New Research Project\nThis first dashboard is really just the DUR form itself, and is used to request the creation of, and access to, a brand new research project with access to one of the levels of protected N3C data. The form has a number of sections, and some of these are dynamic, depending on choices to earlier questions. The left side of the DUR form provides quick links to other dashboards or actions; “Create a New Project” simply refreshes the page (since we are already creating a new project DUR), “Request to become a collaborator on an existing project” opens the Explore Projects dashboard (discussed below), and Public Health Proposal opens the application form for the N3C PHASTR initiative (which utilizes a specialized variant of the DUR process discussed here).\n\n\n\nFigure 6.6: New Data Use Request (DUR) form.\n\n\nMoving to the main content of the DUR form, the first three questions prompt for a Title, Abstract, and Research Project Rationale. (The green Submit button is always visible, and will be clickable when all fields for the DUR are properly completed.) Title and Abstract are self-descriptive, but know that these entries will be listed for others to see, both inside the Enclave via the Explore Projects dashboard, and outside at https://covid.cd2h.org/dashboard/.\nThe Research Project Rationale deserves special attention. This information will be visible only to you (the submitter) and the Data Access Committee (DAC) who will approve or deny the request. The project rationale should describe for the DAC why data access is being requested in light of the goals of the research project. The rationale should include a justification for the level of data being requested, especially if requesting the Level 3 Limited Data Set (LDS). If your project doesn’t require the accurate dates or full patient zip codes provided by Level 3 data, it will be rejected, or approved for the Level 2 data even though Level 3 was requested. Similarly, if requesting access to any PPRL (see Section 6.2) datasets the rationale should include why. (You will also need to have the PPRL data mentioned in your IRB letter of determination - see below.)\n\n\n\nFigure 6.7: DUR metadata fields.\n\n\nThe next questions pertain to potential collaborations. The “Please associate your project with an existing N3C Domain Team” prompts you to select a domain team that best fits your project. Domain Teams are N3C working groups encompassing multiple research projects and are used to facilitate collaboration. Linking your project to a domain team this way in the Enclave allows others to navigate domain teams and browse linked projects dynamically. It is possible to modify this linkage later, so if you aren’t sure feel free to check the “need help choosing” check box.\n\n\n\nFigure 6.8: DUR Domain Teams and collaborator fields.\n\n\nThe next question, “Allow other researchers to join this project,” configures the project so that others can request to join via the Explore Projects dashboard. If this is enabled, other researchers will be able to fill out a collaborator DUR (a copy of this DUR but with the title, abstract, and data level fixed). As the lead submitting this primary DUR, you will have the opportunity to approve or deny such requests (via the My Projects dashboard); if you approve, their request will be forwarded to the DAC for final approval to the workspace and data. This option can be changed later.\nThe Collaborators section allows you to send invites to potential collaborators to join the project. Added collaborators will receive an email notification (with instructions to register for N3C in case they don’t already have access), and will see the invite in their My Projects dashboard with a link to fill out a collaboration DUR (much like those who request to join described above). These invites are sent regardless of the earlier choice to Allow/Do not allow others to join the project.\nNext is the choice of data level to request. Levels 2 and 3 require additional information, so we’ll start with the Level 1 data and return to see the differences for Levels 2 and 3.\n\n\n\nFigure 6.9: DUR data level request.\n\n\n\n6.6.1 Level 1 DUR Requirements\nFor Level 1 access, the first requirements are that you have read and attested to the Data Use Agreement, and that you have completed the required NIH IT Security training course within the past year. Both of these are also required as part of onboarding, so we won’t cover them here (see Chapter 5).\n\n\n\nFigure 6.10: DUR attestations.\n\n\nFinally, you will need to attest to having read the N3C Code of Conduct (with the text provided above), that you have read and understood the N3C download policy (a link is provided via the more information icon), and that if you choose to use any additional data sources in connection with N3C protected data that you are aware of your institutions’ policies around doing so. (Some institutions restrict how and when EHR data can be linked to other datasets, including publicly available data ingested for use by N3C researchers.) \n\n\n\nFigure 6.11: DUR attestations and acknowledgements.\n\n\nWith all of this information provided, you will be able to click Submit and send the request to the Data Access Committee for review.\n\n\n6.6.2 Level 2 DUR Requirements\nRequesting access to Level 2, De-Identified data requires additional information. First, when selecting the Level 2 request option, you will be asked to confirm that you have taken Human Subjects Research Protection training within the last three years and provide the date of completion. Most research institutions provide specific training courses to satisfy this requirement, for example via the CITI Program. The specific courses required vary from institution to institution, so check with your research office or equivalent authority for guidance. If your institution does not offer or recommend specific courses, you can utilize the free course provided by Health and Human Services.\n\n\n\nFigure 6.12: DUR Human Subjects Research Protection (HSRP) training attestation. HSPR training must have been completed within the last three years.\n\n\nIn addition to human subjects research protection training, you will need to consider whether your institution requires a letter of determination from your local IRB when using De-Identified data. If so, you’ll be prompted to upload the letter as a PDF before you can submit the DUR.\n\n\n\nFigure 6.13: DUR IRB letter upload. For a Level 2 DUR, this is required only if your local institution requires IRB approval for de-identified data access.\n\n\n\n\n6.6.3 Level 3 DUR Requirements\nThe Level 3, Limited Data Set (LDS) offers the most complete view of N3C data. Unlike Level 2 De-Identified data, dates are not shifted (for most contributing data partners) and 5-digit patient zip codes are available (for most zip codes). See Chapter 3 for more details\nLevel 3 DURs require the same attestations and human subjects research protection training as Level 2 DURs. A Level 3 DUR also allows researchers to request access to one or more supplemental PPRL datasets providing additional information about patients. These provide additional mortality information, viral variant records, and claims data from the Centers for Medicare and Medicaid Services, but are available only for subsets of N3C patients (see Chapter 7 for details about PPRL).\n\n\n\nFigure 6.14: DUR Privacy-Preserving Record Linkage (PPRL) options for Level 3 requests.\n\n\nA Level 3 DUR additionally requires a letter of determination from the submitter’s local IRB:\n\n\n\nFigure 6.15: The lead (submitting) investigator must supply an IRB letter of determination from their local institution. Collaborators from the same institution as the lead aren’t required to, but collaborators from other institutions must also obtain and submit a determination letter from their own local IRB.\n\n\nNote that when requesting access to PPRL datasets, both the Research Project Rationale (reviewed by the DAC) and the IRB letter of determination should address the need for the PPRL datasets requested. Failure to adequately justify the use of PPRL datasets may result in denial of the DUR, or approval with lack of PPRL dataset access.\n\n\n6.6.4 Project Workspace Creation and Data Access\nThe Data Access Committee reviews all DURs. After a new project DUR is approved, a workspace is created inside the enclave with access to the approved datasets (see Workspaces, Permissions, and the Data Catalog above), and the submitting lead is given access to this project workspace. An email is also sent to the submitting lead with links to useful learning resources.\nThe full review and access process varies depending on the level of data requested and workload of the DAC, but most DURs are evaluated within 2 weeks. The status of your DUR will be shown in the My Projects dashboard (below). If a submitted DUR has not received any update for longer than two weeks, you should submit a request for follow up to the Enclave-external ticket support system."
  },
  {
    "objectID": "chapters/access.html#sec-access-explore",
    "href": "chapters/access.html#sec-access-explore",
    "title": "6  Getting & Managing Data Access",
    "section": "6.7 Explore Projects (DURs): Browse and Join Projects",
    "text": "6.7 Explore Projects (DURs): Browse and Join Projects\nAlthough all N3C research projects are listed in the public dashboard, the Explore Projects dashboard within the Enclave allows researchers to request to join projects that allow for it as collaborators.\nFirst, let’s explore the dashboard. The initial list of projects shown are “Projects to join” - these have been configured by the project lead to allow any N3C researcher to request access to the project for collaboration. Below this list are two expandable sections, “Projects to Explore” listing projects that do not allow requests to join, and “Operational Projects (N3C Technical Development)” listing DURs used by the data ingestion and harmonization teams and others.\nThe “Search Projects” field is helpful in finding one of the many N3C projects and supports searching in any of the shown text columns. The “Projects by N3C Domain Team” button shows the project list organized by domain team, which can be useful in identifying potential overlapping research. Finally, “Closed Projects” lists non-active projects, for example those whose 1-year expiry has passed without renewal.\n\n\n\nFigure 6.16: The Explore Projects dashboard.\n\n\nTo request to join a project, simply click its Request to Join link from the list. This opens up a collaborator DUR: a copy of the DUR submitted by the project lead, but the Title, Abstract, selected data level, and domain-team linkage are not changeable. You will need to complete the other requirements for the data level however (see above), including submitting a letter of determination from your local IRB if needed. If the DUR indicates that PPRL data are requested, your IRB letter should also reference these datasets. IRB determination letters are required for all Level 3 DURs, or if the DUR is Level 2 and your institution requires IRB review. (Note that the DUR form will not ask for an IRB letter if you are registered with N3C as being from the same institution as the project lead; in this case it is assumed that the lead’s already-submitted letter covers you as well.)\n\n\n\nFigure 6.17: Requesting to join a project from the Explore Projects dashboard opens a collaborator DUR for the project, where metadata fields such as title and abstract are fixed.\n\n\nCollaborator DURs must first be approved by the project lead (via their My Projects dashboard, see below). You may thus want to reach out to the lead prior to requesting access to their project so they can expect the request. Once approved by the lead the DUR is sent to the DAC for final approval. After DAC approval you will be given access to the project workspace (see Workspaces, Permissions, and the Data Catalog above). Collaborator DURs are generally faster to approve than new project DURs, but as with new project DURs if no progress is made in two weeks you should submit a request for follow up to the Enclave-external ticket support system.\nAs with new project DURs, the status of your submitted collaborator DURs can be found in the My Projects dashboard."
  },
  {
    "objectID": "chapters/access.html#sec-access-my",
    "href": "chapters/access.html#sec-access-my",
    "title": "6  Getting & Managing Data Access",
    "section": "6.8 My Projects (DURs): DUR Invitations, Status, and Management for Leads",
    "text": "6.8 My Projects (DURs): DUR Invitations, Status, and Management for Leads\nThe My Projects dashboard, linked from the Enclave homepage, shows the status of your Data Use Requests and allows leads to configure some parameters of their projects.\n\n\n\nFigure 6.18: The My Projects dashboard.\n\n\nAlong the left are the various projects you’ve requested access to, either as a lead who’s submitted a new project DUR, or as a collaborator who’s requested to join a project. Notice along the bottom the “Invitations to Submit a Project Collaborator DUR” - this section may be expanded, and if any leads have added you to their DUR as invited collaborators, you will see a link to fill out a collaborator DUR here. (This is the same process as requesting to join a project as described above.) A quick link to the Download Requests Dashboard is also provided near the top, which we won’t discuss further here. Nearby are two buttons for project leads, “Review Collaborator Requests for My Projects,” and “Manage My Projects,” both discussed below.\nHaving selected a project from the list, the interface shows a variety of information about the project, including the data access level, project lead, and a direct link to the project workspace. If you are a collaborator on the project rather than the lead, you will also see a button to voluntarily withdraw from the project, which will remove your access to the project workspace but leave intact your work within that workspace for your collaborators.\nThe table below shows the DURs you have submitted pertaining to the project. Usually this will list a single DUR, but as noted at the beginning of this chapter in theory you may have multiple outstanding DURs pertaining to the same project. The “DUR Review Status” column shows the state of the DUR, and may contain one of several values indicating its progress: “Pending Lead Investigator Approval” if the DUR still needs to be approved by the project lead (“Denied by Lead Investigator” if they deny the request), “Pending DAC Review” for DURs that have been approved by the lead and are awaiting DAC review (“Denied” for those DURs that have been denied by the DAC), “Reviewed and Pending Workspace Access” for DURs that are fully approved but awaiting permissions to be applied for project workspace access, and “Complete” for fully approved DURs with enabled data access.\nIf we scroll further to the right we see other important columns, particularly the Action and Renewal columns.\n\n\n\nFigure 6.19: Scrolling rightward when reviewing a project in the My Projects dashboard reveals more information and options.\n\n\nN3C DURs are valid for one year–continued access to the project workspace requires submitting a renewal DUR before the expiration date listed. The Renew DUR link in the Renewal column opens up a renewal DUR: a copy of the DUR with almost all information uneditable, but requiring re-attestation of information like the Data Use Agreement, Code of Conduct, and human subjects research protection training. Renewal DURs are processed more quickly than other types, but if you miss the renewal deadline and you are a collaborator (not the lead) you will need to resubmit a collaborator DUR for a more thorough review. Leads who miss the renewal deadline will have their projects closed and will need to submit an Enclave-external support ticket for help re-opening it.\nIf you are the lead for the project you will have the option to “Appeal” the DUR under the Action column to appeal a DUR rejected by the DAC. The appeal DUR is a copy of the submitted DUR, but all fields are editable, including Title, Abstract, data access level, and research project rationale. When appealing a rejected DUR, you will likely want to update at least the research project rationale. Finally, for active projects there will be an “Update DUR” option to request a significant change to an approved DUR requiring DAC review, such as updating a project to a higher level of data access.\n\n6.8.1 Project Management for Leads\nThe My Projects dashboard provides some additional functionality for project leads not available to collaborators in addition to the Appeal link. First is the dashboard accessed via the “Review Collaborator Requests for My Projects” button. The projects for which you are a lead are listed along the left, and the requests to access that project are listed along the right. You can choose to approve or deny each request by selecting it and clicking the large blue “Approve Selected Collaborator Request” button. Collaborator access cannot be revoked this way, however.\n\n\n\nFigure 6.20: Reviewing collaborator requests to join my projects.\n\n\nBack in the My Projects dashboard, we can select the “Manage My Projects” button to see more project configuration options. The main configuration options are “Configure N3C Domain Teams” to modify the listed linkage between the project and domain teams, “Review Collaborator Requests for this Project” which opens the collaborator approval dashboard above, and “Allow or Disallow Collaborator Requests.” This last option enables or disables the “Request to Join” link for the project in the Explore Projects dashboard.\n\n\n\nFigure 6.21: Adjusting other project settings."
  },
  {
    "objectID": "chapters/access.html#footnotes",
    "href": "chapters/access.html#footnotes",
    "title": "6  Getting & Managing Data Access",
    "section": "",
    "text": "There are some exceptions where five digit zip codes are not visible in Level 3 data: zip codes represented by fewer than 20,000 patients are removed altogether; sometimes only the first three digits of the zip code are displayed (such as zips with a predominantly American Indian/Alaskan Native population and zip codes from participating institutions who send only the first three digits).↩︎\nIn actuality, a few data partners also perform small amounts of date shifting–randomly shifting all dates on a per-patient basis to further protect patient privacy–prior to sending their data to N3C. The manifest information (described below) indicates which data partners do so and the range of potential shift so researchers can remove data from these partners if they need highly-accurate date information.↩︎\nLike Level 3 data, birth dates are available only at the year and month level, and these are similarly shifted along with other patient-associated dates. In Level 2 data birth date information is removed entirely for patients who are more than 90 years old, and a separate is_age_90_or_older column identifies these individuals as a group.↩︎\nThe Synthetic data discussed here should not be confused with the notional (fake) datasets described in Chapter 10. These happen to have similar sounding names: SynPuf and Synthea.↩︎"
  },
  {
    "objectID": "chapters/understanding.html",
    "href": "chapters/understanding.html",
    "title": "7  Understanding the Data",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1OHv1P2DKGucKBpSNEiQp8lGfR2xYHPAW\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-intro",
    "href": "chapters/tools.html#sec-tools-intro",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThis chapter introduces tools in the N3C Enclave used to analyze data, view results, track project progress, and obtain shared data and code developed in the N3C community. The focus is on accessing and using each tool, the skill level needed, as well as what types of analyses each tool is geared toward. It is expected that you know how data are organized in the N3C Enclave, including the OMOP data model, vocabulary, and concept sets (see Chapter 7 for details).\n\n\n\nFigure 8.1: High-level overview of an N3C project\n\n\nDue to the complexities of analyzing large clinical datasets, such as that compiled in the N3C Enclave, it is common, and many times necessary, to work in multidisciplinary collaborative teams to answer a research question. Figure 8.1 provides a high-level overview of the process behind forming a team and performing research using the N3C Enclave, along with the recommended field expertise needed during each phase. It is important to note that while certain team members may take the lead at various stages, a project benefits if all team members are engaged to some degree at all phases. Managing these collaborative and multi-faceted projects requires good record keeping. The N3C Protocol Pad is designed specifically for N3C research and to aid teams in designing, implementing, reporting, and publishing their work in a Findable, Accessible, Interoperable, and Reusable (FAIR) manner (Wilkinson et al., 2016) (see also https://www.go-fair.org/fair-principles/). Thus it is recommended that you utilize this tool throughout the implementation of your project.\nA research project in N3C starts with organizing a team with the required expertise (clinical, informatics, statistical, etc), followed by defining clinical questions around COVID-19 and characterizing the cohorts needed to answer each, i.e. clinical phenotyping. Because the N3C contains real-world EHR data that is harmonized from multiple data models and dozens of institutions, some information needed to identify an ideal clinical phenotype may be missing or incomplete. Thus, it is important to assess what information is needed to create an N3C computational phenotype for your cohorts. This could include using conditions, labs, or medications as proxies to identify a cohort if some information is not available. Generally, clinicians or other subject matter experts are leading this process with informaticians/data scientists providing guidance on what information is included in the N3C Enclave, what is missing or sparse, and overall data quality (see Chapter 9). Some data quality aspects can be easily obtained through the use of Logic Liaison Templates accessible through the N3C Knowledge Store. The N3C Enclave application Contour can be utilized at this stage, along with Code Workbooks for quick querying and visualizing of the data. Additionally, Fusion can be utilized to keep track of developed concept sets and utilized to easily input them into Logic Liaison Templates.\nThe generation of a computational phenotype overlaps with the generation of concept sets (see Chapter 7 for details), and is often a cyclical process. Well-vetted concept sets are key to obtaining robust cohorts, thus, having a team member familiar with the organization of data and the OMOP vocabulary, such as a data liaison, who can work closely with a clinician is beneficial. Concept set generation can be done using the N3C Enclave Concept Set Browser, or externally through OHDSI ATLAS.\nInformaticians and data scientists then utilize the computational phenotype and vetted concept sets to generate fact tables (i.e. datasets containing information about each patient like demographics, comorbidities, lab results, etc) for the cohorts of interest using the raw OMOP tables, which requires specific knowledge of how to work with large datasets in a Spark environment. Fact tables include all the information needed to characterize a cohort and perform downstream analyses to answer your research questions. Facts can include patient demographics, socioeconomic status, COVID status/severity, medications, comorbidities, etc. Logic Liaison Fact Table Templates can provide you a boost by allowing fast and robust generation of commonly used facts using N3C vetted concept sets and peer-reviewed code as a starter table. You can then append this base fact table to include project-specific facts needed for analyses. Figure 8.6 and Figure 8.7 in the N3C Knowledge Store section of this chapter provide a more detailed view of how Logic Liaison Templates can be integrated into a project to expedite fact table generation. The generation of the original fact tables from raw OMOP tables can be done using Code Workbooks (Section 8.4.3) or Code Repositories (Section 8.4.4).\nData scientists and statisticians can then analyze the extracted and formatted fact tables. This includes statistical tests, summary tables, visualizations, and reports for the team to discuss. Data analysis is also a cyclical process with all team members engaged in assessing results and circling back to further refine the computational phenotype and concept sets if needed. Depending on the type of analysis needed, Code Workbooks or Contour can be utilized at this step, followed by Foundry’s Notepad for reporting out results for secure team dissemination within the Enclave environment.\nOnce you obtain results that you wish to share with others, all tables, figures, and other data needed for reporting in publications, conference submissions, presentations, or any other activity outside the N3C Enclave environment must be submitted as a Data Download Request for a download review by NCATS (see Chapter 10). The download request is meant to ensure no prohibited data is being downloaded as per the N3C Data Download Policy summarized in the Publishing and Sharing Your Work chapter. After approval, your results can be included in research outputs, such as publications, and then submitted to the Publication Review Committee (see Chapter 10). This step is necessary to ensure data are being reported properly in the context of the research project and that proper attribution is being given to all those who contributed to the success of the research, either directly or indirectly. Upon approval, you are free to submit to the venue of choice and freely present the approved data to anyone at any time. Data download requests are performed within the Enclave environment, followed by submitting a Google Form to the Publication Review Committee.\nThe following sections of this chapter discuss each of the features and applications needed to perform research in the N3C Enclave, and include links to external Foundry documentation, as well as direct the reader to other chapters of this book that contain a deeper dive into various N3C topics, such as the organization of data and best practices. This chapter is best utilized along side the information provided in the next chapter, Best Practices and Important Data Considerations (see Chapter 9), which includes information on recommended data workflows, such as scheduling automatic data builds, to keep your research current, managing your projects using the Protocol Pad, and much more."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-concepts",
    "href": "chapters/tools.html#sec-tools-concepts",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.2 Using Concept Sets",
    "text": "8.2 Using Concept Sets\nAs discussed in the previous chapter Understanding the Data (see Chapter 7), the electronic health information coded in the various vocabularies used across the country are mapped to the OMOP common data model. By leveraging the hierarchical structure, parent codes and descendants can be captured in one fell swoop to create intensional concept sets for use in analysis.\n\n\n\nFigure 8.2: Concept Set Browser Homepage\n\n\nThe Concept Set Browser shown in Figure 8.2 is an N3C specific tool that allows you to explore and modify existing concept sets as well as create new concept sets to fit your exact study needs. For more details around the process of concept set creation, read the Concept Sets section in Chapter 7. It is recommended that, if you are a new researcher, you start your search for a concept set with the list of N3C Recommended concept sets. These concept sets have been frozen in their validated state by the Logic Liaisons and the Data Liaisons after obtaining clinician and informatic reviews (see Chapter 7). They are also the ones used to identify common comorbidities and other facts on the Phenotype Explorer and in the Logic Liaison Fact Table templates. The recommended other method of finding commonly used concept sets is by exploring bundles within the Concept Set Browser. These are groups of concept sets that are often used together. Further exploration of all other concept sets in the Concept Set Browser is also an option, though only advised when proceeding with the understanding that many of the existing concept sets that are not part of the N3C Recommended or bundles are crafted to be specific to one particular study team’s requirements.\nOnce concept sets have been identified for use in your analysis through the Concept Set Browser, the concept set members table becomes the link between concepts and the encompassing concept set as shown by Figure 8.3. You then have to “point” the code to the concept set members table to access this linkage. Once this has been accomplished, the choice becomes using the most recent version of a concept set (concept_set_members.concept_set_name where most_recent_version = TRUE) or using a specific version of the concept set (concept_set_members.codeset_id = {codeset id value}). While there are alternative ways to utilize concept sets and concept ids, the method described above is highly recommended primarily for the ability to quickly update a concept set without having to find and change hard-coded concept ids in a data processing pipeline.\n\n\n\nFigure 8.3: concept_set_members table\n\n\nReferring to a concept set by name and using the most recent version is often the preferred method for concept sets marked as N3C Recommended since these concept sets can only be updated by N3C core contributors after they have gone through a validation process which has been described in the previous chapter, Understanding the Data.\nFor concept sets that have not undergone the validation process and have not been marked as N3C Recommended by the N3C core contributors, it is recommended that the research team performs their own validation on an existing concept set or creates a new concept set with the input of a clinician. The concept set should then be referenced using its codeset ID when you are performing your data analysis. This will allow your team to perform their analysis from start to finish without worry about unvalidated modifications to the concept set. However, the codeset id being referenced in the code may need to be updated if the team chooses to modify the concept set once starting the analysis.\nIn constructing phenotypes from concept sets, concept sets may also need to be joined together; these actions are best done in SQL/R/Python Code Workbook transforms with the use of the Logic Liaison’s Combined Variable template  or in Code Repositories."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-store",
    "href": "chapters/tools.html#sec-tools-store",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.3 N3C Knowledge Store",
    "text": "8.3 N3C Knowledge Store\nThe N3C Knowledge Store is an application where you, as an Enclave user, can discover shared code templates, external datasets, reports, cohorts, and Python libraries (collectively also known as Knowledge Objects or KOs) and share similarly re-usable Knowledge Objects of your own with other Enclave users, regardless of the specific project from which the resource originated. Most Knowledge Store (KS) objects come about as core contributors and researchers alike develop resources they believe may be useful for others either within or outside of their research project team and wish to share them with the broader community. If you find yourself in this situation, you can easily create, submit, and share a KS resource by following this Code Workbook Template Quick Start Guide . Otherwise, more specifics on how to navigate the KS can be found in this Knowledge Store Guide  within the Enclave.\n\n8.3.1 Datasets\nOf the many types of Knowledge Objects, the most common are datasets and code templates. Datasets in the Knowledge Store can be internal or external. Internal datasets are generated from data inside the enclave, typically by researchers as part of their project, and are often of patient or row level granularity. As described in the previous chapter Understanding the Data, external datasets found in the Knowledge Store provide a wealth of information from public datasets that have been brought into the Enclave along with the crosswalks necessary for joining these aggregate data to person level data at various levels of granularity (see Chapter 7). Either type of dataset can be imported into a workbook or code repository of the appropriate data access level to be used as a starting point for further transformation or analysis.\n\n\n8.3.2 Code Templates\nDepending on the author’s intended use, some code templates can be applied to your custom input dataset while other code templates produce a dataset that can be joined to your study dataset. The code templates themselves can also be imported and customized to produce a dataset for defining a study cohort with key information to use during analysis or simply used as example logic if you are newer to coding. Code templates, in general, are often meant to help transform the massive amount of raw data to smaller, more digestible, and more readily applicable datasets and facts. A few helpful starter templates are those produced by the Logic Liaisons (see the Logic Liaison Services section in Chapter 11), some of which can be seen in Figure 8.4.\n\n\n\nFigure 8.4: N3C Knowledge Store Homepage\n\n\n\n\n8.3.3 Logic Liaison Fact Tables and Templates\nThe Logic Liaison Fact Tables and Templates are specifically designed to provide a validated and community agreed-upon method for calculating particular facts at the date level and/or the patient level. Through surveying the Domain Team Leads to establish a list of commonly derived variables and continuous feedback from the N3C Community to refine and update this list, the Logic Liaisons have developed, disseminated, and maintain two main fact table templates and an additional fact table template for SDoH variables. These two main fact table templates each produce day-level and person-level data frames of commonly used derived base variables for all N3C patients  as well as a subset who have an index date for their acute COVID-19 infection, the confirmed COVID-19 positive patients  (PCR/AG positive or U07.1 COVID-19 diagnosed). These day-level and person-level datasets are commonly referred to as the Logic Liaison Fact Tables and can be imported directly into a workbook for use without modifying the template. These fact tables are produced using a default set of concept sets from the N3C Recommended Concept Sets list and a set of default values for the template’s parameters.\nThe main fact table template KOs include not only the shared logic for importing/customizing the template for both data access levels, but also a detailed README, example datasets (aka default Logic Liaison Fact Tables), and example code workbooks as exemplified by Figure 8.5. It is recommended you first open the README and example code workbooks to see how the default fact tables are generated and then decide whether you would like to use the fact tables as they are or import the template to customize concept sets and/or template parameter values to generate your project specific version of the fact tables. The SDoH Variables ALL PATIENTS  template provides you with a curated set of 70 geographically-based SDoH measures tied to each patient. Because joining this data requires a valid five digit zip code, these fields are only available for patients with a five digit zip code in Level 3 (LDS) data.\n\n\n\nFigure 8.5: Example Logic Liaison Fact Table Template Knowledge Object\n\n\nThe Logic Liaisons have also developed, disseminated, and maintained a handful of overall data quality templates, ancillary fact table, and ancillary data quality templates in the Knowledge Store. Figure 8.6 depicts how one could apply the set of Logic Liaison Templates to generate augmented fact tables with a goal of asking one or more research questions within that project.\n\n\n\nFigure 8.6: Example Application of Logic Liaison Overall Quality, Fact Table, and Ancillary Fact Templates\n\n\nThe data quality templates provide a variety of data tables and visualizations. The first two of which provide a method for evaluating overall quality of the harmonized data ingested from sites.\n\nData Density by Site and Domain : Calculates the Standardized Density, Median Absolute Deviation (MAD), and Directional Median Deviations (DMD) with respect to the number of unique patient/concept/days for each of the major OMOP tables (i.e. condition_occurrence, drug_exposure, etc) and uses them to create a heatmap displaying how many MADs each site is from the median for each OMOP table. The template also scores the site’s date shifting practices.\nWhitelist Filtering : Creates a bar plot showing whitelisted data partners that have, at minimum, a certain percentage of COVID patients associated with a specified measurement, condition, drug, procedure, etc. Sites not meeting the minimum requirement are removed from the whitelist. These tables can be used in downstream filtering to keep only sites meeting the user-defined minimum data quality.\n\nOnce the main fact table templates mentioned earlier this section have been applied to generate the base fact tables, the ancillary fact templates utilize the day-level and person-level datasets of the base fact templates to efficiently generate additional derived variables based on broadly requested and applicable logic such as:\n\nVaccine Fact : Creates a vaccine fact table at the person level that summarizes their vaccination information\nStudy Specific Fact Indexing : Summarizes the indicators of a visit-level patient fact table with respect to whether they were present pre-, post-, or in a user-defined window surrounding an index date corresponding to a study-specific event. This template is similar to the All Patients Facts Tables with the exception that it is organized around a study-specific event rather than COVID diagnosis.\nCombined Variables ALL PATIENTS : Allows you to combine two variables (variable 1 and variable 2) in a visit-level table by creating a new “same-day occurrence variable” indicating that both variables appear that day for that patient. You can also choose to make an “either/or” variable in the visit-level table that combines two variables (variable 3 and variable 4) into a new variable that flags days where at least one of the input variables is recorded for that patient.\nCombined Variables COVID PATIENTS : Same as above except for patients with facts found based on their covid index date.\nCCI Score : Provides an all-time score or a before-or-day-of-covid score (depending on your selection in the template) per patient based on the CCI weights (Charlson et al., 1987).\n\nFigure 8.7 is a continuation of Figure 8.3 to demonstrate how you could continue to apply the Logic Liaison Templates to the augmented fact table created in order to answer their specific research question within a project.\n\n\n\nFigure 8.7: Example Application of Logic Liaison Ancillary Quality Templates\n\n\nThe ancillary data quality templates are intended to be applied to fact tables after cohort creation and initial variable creation to stratify question specific facts by site.\n\nSystematic Missingness by Site and Study Variable : Produces a final visualization that has a binary indicator for whether or not a site is systematically missing meaningful data for the study variables in the input dataset\nFact Density by Site Visualization : Calculates the Standardized Density, Median Absolute Deviation (MAD), and Directional Median Deviations (DMD) with respect to the numerical values in each column of the input table (any non-numerical field is converted to a binary value using the isNotNull() function) and creates heatmaps to visualize the metrics.\n\nOnce you obtain results you wish to share outside of the Enclave for a project, the results must undergo a Data Download Request before being available to export outside of the Enclave. Additional details around exporting results can be found in the Publishing and Sharing Your Work chapter (see Chapter 10). Training materials for getting started with Logic Liaison Templates are available here  within the Enclave. Outside of the main confirmed COVID+ template and a subset of columns in All Patients, these templates could be generalized to apply to research projects outside of the Enclave where studies do not necessarily need to fall within the scope of COVID-19.\nWhile it is not necessary to utilize Knowledge Store resources when conducting your research project, it does allow you to get a jumpstart on gathering and understanding the data by avoiding effort duplication and providing a general starting point. You can then build upon this fact table using the ancillary templates that allow self-definition of the index event, combining variables, generating a CCI score, and associating SDoH variables based on zip code crosswalks. The Logic Liaison ancillary data quality templates provide the same structure for analyzing data missingness, density, and contribution quality by site. Further explanation as to why these Knowledge Store objects are highly applicable can be found in Best Practices and Important Data Considerations (Chapter 9)."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-apps",
    "href": "chapters/tools.html#sec-tools-apps",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.4 N3C Enclave Applications",
    "text": "8.4 N3C Enclave Applications\nThis section will cover the usage of various applications made available in the N3C Enclave, including Protocol Pad, Contour, Code Workbooks, and more (a complete list of Foundry applications can be found here). Before designing and running an analysis utilizing data in the Enclave, it helps to understand the concepts of a “data pipeline” and a “data transform” as well as how the data are stored and accessed via Apache Spark on a distributed file system. \n\n8.4.1 N3C Protocol Pad\nN3C Documentation: Quick Start Guide  and Detailed Instructions .\n\nElectronic lab notebook\nPromote collaboration, organize work, and translation to final manuscript\nMethodology checklist\n\n\n\n\nFigure 8.8: N3C Protocol Pad Homepage\n\n\nBefore diving into an analysis, it is highly recommended that you use N3C’s Protocol Pad shown in Figure 8.8 to organize your thoughts and path forward. Research studies can span many months and pass through the hands of many team members before reaching a stage where you may want to share the results through publication or other approved means. The Protocol Pad serves as an electronic lab notebook to help organize tasks, track progress, and document results in a cohesive format throughout the process of reaching a study’s final state. As a result of this organization and tracking, the tool facilitates easy translation of the work, decisions, and contributors to producing a final manuscript if that is the goal of the protocol.\nProtocol Pad is the foundation for performing reproducible and repeatable science within the N3C Enclave. The templates, checklists, and links to key resources provided within the tool guide you along the path of well structured division of labor and use of best practices when performing research using the observational data in the Enclave. The tool also assists your team in keeping the work in alignment with the project. A more in depth explanation of how Protocol Pad can facilitate research will be available in the future.  Documentation on the tool’s functionality can be found in this Quick Guide  as well as this more Detailed Guide .\n\n\n8.4.2 Contour\nFoundry Documentation: Contour Overview\n\nProgramming-free analysis interface\nPoint-and-Click data analysis pipeline development\nData summary and visualization\nAllows customization with Contour’s expression language\nDashboard development\n\n\n\n\nFigure 8.9: Example Contour Analysis\n\n\nThe Contour application is a programming-free interface to the N3C Enclave that allows those with limited knowledge of Python, R, and SQL to create top-down analysis pipelines in a point-and-click fashion, as well as generate dynamically updated dashboards. A Contour analysis starts by specifying the path to the input dataset as shown in Figure 8.9. Subsequent transforms of that dataset are then specified, which can include adding/calculating new columns, filtering rows or columns, joining with other datasets, creating summary figures and charts, summarizing tables through pivoting, and more. Contour’s expression language also allows for more complex querying and data aggregation beyond the default operations provided. Once an analysis is complete, resulting datasets can be saved to the Enclave for use in additional Contour pipelines or other Enclave applications, like Code Workbooks. Contour can also quickly create summary figures from source tables without code, and has a variety of visualization options, including bar charts, histograms and heatmaps.\nFigures or summary tables created in Contour can be exported to a dashboard within Contour. Contour Dashboards allows chart-to-chart filtering, and an easy drag-and-drop interface to build the dashboard while iterating on an analysis. A dashboard is dynamic and interactive, allowing the reader to adjust the graphs to better explore analysis results in a guided and structured way. Figures generated in a Contour analysis can also be exported to other Enclave applications like Notepad for reporting out results. The main difference between Notepad and a Contour Dashboard is that Notepad provides a static report with figures that cannot be dynamically changed by the reader. A detailed orientation to Contour can be found in the Foundry Documentation here.\n\n\n8.4.3 Code Workbooks\nTutorial: Intro to Code Workbook \nFoundry Documentation: Code Workbook Overview\n\nGraphical organization of logic\nSimplification of code\nEasy reuse of pre-authored logic\nAdd visualizations to reports\nSupports Python, R, SQL, PySpark, and SparkR\nConsole debugging and development\nBranching facilitates collaboration and reproducibility\nWorkspace to reuse templatized logic\nExporting code to GitHub\n\nCode Workbook is a GUI-based application for you to apply code-based transformations to datasets for the purpose of creating new datasets and visualizations. The explicit goals of the application are to facilitate a collaborative environment in which you can quickly iterate over logic to produce artifacts interoperable with the suite of Enclave applications. The default Code Workbook interface is structured as a directed graph in which nodes represent either datasets or transformations that output datasets. Edges represent the flow of data through the graph such that upstream datasets are inputs for logical operations performed by downstream code transforms. Figure 8.10 shows an abstract version of a Code Workbook, while Figure 8.11 is an actual screenshot in the Enclave where data flows from the left to the right. Any dataset which you have access to within the workspace where the Code Workbook is located can be imported as an input to the various types of transformations. For each transformation one or more tables are specified as the input and are transformed into a single output table (Figure 8.10). Multiple transformations can also be strung together (Figure 8.11) to create an analysis pipeline (see Foundry’s Anatomy of a Data Pipeline module for more detailed information, as well as Chapter 9). In Code Workbook, the primary and required output of a transformation is always a single table; however, visualizations such as graphs and charts can also be generated and saved along the way. Note that the code performing the transformation and the resulting output table are always represented as a single node in the Code Workbook interface.\n\n\n\nFigure 8.10: Abstracted view of data transforms\n\n\n\n\n\nFigure 8.11: Example Data Transformation Pipeline in Code Workbook\n\n\n\n8.4.3.1 Types of Transforms\n\nManual Entry transforms allow you to manually populate a custom dataset directly in the specific Code Workbook you are currently working in as a “quick and dirty” alternative to manually populating importable or referenceable datasets with Fusion).\nPython/PySpark Code transforms contain a Python function that takes one or more datasets as input parameters and returns a single dataset as output.\nR/SparkR Code transforms contain an R function that takes one or more datasets as input parameters, and outputs a single data frame.\nSQL Code transforms let you write a Spark SQL query to create a new dataset from the available input datasets.\nTemplate transforms are parameterized blocks of reusable code which you can configure from a point-and-click interface. Many code templates are available in the Knowledge Store, but you can create your own templates scoped to their workspace. Multiple single-node templates can be combined to create a multi-node template to allow reuse of entire configurable pipelines.\nVisualize transforms offer a point-and-click interface for you to create figures and pivot tables. Note this transform is only available for use on saved datasets; it cannot accept unsaved transformations as an input parameter.\n\nBoth Python and R transforms can optionally return a single dataset and produce visualizations using Python libraries/R packages. Any visualization produced in a Code Workbook can subsequently be embedded in a Notepad document. Datasets returned by a transform are ephemeral by default, that is, the transform must be recomputed each time the dataset is used as a downstream input, but options exist to conserve compute power by either caching or saving the output. Caching stores the output temporarily, while saving the dataset stores it permanently in the Enclave. It is recommended that transforms in a pipeline requiring significant compute be saved as datasets to reduce iteration time during development. In addition to manually clicking “Run” on a transform, a build schedule can be defined to recompute it at regular intervals based on a trigger. You can choose the trigger to be the successful update of an input dataset or simply the elapsing of a specified period of time. This capability can best be visualized using Data Lineage.\nThe Console, Global Code, and Logs panels facilitate quick iteration and code development and debugging. The Console allows you to interactively execute and check syntax in Python, R, or SQL outside of a transform node. Global Code allows you to define custom functions available to all transforms in the Code Workbook. Each code transform includes a Logs panel to view console output generated by code and to view detailed stack traces when transforms fail due to an error.\nMany Python and R transforms rely on external libraries and packages which can be made available via the Environment Configuration. The Enclave provides default configurations tailored to common use cases. For instance, the default profile includes common packages like pandas and tidyverse for routine analysis, whereas the profile-high-memory profile includes packages like foundry_ml for machine learning. You can create your own Environment Configurations which include packages meeting your project’s specific needs.\nNot all libraries and packages are included in the list of options, but you can submit a ticket within the Enclave by clicking on Help & Support &gt; Help Center &gt; Report an Issue to request additional packages be made available. The Enclave maintains instances of the default configurations on warm standby, allowing them to be quickly initialized when you request a new environment. Custom configurations require more time for initialization as instances of these must be started from scratch rather than merely assigned. For this reason, it is recommended to use default configurations when possible.\nFollowing best practices for collaborative software development, Code Workbook allows for branching of the logic within a workbook. As with other popular version control technologies (i.e. Git), branching allows you to make copies of a workbook which your team members can develop independently of the source workbook. Once the development in a particular branch is deemed complete, it can be merged back into the originating branch. Prior to the merge, you can preview both line-level differences within each node, as well as node-level differences of nodes that have been added/removed. Good practice dictates that you perform all development on individual branches, which are then merged back into a common master branch.\nBecause the master branch can change in the interval between you creating a branch and merging it back in, it is important to preview merge changes to ensure that the branch’s contributions are both correct and compatible with the current state of the master branch. However, note that your branch will be automatically deleted after it is merged, which is an important difference from the normal Git behavior. Another prime use case for code branching is to ensure the reproducibility of a given dataset used in a research project. Because the OMOP and N3C-curated datasets are also versioned, you can create a code branch in which all input datasets are set to the same version release (as shown in Figure 8.12) to effectively freeze a dataset used in a specific analysis for later reproducibility while still allowing the possibility of adding additional features. User-generated datasets are set to the same branch as the Code Workbook in which they were created. Finally, Code Workbooks allow you to collect and download all coded transformations within a single workbook into a Git repository that can be easily uploaded to GitHub for public dissemination.\n\n\n\nFigure 8.12: Pinning to an input dataset’s release version\n\n\nPalantir has created extensive documentation of the Code Workbook application including tutorials. N3C has also published training materials .\n\n\n\n8.4.4 Code Repositories\nFoundry Documentation: Code Repository Overview\n\nProduction pipelines\nCode reuse across projects\nBuilt in version control\n\n\n\n\nFigure 8.13: Example Code Repository\n\n\nCode Repositories shown in Figure 8.13 are available within the N3C enclave and should be used when you wish to share code across multiple Code Workbooks or projects, or need to develop a robust production pipeline. Code Repositories do not support data visualization, coding in R, or point-and-click templates. Additional differences between Code Repositories and Code Workbooks can be found in the Foundry Documentation. Palantir has also compiled several tutorials on how to create, publish, and maintain Code Repositories .\nFor any large analytic project, there are many pieces of code and other artifacts that should be shared between different components within the project. Some code is useful enough that it should be shared across projects. Code Repositories provide a standard mechanism for encapsulating reusable components and allowing them to be reused within projects.\nUnderlying the Code Repository is the Git version control system. You can edit and maintain code using all of the capabilities of Git including comparing versions, branching and pull requests. Versions that are tagged in Git will be automatically published in the Enclave as a shared library, which allows you and others that have access to your project space to import that code into other Code Workbooks (note this requires you to create a customized environment that imports your code specifically, which may increase workbook initialization time). If you want to make your repository public so others outside of your project workspace can use it you can do one of the following:\n\nPackage your code into a properly structured python package. This allows other researchers from different project spaces to run it from their Code Workbooks or Repositories.\nSubmit the Code Repository to the Knowledge Store where it will then be visible to anyone who has Enclave access.\nPublish the Code Repository to a public Github where the code will then become accessible to anyone outside the Enclave.\n\nMore information on Code Repositories can be found in the Foundry Documentation (Palantir, 2023).\n\n\n8.4.5 Fusion\nFoundry Documentation: Fusion Sheet Overview\n\nUseful for writing back datasets for use within the Enclave\nLeverage cell references and spreadsheet functions\nSync tables to a dataset to use in other Foundry applications\nCreate charts\nAllow customization and flexibility\n\n\n\n\nFigure 8.14: Example Fusion Sheet\n\n\nFusion, shown in Figure 8.14, is a spreadsheet application within the Enclave analogous to Microsoft Excel or Google Sheets. Palantir provides extensive documentation. Fusion allows you to sync specific cell ranges within a spreadsheet to Spark datasets, which can subsequently be imported into any other Enclave application. Fusion is an excellent option for use cases which require manual data entry, such as curating lists of concept sets (see Chapter 7) to configure the Logic Liaison Fact Tables and Templates. Unlike many other Enclave applications, Fusion is not suitable for large datasets; each document has a maximum size of 50 MB. Similar to Google Sheets, multiple users can simultaneously view and edit the same document.\nFusion provides many features familiar to other spreadsheet applications such as cell-referencing formulas, formatting, and a charting library to name a few. While you cannot directly import external .xls/.xlsx formatted files into the Enclave, you can copy/paste external dataset values into Fusion. For example, you can copy and paste concept IDs from ATLAS to use in an analysis or metadata manually curated for a dataset. To import larger external datasets into the N3C Enclave see Chapter 7, and N3C procedures and protocols around importing large external datasets. In addition to standard spreadsheet functionality, Fusion has additional features which allow it to integrate with the rest of your Enclave environment. Objects created within Fusion, such as formatted tables, can be embedded in Notepad. Finally, Fusion sheets can be templatized to facilitate replication of similar functionality.\n\n\n8.4.6 Notepad\nFoundry Documentation: Notepad Overview\n\nNote taking with ability to add embeds of the workflow\nDocumentation of pipelines or datasets\nCreate report templates\nMonthly status reports using template function\nUnable to create dashboard or complex page-based text editing\n\n\n\n\nFigure 8.15: Example Notepad Document\n\n\nMany research projects in the Enclave are complex, involving multiple summary datasets, statistical analyses, and visualizations scattered across multiple applications and documents. Notepad is a tool that is often used for consolidating various research artifacts from multiple sources within the Enclave into a single coherent document as shown in Figure 8.15. Formatted Fusion tables, Contour charts, Python/R-generated images from Code Workbooks, and more are all embeddable in a Notepad document, with the option to add a title and caption for each artifact. Users can also create sections and provide narrative structure to their documents using Markdown. A Notepad document can be arranged and configured using a point-and-click interface.\nAll embedded objects can be configured to remain static or refresh automatically when the underlying data sources update. Notepad is also useful for annotating documents, presenting an executive summary of results for internal stakeholders, or external presentations after being approved for download request and export as PDF. Logic Liaison Templates in the Knowledge Store generally includes a README which is created using Notepad. The tool does have limitations in that it cannot be used to create dashboards that include chart-to-chart filtering; however, Contour Dashboards can provide this feature for tabular data and Quiver Dashboards can provide this feature for object or time series data. Palantir has curated documentation for creating and editing Notepad documents. Palantir also has documentation for their application known as Reports that was previously used in a similar fashion though with less functionality compared to Notepad.\n\n\n8.4.7 Data Lineage (aka Monocle)\nFoundry Documentation: Data Lineage Overview\n\nFind datasets\nExpand or hide a dataset’s ancestors and descendants\nVisualize a data pipeline and its details\nFacilitate dataset build scheduling\n\n\n\n\nFigure 8.16: Example Data Lineage Visualization\n\n\nWhether you’re creating a data pipeline for your research project or investigating one from the Knowledge Store, you’ll likely want to holistically assess the dataset’s origins with the Data Lineage tool shown in Figure 8.16. The data pipeline flows from left to right, which is an intuitive way to visualize the relationships between datasets and their ancestors or descendants. Views are enhanced with color-coding and grouping. The Data Lineage tool allows you to view details such as a dataset’s schema, last build datetime, and the code that generated the dataset. You can use this lineage tracing tool to understand and verify the data curation methods when using Knowledge Objects and other shared datasets as part of their study analysis. The Data Lineage tool also allows you to see upstream dataset(s) aka potential triggers and downstream dataset(s) aka potential targets for setting up dataset build schedules. Foundry Documentation provides additional instructions and descriptions of Data Lineage.\n\n\n\n\nCharlson, M. E., Pompei, P., Ales, K. L., & MacKenzie, C. R. (1987). A new method of classifying prognostic comorbidity in longitudinal studies: Development and validation. Journal of Chronic Diseases, 40(5), 373–383. https://doi.org/10.1016/0021-9681(87)90171-8\n\n\nPalantir. (2023). Documentation: Code repositories overview. https://www.palantir.com/docs/foundry/code-repositories/overview/.\n\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., et al. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3. https://doi.org/10.1038/sdata.2016.18"
  },
  {
    "objectID": "chapters/practices.html",
    "href": "chapters/practices.html",
    "title": "9  Best Practices and Important Data Considerations",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1ExkYChsnO3hYZk6HCI5cEfQdQJ9F-ynw\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-committee",
    "href": "chapters/publishing.html#sec-publishing-committee",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.1 Publication Committee Process",
    "text": "10.1 Publication Committee Process\nThe Publication Committee is a multidisciplinary team that meets on a weekly basis to evaluate abstracts and manuscripts prior to submission for peer review (and / or preprint). The committee’s primary goal is to ensure that authors comply with N3C policies and procedures. This chapter describes attribution and publication principles regarding community dissemination of research resulting from N3C.\nThe guiding principle of the Publication Committee is to be a steward of the following critical components of the N3C:\n\nData partners and individual patients: Assure that the publication is:\n\nConsistent with the approved Data Use Request (DUR)\nConsistent with policies to protect the identity of individuals, data partners, and marginalized groups\n\nIndividual consortial contributors: Assure inclusive and appropriate attribution of the work that went into the data elements used for a given analysis or publication. Many individuals contribute time and expertise into building the N3C, and it is important to acknowledge their work. Thus, a tiered approach to authorship has been developed (see Table 10.1).\nN3C collaborative: Assure that information about the N3C is represented accurately\nAuthors: Serve as a resource for authors\n\n\n\nTable 10.1: Authorship tiers designed to accurately represent publication contributions\n\n\n\n\n\n\n\nTier 1:Masthead Authorship\nTier 2:Consortial Collaborators\nTier 3:In Text Acknowledgements\n\n\n\n\nSubstantial intellectually important contributions made directly to the manuscript\nLess substantial but still intellectually important contributions made directly to the study/manuscript or to knowledge artifacts used directly by the study**\nImportant contributions to the N3C infrastructure\n\n\n\n\nThe first two tiers are indexed in Medline, which is an important way for contributions to acknowledge those who made them. Tier 1 and 2 authors must satisfy ICMJE criteria:\n\nThe ICMJE recommends that authorship be based on the following 4 criteria:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or revising it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\nThe publication committee has a role in helping authors identify Tier 2 and Tier 3 contributors:\n\nNotify core contributors (named in covid.cd2h.org/acknowledgements) and relevant domain teams of manuscripts that might be using their work\nEnable core contributors and relevant domain teams to review the manuscript and either opt out of acknowledgement or opt into consortial contributorship. Opting into consortial contributorship requires meeting the four ICMJE criteria above\nAny other invited individuals (eg.domain team etc) would opt into acknowledgement OR opt into consortial contributorship; the latter requires meeting the four ICMJE criteria above.\nThe publication committee then informs the author of any responses to consortial-level contributions or changes to default acknowledgments.\n\nThe publications committee helps to maintain the list of core contributors, and progress is underway to automatically track contributions directly within the Enclave, which aids this process.\nThe publication committee also maintains the content on https://covid.cd2h.org/acknowlegements to be included in the acknowledgements section of each manuscript. An abridged version of the acknowledgements is available for use in abstracts, posters, and slides."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-drr",
    "href": "chapters/publishing.html#sec-publishing-drr",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.2 Result Download (aka export) Request Process",
    "text": "10.2 Result Download (aka export) Request Process\n\n10.2.1 Policy summary\nIn compliance with the NCATS Data Transfer and Data Use Agreements that all Enclave users agree to, under no circumstances are tables, figures, parameter estimates, or aggregated statistics to leave the Enclave until download (aka export) approval is obtained by the ResultDownload Committee. Prior approval is required for export regardless of the result format (tables, figures etc.) and regardless of the target venue (manuscripts, posters, presentations, supplementary material, or even your own private hard drive). Prior approval is also required regardless of the export mechanism (screen shot, copy/paste, or download).\n\n\n10.2.2 Constraints with aggregated data pertaining to small groups of patients (aka small ‘cell sizes’)\nTo protect patient’s data confidentiality, aggregations pertaining to fewer than 20 persons, unless the value is zero, must be identified with a symbol (best represented as &lt; 20 with a corresponding statement that group size below 20 has been obscured in accordance with the N3C Data Use Agreement). Users may apply for an appeal to reduce the reportable threshold down to 10 (via the Enclave-external help desk, see Chapter 11). A strong scientific rationale for this must be provided. NCATS makes the final decision about whether to approve the appeal. In no cases may group sizes below 10 be considered. Authors must include an asterisk or footnote in their manuscript to indicate that the values as low as 10 are reported with approval from the N3C Download Committee.\nExample: Consider the following fictional table, which reports several aggregations of fewer than 20 patients in the ‘other’ gender category, race (AI/AN, black), and in comorbidities (Dementia, HIV). In their original form (left) these would violate the policy, as there are several values reported below 20. Even if these values are simply replaced with &lt; 20 (middle), the masking is still insufficient, because marginal totals can be used to calculate some of their contents. A fully-masked example is shown on the right, modifying the reported values to satisfy the policy constraints, and including a note of the changes made.\n\n\n\nFigure 10.1: An example of summary results that require masking prior to export for publication. To comply with N3C policy, counts below 20 are displayed as &lt; 20, and in this case additional values must be skewed by up to 5 in order to render it impossible to back-calculate precise counts fewer than 20 in the ‘other gender’ and HIV categories.\n\n\n\n\n10.2.3 Constraints related to Native Populations\nNCATS has engaged in extensive discussion with the NIH Tribal Health Research Office and developed data use plans based on Tribal input following formal Tribal Consultations. As of September 2022, data from individuals who self-identify as American Indian (AI) or Alaska Native (AN) are indicated in race and ethnicity distribution.\n\nAll zip codes of regions representing rural populations of 20,000 persons or less are fully obscured to ‘00000’\nAll zip codes of regions of more than 20,000 persons that overlap with Tribal communities or where the majority of residents identify as AI/AN are identified by the first 3 digits. For example, if a ZIP code of “01234” represents a predominantly AI/AN community, the user will see only a partial ZIP code of “012”.\n\nThe N3C Data User Code of Conduct was revised to clarify that assumptions about Tribal affiliation are not valid or appropriate. For more information, consult NCATS Tribal Consultations.\n\n\n10.2.4 Constraints related to Data Partner IDs\nFor any exports that include Data Partner IDs, these IDs must be masked before being published and a statement that these have been masked must also be included. You may consider assigning random labels to data partners, such as four digit codes which do not occur in the partner IDs."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-tech",
    "href": "chapters/publishing.html#sec-publishing-tech",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.3 Technical Considerations",
    "text": "10.3 Technical Considerations\nIn addition to processes and policies described above, there are technical requirements for exporting summary data and figures from the Enclave, and features you should be aware of as your project nears publication.\n\n10.3.1 Pinning to a Release\nAll datasets in the Enclave are capable of being versioned with branches, meaning that previous versions of a dataset remain available under a different branch name. N3C uses this mechanism to provide researchers access to the most recent version of the data via the ‘master’ branch, which is selected by default when importing datasets to Code Workbooks or Code Repositories. For example, consider the following Code Workbook, into which we’ve imported the Synthea notional condition_era table.\n\n\n\nFigure 10.2: Selecting the branch for a dataset imported into a code workbook. Branches allow researchers to ensure that the same data is used for further analysis.\n\n\nClicking on the “Branch” tab of the dataset node shows that the master branch is currently being used in this workbook. Instead of master, we could use the dropdown to select another branch–on real N3C data, these branches will be named like Release-v98-2022-10-27. (We call them releases because new versions are ‘released’ periodically after quality checks and harmonization.) Selecting such a branch will import the release of that table as of the date selected, effectively ‘pinning’ the dataset to a point in time. The master branch is configured to always match the latest release, and thus changes over time.\nWhen working with a Code Repository, the branch can be selected as a parameter to the Input entry in the @transform decorator, for example:\nsource_df = Input(\"/UNITE/LDS Release/datasets/condition_era\", branch = \"Release-v98-2022-10-27\")`\n\n\n\n\n\n\nNote\n\n\n\nAfter changing the branch of an imported dataset, you will need to re-compute downstream results for their output to reflect the new input.\nAdditionally, all primary N3C tables are versioned with similar release names; if you are using both the condition_era and drug_exposure tables, you should select the same branch for each. It is not recommended to mix data from different releases (or a pinned release with the master branch), because they may contain incompatible data–each N3C release is designed to be a self-contained set of datasets.\nIn fact, because downstream results are only updated when they are explicitly run, you may wish to avoid using the master branch altogether, or run the risk of one set of different sets of results being based on different releases, depending on when they were run.\n\n\nWhy is pinning to a release helpful? Because the default master branch is continuously being updated, analysis results based on it will change over time along with the underlying data (if they are re-run). This becomes cumbersome when writing about results!\n\n\n10.3.2 Download Request Process\nAll research results derived from N3C data–including summary tables, figures, and logs–must be reviewed to ensure they don’t inadvertently leak any patient-level data. The review will check for the policy requirements described above (such as cell sizes less than 20 being masked), and when approved you will be able to download tabular data as a comma-separated-values (CSV) file, images as PDF or PNG, and logs as plain text.\nThe submission and export process is described in detail in the How to download results outside the Enclave training module , but we’ll provide a brief overview here.\n\n10.3.2.1 Submitting a Download Request\nThe download request dashboard may be accessed from the Enclave homepage under “Download Dashboard.” This interface shows your previous requests, and you can submit a new one via the “Submit a new Download Request” button.\n\n\n\nFigure 10.3: The download request dashboard, where researchers can submit requests to export summary data and figures and see the status of those requests.\n\n\nWhen submitting a new download request, you will first be asked to complete a short quiz to ensure you understand the requirements. Subsequently, you will be prompted to enter information about the request, and select one or more “Resources” that you wish to export. A single download request may include multiple resources, such as multiple summary tables saved as datasets. To export figures you should include them in a Report, which can be done from either the Code Workbook or Contour interfaces. Desired log information (such as textual results from statistical tests) should also be copied and pasted into the report so that it can be reviewed.\n\n\n\nFigure 10.4: The download review request form.\n\n\nIf you need the review request to be expedited, you have the option of entering a Need By Date and providing a justification.\n\n\n10.3.2.2 Downloading an Approved Request\nWhen a request is approved, behind the scenes a read-only copy of the materials you requested is created in a location that you will have access to download from. This is to prevent modification of export materials post-review.\nWhen your request is approved, you will see it listed as Approved in the Download Dashboard (see above). Each approved request is provided with a DRR ID, for example “DRR-E5C4B6C” shown above. If we scroll down further in the specific request, we can use the “View all…” link to see the individual approved “Resource(s) To Download”:\n\n\n\nFigure 10.5: A view of exportable datasets and a report from an approved download request.\n\n\nTo export one of the datasets as CSV, we can open it by clicking on it, and in the resulting new tab select “Download as CSV” from the “Actions” menu - this will prompt you to enter the corresponding DRR ID, after which the file will be downloaded to your local computer.\nExporting figures and logs stored in Reports can be done in multiple ways. After opening the approved report from the list of resources to download, you can select either “Export to PDF” or “Export to PowerPoint” from the “Actions” menu. Again you will be prompted to enter the DRR ID before the file download begins. A common issue with “Download as PDF” is that figures produced in Code Workbooks may be lower resolution than expected.\n\n\n\n10.3.3 Exporting Code to a Git Repo\nWhile code written inside the Enclave is generally not executable in other contexts (a result of the proprietary execution environment), exporting code for review and publication can be an important part of computational science. Fortunately, both Code Workbooks and Code Repositories support cloning as git repositories, which can then be mirrored to public git hosting like GitHub or BitBucket. This publication step is completely optional and may not even be applicable to all work.\n\n\n\n\n\n\nNote\n\n\n\nGit repositories cloned from the Enclave are read-only; changes made to the locally cloned copy cannot be pushed back to the Enclave. You can however pull new changes made in the Enclave, see below.\nUnlike summary tables, figures, or other results derived from N3C data, N3C does not require review of code prior to export. However, you should be careful to ensure that the code does not include any sensitive information added by hand. The most common use case for such hard-coded data are data_partner_id values added to filter a data partner’s data. These should be removed or masked to comply with N3C policy.\n\n\nCode Workbook code can be exported via the “Gear” icon in the interface, under “Export git repository.” Selecting this item will open the workbook in a new browser tab, with a new “Export Code Workbook” panel on the right, providing the git command to use to clone the repo to your local computer. As described in the panel text, the link contains an authentication token that you should keep private.\n\n\n\nFigure 10.6: Code workbooks can be exported as Git repositories for external publishing. Researchers must ensure that no identifying information, including pseudonymous identifiers like data_partner_id values, are present in the code prior to export.\n\n\nThe resulting git repo will contain three files, pipeline.R, pipeline.py, and pipeline.sql, containing all of the workbooks’ R, Python, and SQL transform code respectively. These files are not very reader-friendly, and as of this writing N3C is working on parsing tools to help researchers publish their work in an accessible manner.\n\n10.3.3.1 Pulling Changes, Pushing to GitHub, Branches, and Code Repositories\nWhile it is not possible to push changes made locally up to the Enclave, it is possible to pull updates made in the Enclave with a simple git pull. However, updates are not pulled unless some action is taken within the workbook to commit them prior to pulling. Simply editing the code for a transform is not enough, but executing a transform or adding a new transform will commit the current state of the workbook so that it can be pulled.\nOnce you’ve pulled the latest version of your workbook locally, you likely will want to push a copy up to a public repository such as GitHub. The recommended way to do this is to create a new empty repository in GitHub (or your git hosting service of choice) with the same name as the workbook, and set it up as a push-only remote repository. For example, for a workbook named example_workbook, one would create a new GitHub repository named example_workbook, and in the local copy run:\ngit remote set-url --add --push origin https://github.com/&lt;username&gt;/example_workbook.git\nA subsequent git push will sync the state of the remote repository. You will of course need the appropriate permissions to push a repository to GitHub, via a Personal Access Token or some other means.\nIf you have created one or more branches in your code workbook, these are treated as regular git branches and so can be synced locally with git fetch and git switch &lt;branch-name&gt;. A subsequent git push will push the new branch to GitHub as well.\nCode Repositories work much the same way. The primary differences are that\n\ncommits to a repo in the Enclave are made manually, rather than automatically when a transform is run or created, and\nthe Clone button provides only the git URL with the embedded token, you will need to use this URL in combination with git clone or other git commands as usual. Code Repos similarly support branching.\n\n\n\n\nFigure 10.7: Code workbooks can also be exported as Git repositories for external publishing. Researchers must ensure that no identifying information, including pseudonymous identifiers like data_partner_id values, are present in the code prior to export.\n\n\n\n\n10.3.3.2 Python Libraries and Code Repositories\nCode Repositories are less frequently submitted to the Knowledge Store, but they provide a feature for sharing code that Code Workbooks don’t: it is possible to author a Python library in a Code Repository that can then be imported for use in either another Code Repository or a Code Workbook by any N3C researcher. One such example is the Semantic Similarity Python Library . Authoring Python libraries is different from authoring Code Repositories that transform data, and is covered in the official documentation, as is utilizing such libraries in Code Repositories. Using them in Code Workbooks requires updating the workbook environment to include the library, just like with any other Python or R library you might like to use.\nIn fact, Python libraries published this way are automatically usable by others in N3C without submission to the Knowledge Store, but you should still submit the repository itself along with documentation as a Knowledge Object for discoverability in N3C. If you run into issues or have questions, be sure to submit an Enclave-internal support ticket or visit office hours.\n\n\n\n10.3.4 Exporting Concept Sets\nAs discussed in Chapter 7, concept sets are sets of OMOP concept_ids representing clinical concepts. These are organized and curated as shared resources in N3C, see the section on the Concept Set Browser for details.\nWhen publishing work utilizing a concept set, you will likely want to download the concept set in a tabular format for inclusion in supplemental materials. Fortunately, the concept set browser supports this functionality–when viewing a specific version of a concept set, the “OMOP Concepts” tab lists the individual concepts and provides an “Export list of Concepts as Excel.” The provided Excel export includes the concept IDs and other relevant information.\n\n\n\nFigure 10.8: The N3C Concept Set Browser supports exporting a concept set version as an Excel spreadsheet for inclusions in supplementary materials. Concept sets are referenceable via DOI (see text).\n\n\nSome concept sets, notably those properly reviewed and marked N3C Recommended, are already published externally and referenceable via a DOI at the N3C Zenodo community."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-ks",
    "href": "chapters/publishing.html#sec-publishing-ks",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.4 Submitting to the Knowledge Store",
    "text": "10.4 Submitting to the Knowledge Store\nFor security reasons the permissions around project workspaces are very tight; it is not possible for a researcher to share data, code, or other resources with others outside of the project workspace. While necessary, this prevents code sharing and re-use amongst the many analysts working in the Enclave. The N3C Knowledge Store provides an authorized mechanism for researchers to share code, and datasets derived via code, with other researchers (provided they also have access to the correct input datasets in the case of shared datasets). Because the Knowledge Store is internal to the Enclave and protected to those who have the authorization to view row-level data, the publication is not involved in monitoring or approving the contents.\nIf you’ve written an analysis pipeline that would be of benefit to others, you should consider submitting it to the Knowledge Store for others to find and use. Details on this process are covered in the Knowledge Store Guide  training module, but we’ll give an overview here. Note that the process can be cumbersome and sometimes requires administrator help–be sure to submit an Enclave-internal support ticket or visit office hours if you run into any issues or have questions.\nCode sharing within N3C is usually accomplished by sharing Code Workbook “templates” in the knowledge store, which provide a set of workbook nodes that can be imported into a workbook and configured via parameters for the end user. Authoring workbook templates is covered in the official documentation. Templates are typically accompanied by a README (as a Report) with documentation, an example workbook using the template on N3C data, and potentially the derived dataset created by the example workbook. We suggest placing these resources in a single folder prior to submitting them to the Knowledge Store.\nTo submit your new “knowledge object” to the store, first find the “Publish New Knowledge Object” item under the … menu in the upper right of the Knowledge Store:\n\n\n\nFigure 10.9: The N3C Knowledge Store is a repository of community-developed code and derived datasets for use by other researchers with sufficient access.\n\n\nThis will open a form where you are able to select your knowledge object (you should select the folder containing your template and other materials) and add details such as a title and description.\n\n\n\nFigure 10.10: Knowledge Store contribution form.\n\n\nOnce the form is completed the submission will be sent to a queue for system administrators to finalize the creation of the knowledge object (which requires creating read-only copies in a place where other researchers can access them) and listing them in the store. Depending on the complexity and goals of your template, administrators may help create multiple versions of your example workbook with pre-configured access to different levels of N3C data and set up a schedule so that they produce new derived datasets whenever a new release of N3C data is available. This allows other researchers to use the computed datasets directly, or import the template for customization."
  },
  {
    "objectID": "chapters/support.html#sec-support-internal",
    "href": "chapters/support.html#sec-support-internal",
    "title": "11  Help and Support",
    "section": "11.1 Support Tickets: Enclave-Internal",
    "text": "11.1 Support Tickets: Enclave-Internal\nWhile live-support options are available, submitting questions via “tickets” (also known as “issues” in the enclave) helps ensure they reach the right person and that questions are logged and tracked. Given the sensitive nature of N3C data, questions that pertain to patients or data partners should be asked within the enclave itself.\nThe within-enclave support ticket system is also a good avenue for technical questions, including about platform features, performance, permissions, and tooling. In fact, when submitting a ticket in the enclave, the ticket itself will automatically track the resource being viewed when the ticket is submitted.\nTo illustrate an example, we first navigate to the Synthea Notional Data entry in the Data Catalog (under “Projects & files” in the left navigation menu).\n\n\n\nFigure 11.1: Navigating to Synthea\n\n\nNext, we’ll open the condition_era table which displays a preview in the Dataset Preview application. Let’s suppose we have a question about this data, or perhaps have discovered a potential data quality issue.\n\n\n\nFigure 11.2: Synthea Preview\n\n\nTo submit a ticket about the currently opened dataset, we’ll open the Help menu near the top, and select “Report Issue.”\n\n\n\nFigure 11.3: New Issue\n\n\nThis opens a dialog requisition information about the ticket. Notice that the RESOURCE is identified as the condition_era table we had opened. Since we are asking a question about the data, we’ll select “Data quality.”\n\n\n\nFigure 11.4: New Issue: What kind of help do you need?\n\n\nOnce we click Next, we’ll be prompted to change the resource of interest or application being used (if desired). Since we are reporting an issue on a dataset, we even have the option of selecting the specific column we are interested in. We’ll just click Next here.\n\n\n\nFigure 11.5: Share some details\n\n\n\n\n\n\n\n\nReporting Issues via Help Center\n\n\n\nUsing “Report Issue” from the Help menu of an enclave application is the preferred way to submit a ticket, as this option keeps the best track of the resource being reported from. While most enclave applications have a Help menu near the top left, not all do. In these cases you can alternatively submit an issue by finding the “Help & support” option in the lower part of the left navigation bar and choosing the “Help Center”. This will open a sidebar to the right, with a large blue button at the bottom for “Report an Issue.”\n\n\nFinally, we are prompted to submit our issue, including a title and description with pre-filled questions depending on the issue type selected. Answering all of these is not required, but any information you can add that speaks to them is helpful. This section also allows you to upload a screenshot if desired. Even though these issues are protected in enclave, you should not screenshot any data (or results like summary tables or figures), as that would result in your local computer storing, even if temporarily, unapproved patient-level information. Nevertheless, when excluding patient data is possible, a screenshot may help diagnose the problem, and the support personnel who respond to the issue may request a screenshot during follow up.\n\n\n\nFigure 11.6: More details\n\n\nWe can scroll down in this panel to see more advanced information pertaining to the ticket. Priority should generally be left to “Medium,” since “High” priority is used to alert infrastructure support of system-wide issues or outages likely to affect a majority of users. The default assignee is the “N3C: Issues Triage Team,” who will further route the ticket to the appropriate support group (issues are triaged most business days, but follow up from support may take longer). Followers allow you to specify other users who will receive alerts about this issue. Adding labels to the ticket is optional as well, since the triage team usually applies relevant labels for tracking purposes.\n\n\n\nFigure 11.7: Submit\n\n\nOnce we click Submit and refresh the browser page, we’ll see that a new “warning” icon has been added to the interface indicating that the resource now has one or more open issues relating to it, and it can be clicked on to open a menu with details. This warning will also show for other users who open the resource, and it will show in the file browser for this dataset. Reporting issues about datasets from the datasets themselves is thus a mechanism for alerting support teams and other N3C researchers about potential data quality issues. The same principle applies to other resource types like Code Workbooks, in cases where multiple researchers are working with them.\n\n\n\nFigure 11.8: Recent Issues\n\n\n\n11.1.1 Issue followup\nAfter your ticket is submitted, it will be routed to a triage team who will decide which support group is best able to address it. These include groups with admin-level access and general knowledge of the enclave (at least one of which is familiar N3C-specific tools or workflows), experts in N3C data ingestion and harmonization processes, as well as individuals with expertise in OMOP and other N3C technologies.\nWhen activity occurs on your ticket you will see a small orange dot appear on the Notifications navigation menu item indicating you have a new notification, clicking this will show this notification (and any others you may have received). By default, you will also receive email notifications for ticket activity. This can be configured under your account preferences (the Account item in the left navigation menu). Finally, you can review and respond to tickets via the Issues application (which may be hidden for you in the left navigation bar under “View all apps”)."
  },
  {
    "objectID": "chapters/support.html#sec-support-external",
    "href": "chapters/support.html#sec-support-external",
    "title": "11  Help and Support",
    "section": "11.2 Support Tickets: Enclave-External",
    "text": "11.2 Support Tickets: Enclave-External\nWhile the enclave-internal ticket system is a good avenue for more technical questions about data analysis or the data itself, most other questions should be directed to an enclave-external ticket system (sometimes called the “support desk”). At the very least, if your issue is you cannot login to the enclave, that cannot be reported via the enclave-internal ticketing system!\n\n\n\nFigure 11.9: Starting a ticket\n\n\nThe external help desk can be found at https://covid.cd2h.org/support. Here you will find a link to “Submit a Support Request” that directs you to select the kind of support you need.\n\n\n\nFigure 11.10: Ticket details\n\n\nEach of the options are described, and range from enclave access support (commonly used for login issues), Domain Team creation or support, questions about Data Use Requests or the Data Access Committee (commonly used to check on DUR review status), PPRL data, and “everything else.” In general, this help desk is staffed by a broader range of core N3C administrators, and so is generally the best option outside of technical or data questions.\nAfter selecting a support area, you will be given the option to select sub-categorizations, enter a description of the issue or question, provide a summary title for tracking and select the user (usually you) submitting the request. The list of users is pre-populated based on N3C data, but you can also type an email address in the same field.\nOnce submitted, you will receive an email with a link to the ticket. You can use this link to make further comments, or do so by replying to the email directly."
  },
  {
    "objectID": "chapters/support.html#sec-support-office",
    "href": "chapters/support.html#sec-support-office",
    "title": "11  Help and Support",
    "section": "11.3 Office Hours",
    "text": "11.3 Office Hours\nN3C hosts office hours on Tuesdays and Thursdays of most weeks, at 10a PT/1p ET. The join link can be found at https://covid.cd2h.org/support. All are welcome to join, from experienced N3C analysts looking for help with complex machine learning implementations, to brand new researchers needing help finding their project workspace. Experienced N3C volunteers are on hand and able to help with most questions. They can also refer you to external resources, or suggest submitting a ticket when appropriate. Some researchers join just to watch and learn. To satisfy N3C data privacy rules, N3C staff utilize Zoom breakout rooms allowing researchers to share their screen only with others who have the same level of access."
  },
  {
    "objectID": "chapters/support.html#sec-support-training",
    "href": "chapters/support.html#sec-support-training",
    "title": "11  Help and Support",
    "section": "11.4 Training Resources",
    "text": "11.4 Training Resources\nMany training and educational resources are available within the enclave where we can readily organize and link them to relevant resources. The “Training Material” button on the enclave homepage displays several categories of training materials:\n\n\n\nFigure 11.11: Training Material\n\n\nWhile the documentation and self-guided tours provide information about the cloud-based enclave platform, they don’t provide any information specific to N3C. The Training Portal is the primary location for N3C-related training materials, while N3C Community Notes allow researchers to post short articles/guides for others to use. The Support option will redirect to a page linking to the two ticket systems described above.\n\n11.4.1 Training (Training Portal)\nThe N3C Training Portal hosts training “modules.” The list of training modules is roughly sorted by researchers’ N3C journey–those new to N3C will likely find the first modules of most interest, while those preparing to publish their results should scroll to the end.\nModules are searchable by keyword (from their title and description), and a brief list of Suggested Modules can be found in the orange button in the upper-right, though browsing through the full list is recommended.\n\n\n\nFigure 11.12: Training Modules\n\n\nThe Training Portal also has a Paths View, which shows potential learning paths of interest. These links are not formally assigned, and act more like a recommendation system to help navigate and find modules and resources of interest. This interface is limited in the number of items it can display, so you may want to filter using the “Starting Module Category” dropdown.\n\n\n\nFigure 11.13: Learning Paths\n\n\nOpening a module from the main list view reveals an overview of the module, including title, description, topics, learning objectives, suggested background, and estimated time to complete. Immediately below the title is a link whose URL points at this specific module in the portal for sharing.\n\n\n\nFigure 11.14: Overview of a Training Module\n\n\nTo the right is a list of resources comprising the materials of the module; these may be videos or documents, example enclave resources like code workbooks, or in some cases links to relevant external resources. The small search box allows you to filter the list, and is especially useful for modules with many resources such as our Enclave Users’ Group series discussed below.\nN3C community members are welcome to suggest or develop new training modules for inclusion in the portal. Several have been developed this way, and each module tracks authorship information. To contribute to the training portal or other N3C-related education and training efforts, just contact the Education & Training Domain Team at https://covid.cd2h.org/ET-DT.\n\n\n11.4.2 Self Guided Tours (Academy)\nThis platform feature provides step-by-step walkthroughs of individual tools like Contour and Code Workbooks. The Foundry 10X and 20X series are recommended and cover the basic tools researchers will encounter. Along the right individual steps walk you through an example workflow or analysis. Note that because these tours are not developed by N3C, the example analyses and data will not be N3C-relevant. You may also be prompted to create files or work in a “home folder” (which N3C has disabled) or a project workspace you don’t have write permissions to. Instead, you can utilize the N3C Training Area (see below).\n\n\n\nFigure 11.15: Academy walk-through\n\n\n\n\n11.4.3 N3C Community Notes\nN3C Community Notes is a within-enclave application where researchers can author and share short articles, code snippets, or FAQ items. The application supports a rich tagging system, and notes can be linked to other N3C resources like training modules, knowledge objects, and concept sets. The note overview contains a link whose URL points at this specific note in the application for sharing.\n\n\n\nFigure 11.16: N3C Community Notes\n\n\n\n\n11.4.4 Documentation\nThe official platform documentation is a rich resource for details on applications, and includes many guides and how-tos. If you don’t desire to read all of the documentation in detail, you should at least skim sections relevant to applications you use. The search function can find articles relevant to specific application features or techniques.\n\n\n\nFigure 11.17: Foundry Documentation\n\n\n\n\n11.4.5 Having Trouble? (Support)\nThis last entry in the Training Resources page simply redirects to a page describing, and linking to, the two ticket systems described earlier in this chapter."
  },
  {
    "objectID": "chapters/support.html#sec-support-area",
    "href": "chapters/support.html#sec-support-area",
    "title": "11  Help and Support",
    "section": "11.5 N3C Training Area",
    "text": "11.5 N3C Training Area\nThe N3C Training Area is a project workspace where all N3C users can practice and learn using notional datasets (described below). This workspace is also used to organize other training resources (like the Training Portal).\n\n\n\nFigure 11.18: Training Area\n\n\nIf you wish to create a practice folder, you are free to do so inside the “Practice Area - Public and Example Data.” Simply open it up, and using the green +New button create a new subfolder with a unique name (many use shortened usernames, e.g. “oneils”). Within this folder you will be able to create new analyses, and these will have access to the notional datasets described next."
  },
  {
    "objectID": "chapters/support.html#sec-support-notional",
    "href": "chapters/support.html#sec-support-notional",
    "title": "11  Help and Support",
    "section": "11.6 Notional Datasets",
    "text": "11.6 Notional Datasets\nOMOP-formatted N3C patient data are protected by a Data Use Request process, but researchers may wish to explore OMOP tables and enclave tools prior to completing a DUR. The N3C Training Area is the place to do such practice, and N3C provides two notional (i.e. fake) datasets formatted similarly to the Level 2 and Level 3 data that do not require a DUR to access. They are both available via the data catalog under “Synpuf Synthetic Data” and “Synthea Notional Data”.1 The data they contain differ in some important ways, described next.\n\n\n\nFigure 11.19: Two sets of synthetic data\n\n\n\n11.6.1 SynPuf Synthetic Data\nSynPuf is short for “Synthetic Public Use Files,” or EHR records that have been scrubbed of personally identifiable information and released for public educational use. These SynPuf files originate from SynPuf Medicare Claims data and have been converted to OMOP format by the OHDSI community. The content of these data differ from N3C data in many ways (e.g. records prior to Jan. 1, 2018 are included), and they represent a distinctive population of Medicare-eligible patients. Lastly, the data are not recent, and so contain no COVID-19 related records such as diagnoses, lab tests, or vaccine records. The SynPuf data do not contain some N3C customizations to the OMOP data model, for example the manifest table used in N3C data to describe metadata about contributing data partners.\nCompared to the Synthea data however, SynPuf data better represent real EHR data, including the potential for data entry errors, diversity in medical codes used, and missing data. We thus recommend that researchers interested in trying statistical or machine learning models (or other applications better suited for realistic data) use the SynPuf notional data.\n\n\n11.6.2 Synthea Notional Data\nIn contrast to the SynPuf data, the Synthea notional data are derived from a probabilistic model of early-pandemic COVID-19 patient trajectories published by Walonoski et al. (2020) converted to OMOP. These data include COVID-19 diagnoses and lab tests for a subset of patients. The main limitation of this notional data is its model-generated cleanliness. Pneumonia in the Synthea dataset, for example, is always represented with the same concept ID, while in real data a variety of pneumonia sub-type concept IDs are represented. Real EHR data also contain missing, erroneous, or inconsistent information. With regard to COVID-19, N3C has modified the original data published by Walonoski et al. (2020) to include more diversity and realism in COVID-19 diagnoses and lab tests; a README file in the data catalog describes the modifications in detail.\nThe Synthea data have an additional benefit of being slightly more aligned with real N3C data for additions beyond the OMOP standard. For example, while SynPuf data tables include data partner IDs, Synthea also includes a manifest table with mock data partner metadata. The Synthea data also include constructed macrovisit information."
  },
  {
    "objectID": "chapters/support.html#sec-support-ohdsi",
    "href": "chapters/support.html#sec-support-ohdsi",
    "title": "11  Help and Support",
    "section": "11.7 OHDSI Resources",
    "text": "11.7 OHDSI Resources\nN3C relies heavily on the OMOP common data model, developed by an international group of researchers comprising the Observational Health Data Sciences and Informatics consortium, or OHDSI. OHDSI provides a wealth of training and support resources, the most significant of which are the Book of OHDSI (the inspiration for this book), EHDEN Academy (online video-based courses and lectures), and the OHDSI forums. These cover basic and advanced usage of OMOP data as well as techniques and good practices for working with observational EHR data.\n\n\n\nFigure 11.20: The Book of OHDSI is a great starting place for learning OMOP"
  },
  {
    "objectID": "chapters/support.html#sec-support-community",
    "href": "chapters/support.html#sec-support-community",
    "title": "11  Help and Support",
    "section": "11.8 Community Resources",
    "text": "11.8 Community Resources\nIn addition to Community Notes mentioned above, several venues are available to get help and support from the broad community. N3C researchers include statisticians and data scientists of all stripes, clinicians, and even industry and government representatives. More than a few new collaborations have resulted from peer-to-peer support in N3C!\n\n11.8.1 Enclave Users’ Group\nThe Enclave Users Group (EUG) is a community-focused forum where analysts can share practical information on techniques, tips, and methods in the N3C Data Enclave. Each session one or more presenters share a topic, emphasizing live Q&A, discussions, and meeting new people. Topics range from statistical techniques like propensity score matching, scaling machine learning algorithms for use on billion-row datasets, tips for scientific software development, and introductions of new N3C resources and initiatives. EUG sessions do not present protected data, so sessions are recorded and example resources are available in the N3C Training Area. For more information and an index of recorded sessions see the Enclave Users’ Group module  in the Training Portal.\n\n\n\nFigure 11.21: Enclave Users’ Group\n\n\n\n\n11.8.2 Slack\nSlack is commonly used for team communication in N3C, and several widely-subscribed channels are great support resources. These include #n3c-analytics where researchers ask general questions about methods or data (with 390+ members), #n3c-training where training-related announcements are posted, and a variety of topic-focused channels such as #n3c-ml for machine-learning. N3C uses the Slack organization of the National Center for Data To Health at https://cd2h.slack.com. Access however is managed via the N3C onboarding process, where Slack-preferred emails are collected.\n\n\n11.8.3 Domain Teams\nDomain Teams, covered in more detail in other parts of this book, are excellent support and training resources for their members. Not only can Domain Teams answer common questions of new N3C researchers, they can answer questions that pertain to their area of expertise. The pregnancy domain team, for example, is the best source of knowledge for locating pregnancy-related records in EHR data.2"
  },
  {
    "objectID": "chapters/support.html#sec-support-liaisons",
    "href": "chapters/support.html#sec-support-liaisons",
    "title": "11  Help and Support",
    "section": "11.9 Data and Logic Liaisons",
    "text": "11.9 Data and Logic Liaisons\nLogic and Data Liaisons are teams contributing to the N3C mission through software development and user support, prioritizing the needs of Domain Teams and their members. In order to perform research, users need to identify key variables for analysis. These key variables are generated through Code Workbooks and Templates that utilize specific Concept Sets (lists of key variables from constituent vocabularies), that identify and extract data to answer research questions. Through interaction with Domain Teams, the Data and Logic Liaisons continually develop and refine a core set of N3C Recommended concept sets and code templates that generate commonly used variables and support efficient customization by research teams. They also provide support services as described below.\n\n11.9.1 Data Liaison Services\nEHR data are complex, more so when they cover data contributed by 75+ sites. The Data Liaisons group consist of those most familiar N3C data, including members of the phenotype and ingestion and harmonization teams. Data Liaisons are subject matter experts in biomedical, translational, clinical data standards and Real-World data utilization to support program investigator analyses. Data Liaisons curate and review N3C-recommended concept sets for researcher use, and can field data-related questions, which should be submitted via the enclave-internal ticket system. Potential data quality issues should also be submitted via enclave-internal ticket system for routing to the Data Liaisons for review.\nFor basic questions about the OMOP common data model, refer to the OHDSI resources, and training portal modules for getting started with OMOP. Personalized assistance is provided during N3C Office Hours. Support for Concept Set consultation can be received by submitting a help desk technical support ticket in the N3C enclave. The Data Liaisons team will send a representative to your domain team meetings on an as needed basis for general consultation.\n\n\n11.9.2 Logic Liaison Services\nLogic Liaisons consist of analysts with significant technical expertise for research with N3C data. Although they do not develop project-specific research code as a service, they do create Knowledge Objects such as reusable code templates and convenient derived datasets. Logic Liaison members provide technical support at office hours, and many are active in the #n3c-analytics Slack channel.\nLogic Liaisons support N3C researchers who are learning to use and adapt the Logic Liaison code fact tables and templates. They also help researchers assess the feasibility of the project design with regards to data availability and data limitations. This team helps researchers assess and clean their project-specific fact tables using Logic Liaison Data Quality templates, which help research teams decide which sites to include in the analysis.\nLogic Liaison Code Fact Tables and Templates can be accessed by searching the Knowledge Store for “Logic Liaison Template”. Recorded trainings are provided in the “Logic Liaison Templates” module of the N3C Training Portal. Personalized help is provided during N3C Office Hours. Support for issues and errors encountered when using a Logic Liaison Template can be received by submitting a technical support ticket in the enclave. Team members are also active in the #n3c-analytics Slack channel. The Logic Liaison team will send a representative to your domain team meetings on an as needed basis for general consultation.\n\n\n\n\nWalonoski, J., Klaus, S., Granger, E., Hall, D., Gregorowicz, A., Neyarapally, G., Watson, A., & Eastman, J. (2020). Synthea™ novel coronavirus (COVID-19) model and synthetic data set. Intelligence-Based Medicine, 1-2, 100007. https://doi.org/doi.org/10.1016/j.ibmed.2020.100007"
  },
  {
    "objectID": "chapters/support.html#footnotes",
    "href": "chapters/support.html#footnotes",
    "title": "11  Help and Support",
    "section": "",
    "text": "Note that these should not be confused with the Level 1 Synthetic Data, which are derived from N3C patient data and protected by a Data Use Request.↩︎\nThis is not as trivial as it sounds!↩︎"
  },
  {
    "objectID": "chapters/ml.html",
    "href": "chapters/ml.html",
    "title": "12  Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1HZ3IGv17zUl9t8RxZSl4uOq_FRzrgTp_\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/advanced.html",
    "href": "chapters/advanced.html",
    "title": "13  Advanced Enclave Coding Techniques",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1K660Qn7m1z4TswwepM06CKgAPTojjt7q\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/examples.html",
    "href": "chapters/examples.html",
    "title": "14  Start to finish examples or worked examples",
    "section": "",
    "text": "Note\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1rWxFtzk1kyUSRJPDgPWwlmVCjnWEGf6i\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/funding.html",
    "href": "chapters/funding.html",
    "title": "15  Funding and Institutional Support",
    "section": "",
    "text": "A. Jerrod Anzalone: University of Nebraska Medical Center, Department of Neurological Sciences. Great Plains IDeA-CTR, NIGMS U54GM115458.\nWilliam H. Beasley: University of Oklahoma Health Science Center, Biomedical and Behavioral Methodology Core.\nKaren M. Crowley: Brown University, Brown Center for Biomedical Informatics, RI Advance-CTR Biomedical Informatics, Bioinformatics, and Cyberinfrastructure Enhancement Core. NIH/NIGMS U54 GM115677.\nShawn T. O’Neil: University of Colorado, Department of Biomedical Informatics, Translational and Integrative Biology Lab. NCATS U24 TR002306.\nKenneth J. Wilkins: National Institutes of Health, NIDDK, Office of the Director, Biostatistics Program / Office of Clinical Research Support.\nBryan J Laraway: University of Colorado, Department of Biomedical Informatics, Translational and Integrative Biology Lab. NCATS U24 TR0023 06.\nSharon J. Patrick: West Virginia Clinical and Translational Science Institute, Biostatistics, Epidemiology, and Research Design Core. NIH/NIGMS 5U54GM104942.\nMary H. Mays: University of Puerto Rico Medical Sciences Campus, Hispanic Alliance for Clinical and Translational Research. NIH/NIGMS U54GM133807.\nLisa C. Eskenazi: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science. NCATS-P00438-B.\nSigfried Gold: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science. NCATS-P00438-B.\nStephanie S. Hong: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science. NCATS-P00438-B.\nXiaohan Tanner Zhang: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science. NCATS-P00438-B.\nHarold P. Lehmann: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science. NCATS-P00438-B.\nChristine Suver: Sage Bionetworks, Research Governance and Ethics. NCATS U24 TR002306, Axel Informatics Subcontract NCATS-P00438-B"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anzalone, Alfred Jerrod, Ronald Horswell, Brian M Hendricks, San Chu,\nWilliam B Hillegass, William H Beasley, Jeremy R Harper, et al. 2023.\n“Higher Hospitalization and Mortality Rates Among\nSARS-CoV-2-Infected Persons in Rural America.” The Journal of\nRural Health 39 (1): 39–54. https://doi.org/10.1111/jrh.12689.\n\n\nCharlson, Mary E., Peter Pompei, Kathy L. Ales, and C.Ronald MacKenzie.\n1987. “A New Method of Classifying Prognostic Comorbidity in\nLongitudinal Studies: Development and Validation.” Journal of\nChronic Diseases 40 (5): 373–83. https://doi.org/10.1016/0021-9681(87)90171-8.\n\n\nHaendel, Melissa A, Christopher G Chute, Tellen D Bennett, David A\nEichmann, Justin Guinney, Warren A Kibbe, Philip R O Payne, et al. 2020.\n“The National COVID Cohort Collaborative\n(N3C): Rationale, design, infrastructure, and deployment.”\nJournal of the American Medical Informatics Association 28 (3):\n427–43. https://doi.org/10.1093/jamia/ocaa196.\n\n\nMehta, Hemalkumar B., Huijun An, Kathleen M. Andersen, Omar Mansour,\nVithal Madhira, Emaan S. Rashidi, Benjamin Bates, et al. 2021.\n“Use of Hydroxychloroquine, Remdesivir, and Dexamethasone Among\nAdults Hospitalized with Covid-19 in the United States: A Retrospective\nCohort Study.” Annals of Internal Medicine 174 (10):\n1395–1403. https://doi.org/10.7326/M21-0857.\n\n\nPalantir. 2023. “Documentation: Code Repositories\nOverview.” https://www.palantir.com/docs/foundry/code-repositories/overview/.\n\n\nPfaff, E. R., A. T. Girvin, T. D. Bennett, A. Bhatia, I. M. Brooks, R.\nR. Deer, J. P. Dekermanjian, et al. 2022. “Identifying Who Has\nLong COVID in the USA: A Machine Learning Approach Using N3C\nData.” Lancet Digit Health 4 (7): e532–41. https://doi.org/10.1016/S2589-7500(22)00048-6.\n\n\nPfaff, Emily R, Andrew T Girvin, Davera L Gabriel, Kristin Kostka,\nMichele Morris, Matvey B Palchuk, Harold P Lehmann, et al. 2022.\n“Synergies Between Centralized and Federated Approaches to Data\nQuality: A Report from the National COVID Cohort Collaborative.”\nJournal of the American Medical Informatics Association 29 (4):\n609–18. https://doi.org/10.1093/jamia/ocab217.\n\n\nReese, Justin T, Hannah Blau, Elena Casiraghi, Timothy Bergquist,\nJohanna J Loomba, Tiffany J Callahan, Bryan Laraway, et al. 2023.\n“Generalisable Long COVID Subtypes: Findings from the NIH N3C and\nRECOVER Programmes.” EBioMedicine 87. https://doi.org/10.1016/j.ebiom.2022.104413.\n\n\nSciences, Observational Health Data, and Informatics. 2019. The Book\nof OHDSI: Observational Health Data Sciences and Informatics.\nUnited States: Independent. https://ohdsi.github.io/TheBookOfOhdsi/.\n\n\nSharafeldin, Noha, Benjamin Bates, Qianqian Song, Vithal Madhira, Yao\nYan, Sharlene Dong, Eileen Lee, et al. 2021. “Outcomes of COVID-19\nin Patients with Cancer: Report from the National COVID Cohort\nCollaborative (N3C).” Journal of Clinical Oncology 39\n(20): 2232–46. https://doi.org/10.1200/JCO.21.01074.\n\n\nSun, Jing, Qulu Zheng, Vithal Madhira, Amy L. Olex, Alfred J. Anzalone,\nAmanda Vinson, Jasvinder A. Singh, et al. 2022. “Association\nBetween Immune Dysfunction and COVID-19 Breakthrough Infection After\nSARS-CoV-2 Vaccination in the US.” Archives of Internal\nMedicine (Chicago, Ill. : 1908) 182 (2): 153–62. https://doi.org/10.1001/jamainternmed.2021.7024.\n\n\nWalonoski, Jason, Sybil Klaus, Eldesia Granger, Dylan Hall, Andrew\nGregorowicz, George Neyarapally, Abigail Watson, and Jeff Eastman. 2020.\n“Synthea™ Novel Coronavirus (COVID-19) Model and Synthetic Data\nSet.” Intelligence-Based Medicine 1-2: 100007. https://doi.org/doi.org/10.1016/j.ibmed.2020.100007.\n\n\nWilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3. https://doi.org/10.1038/sdata.2016.18.\n\n\nYang, Xueying, Jing Sun, Rena C Patel, Jiajia Zhang, Siyuan Guo, Qulu\nZheng, Amy L Olex, et al. 2021. “Associations Between HIV\nInfection and Clinical Spectrum of COVID-19: A Population Level Analysis\nBased on US National COVID Cohort Collaborative (N3C) Data.”\nThe Lancet HIV 8 (11): 690–700. https://doi.org/10.1016/S2352-3018(21)00239-3."
  }
]