[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Researcher’s Guide to N3C",
    "section": "",
    "text": "Welcome\nWelcome to The Researcher’s Guide to N3C: A National Resource for Analyzing Real-World Health Data. Here you will find guidance, information, and pointers to resources for conducting research with the National COVID Cohort Collaborative (N3C).\nDuring a time of isolation and unanswered questions in an emerging pandemic, N3C was born of an urgent need to save lives. N3C was created “to acquire and harmonize data across organizations and provide a secure data environment to enable transparent and reproducible collaborative research.” 1 Dozens of technical, clinical, and regulatory experts from government agencies, healthcare organizations, academic institutions, and the private sector worked together to build the nation’s largest and most diverse set of de-identified health records to date. Today, thousands of researchers have actively engaged with N3C resulting in hundreds of scholarly products informing public policy and patient care.\nN3C emphasizes community and team science values which are embodied by the structure of the organization and reflected in the following chapters. This guide originated as a project of the Education and Training Domain Team, one of the many self-organizing teams within N3C. An editorial committee was formed to engage with principal investigators, clinician-scientists, informaticians, data scientists, and others to condense the collective knowledge amassed in N3C. Chapters are authored and peer reviewed by a diverse group of contributors with both broad and deep domain expertise. This has been a work of heart and soul, designed to make this ground-breaking work accessible without obscuring its complexity.\nN3C is continuously evolving, and we expect this book will as well. We invite you to join us in continually expanding and improving this guide, but most importantly, we invite you to get involved in one of the many avenues for impactful research that N3C provides.\nSincerely,\nThe G2N3C Editorial Committee"
  },
  {
    "objectID": "index.html#sec-welcome-contributors",
    "href": "index.html#sec-welcome-contributors",
    "title": "The Researcher’s Guide to N3C",
    "section": "Contributors",
    "text": "Contributors\nIndividual chapter lead and contributor affiliations are also provided in each chapter.\nEditorial Committee Shawn T. O’Neil, Will Beasley, Johanna Loomba, Sharon Patrick, Kenneth J. Wilkins, Karen M. Crowley\nChapter 1 – Introduction Karen M. Crowley, Shawn T. O’Neil\nChapter 2 – A Research Story Will Beasley, A. Jerrod Anzalone, Sharon Patrick\nChapter 3 – Data Life Cycle - From Patients to N3C Researchers Stephanie Hong, Bryan Laraway, Xiaohan Tanner Zhang, Maya Choudhury, Sofia Z. Dard\nChapter 4 – Governance, Leadership, and Operations Structures Christine Suver, Johanna Loomba\nChapter 5 – Onboarding, Enclave Access, N3C Team Science Sharon Patrick, Jonathan F. Emery, Suzanne McCahan, Mary Helen Mays\nChapter 6 – Getting & Managing Data Access Shawn T. O’Neil, Mariam Deacy\nChapter 7 – Understanding the Data Harold P. Lehmann, Lisa Eskanazi, Sigfried Gold, Shawn T. O’Neil, Thomas Richards, Kristin Kostka\nChapter 8 – Introducing Enclave Analysis Tools Amy Olex, Andrea G. Zhou, Johanna Loomba, Evan French, Shawn T. O’Neil, Steven G. Johnson\nChapter 9 – Best Practices for Research Life Cycle Harold P. Lehmann, Hytham Sidky, Jimmy Phuong, Kate Bradwell, Kenneth J. Wilkins, Andrea G. Zhou, David Sahner\nChapter 10 – Publishing and Sharing Your Work Julie A. McMurry, Jeremy R. Harper, Christine Suver, Carolyn T. Bramante, Mary K. Emmett, Amit K. Saha, Farrukh M. Koraishy, A. Jerrod Anzalone, Shawn T. O’Neil\nChapter 11 – Special Topic: Help and Support Shawn T. O’Neil, Saad Ljazouli, Johanna Loomba, Lisa Eskanazi"
  },
  {
    "objectID": "index.html#sec-welcome-contribute",
    "href": "index.html#sec-welcome-contribute",
    "title": "The Researcher’s Guide to N3C",
    "section": "How to Contribute",
    "text": "How to Contribute\nWe welcome suggestions, edits, and larger contributions to this guide. This book is typeset in Markdown, rendered with Quarto, and hosted on GitHub. For errors or requests, please submit an Issue to the book’s issue tracker. To make larger or direct contributions, please make a pull request using the standard GitHub workflow.\nIf you would like to contribute but are unfamiliar with any of these technologies, please feel free to email &lt;to be created: g2n3c_correspondence@ctsi-something-something.com&gt; with comments and suggestions for changes. If you would like to discuss the content or receive further help, see the Help and Support Chapter.\nThis project uses the N3C Community Guidelines as a Contributor Code of Conduct; by participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "index.html#sec-welcome-cite",
    "href": "index.html#sec-welcome-cite",
    "title": "The Researcher’s Guide to N3C",
    "section": "How to Cite this Work",
    "text": "How to Cite this Work\nO’Neil ST, Beasley W, Loomba J, Patrick S, Wilkins KJ, Crowley KM. (Eds.) (2023). The Researcher’s Guide to N3C: A National Resource for Analyzing Real-World Health Data. DOI: 10.5281/zenodo.7749367"
  },
  {
    "objectID": "index.html#sec-welcome-licensing",
    "href": "index.html#sec-welcome-licensing",
    "title": "The Researcher’s Guide to N3C",
    "section": "Licensing",
    "text": "Licensing\nThis book is licensed under the Creative Commons Attribution-NoDerivatives 4.0. Individual chapters are as well, unless otherwise noted."
  },
  {
    "objectID": "index.html#sec-welcome-funding",
    "href": "index.html#sec-welcome-funding",
    "title": "The Researcher’s Guide to N3C",
    "section": "Funding",
    "text": "Funding\nThis content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the N3C program.\nAny analyses herein were conducted using the NCATS N3C Data Enclave supported by NCATS U24 TR002306, Axel Informatics Subcontract Number: NCATS-P00438-B and made possible because of the patients whose data was contributed by partner organizations. We gratefully acknowledge the scientists who have contributed to the on-going development of this community resource (Haendel et al. 2020).\nFunding and support for individual authors is listed in Chapter 13.\n\n\n\n\nHaendel, Melissa A, Christopher G Chute, Tellen D Bennett, David A Eichmann, Justin Guinney, Warren A Kibbe, Philip R O Payne, et al. 2020. “The National COVID Cohort Collaborative (N3C): Rationale, design, infrastructure, and deployment.” Journal of the American Medical Informatics Association 28 (3): 427–43. https://doi.org/10.1093/jamia/ocaa196."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Researcher’s Guide to N3C",
    "section": "",
    "text": "See https://covid.cd2h.org/about.↩︎"
  },
  {
    "objectID": "chapters/intro.html#sec-intro-mission",
    "href": "chapters/intro.html#sec-intro-mission",
    "title": "1  Introduction",
    "section": "1.1 Mission",
    "text": "1.1 Mission\nThe National COVID Cohort Collaborative, or N3C, is an open-science community stewarded by the National Center for Data To Health (CD2H) and the NIH National Center for Advancing Translational Sciences (NCATS), with significant contributions from many partners including the Clinical and Translational Science Awards (CTSA) program, Centers for Translational Research (CTRs), and thousands of researchers from hundreds of participating institutions in the US and abroad.\nFaced with the COVID-19 pandemic, the issue addressed by N3C is clear and direct: the US has no centralized data repository for health records and related information, hindering the response of the scientific community.1 Although healthcare providers are mandated by law to utilize electronic health records (EHR), little guidance coordinates how or exactly what information to collect and store. Commercial data-collection suites (e.g. Epic) are widely used in clinical settings, and controlled vocabularies (e.g. ICD10 and SNOMED) provide standards for representing medical information, but there are many such standards in use and software is highly configurable to the needs of individual organizations. As a result, databases of EHR information across the US are largely non-interoperable, presenting challenges to researchers hoping to use this vast national store of information in practice."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-cdm",
    "href": "chapters/intro.html#sec-intro-cdm",
    "title": "1  Introduction",
    "section": "1.2 Common Data Models and N3C",
    "text": "1.2 Common Data Models and N3C\nIn recent years, the common solution to these issues has been the creation of Common Data Models (CDMs). A Common Data Model is an agreed-upon structure and format for databases containing clinical information, into which diverse organizational EHR databases can be standardized for research purposes. Even so, healthcare organizations are reluctant to share their data directly, because the risks associated with data breaches for protected health information (PHI) defined by Health Information Portability and Accountability Act (HIPAA) are high. As a result, several groups of healthcare and research organizations have formed federated research networks, where each organization in a network translates some subset of its data into an agreed-upon common data model, and affiliated researchers can then write queries intended for data in that format. Examples of such federated networks include PCORNet, i2b2, and OHDSI, each of which utilizes their own common data model. In a federated model, data queries are generated by researchers, but executed locally by the data owners and only summarized or specifically requested results are sent back to researchers. This ensures protection for the patient data (which never leaves the boundaries of any individual organization), but prevents exploratory data investigation and other techniques (like many machine-learning methods) which require direct access to the totality of the data.\n\n\n\nFigure 1.1: A visual representation of disparate, non-compatible EHR databases across the United States.\n\n\nDriven by the imperative of addressing COVID-19, in concert with the use of a FedRAMP-certified cloud-based analysis ecosystem we call the N3C Data Enclave,2 N3C is partnering with EHR data providers across the nation to collect billions of EHR data points for millions of patients with and without COVID-19 in a single, secure, accessible database for research use. In this centralized model, researchers have direct access to the entirety of the data to support complex analyses including AI and other machine learning techniques. N3C simultaneously moderates controlled access to these data by research teams from across the country (and beyond), including private companies, community colleges, universities, medical schools, and government entities. While data cannot be exported from the N3C Data Enclave, published results can be after an export review (see Chapter 10).\nLike federated research networks, N3C also uses a common data model, known as OMOP, chosen for its strong community support and open nature, support of scientific use cases, and availability of tools for translating and working with data (Chapter 7 and Chapter 8 discuss OMOP in more detail).3 To rapidly collect data from around the country, N3C leverages the existing work data owners have already done to convert their organization-unique data to one of a handful of N3C-supported “source” common data models: PCORNet, i2b2, TriNetX, ACT, and OMOP. A potential data partner with data in PCORNet format, for example, will locally run a set of N3C-generated “PCORNet to OMOP” translation scripts prior to transferring the result to N3C via a secure channel. The process of coalescing multiple such data payloads into a unified whole is known as harmonization, and is a complex task even after everything has been mapped to OMOP initially. Two overlapping teams of EHR data experts participate in this process: one works closely with data partners to make it as easy as possible to contribute data to N3C, and another handles the post-ingestion harmonization and comprehensive quality checks of the incoming data."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-enclave",
    "href": "chapters/intro.html#sec-intro-enclave",
    "title": "1  Introduction",
    "section": "1.3 The N3C Data Enclave and Data Access",
    "text": "1.3 The N3C Data Enclave and Data Access\nOnce harmonized and stored in the secure “N3C Data Enclave”, the data are made available via a web-based interface to research teams using common tools such as SQL, Python, and R, as well as a number of code-light graphical user interfaces.\n\n\n\nFigure 1.2: A visual representation of N3C’s data harmonization from source-CDM model, and team-based access to these data in a secure cloud-based Enclave.\n\n\nMere access to the Enclave, however, doesn’t automatically provide access to any of the protected data itself (although we do make other, publicly-available datasets available with minimal restriction for practice and learning purposes). Multiple “levels” of the data are available with different anonymization techniques applied, facilitating “just enough” access to research teams depending on their needs and ability to access protected health information. Accessing the most secure level, for example, requires obtaining approval from an Institutional Review Board (IRB) which validates the appropriateness of human subjects research, while the lowest level is heavily anonymized and accessible by private individuals (citizen scientists) with only certain legal and training requirements.\nBecause effective analysis of EHR data requires a diverse set of skills–especially clinical and data science/statistical expertise–N3C provides organizational structures and resources to rapidly create and support multidisciplinary research teams, many of which are geographically diverse as well. As of February 2023, dozens of these “Domain Teams” have supported over 400 research projects, contributed to by over 3,300 researchers hailing from 350+ different institutions and organizations. Over seventy data partners provide EHR data for 17 million patients (a third of whom have had COVID-19), representing 10.5 billion lab results, 3.5 billion medication records, 2 billion clinical observations, and 1 billion clinical visits. For up-to-date information on these numbers and more, visit our dashboard at https://covid.cd2h.org/dashboard.\n\n\n\nFigure 1.3: Summary statistics for N3C patients as of Aug, 2022. Confirmed COVID-19 patients are those with a known positive PCR or Antigen lab test, possible patients are those with likely symptomatology."
  },
  {
    "objectID": "chapters/intro.html#sec-intro-next",
    "href": "chapters/intro.html#sec-intro-next",
    "title": "1  Introduction",
    "section": "1.4 Where to Go Next",
    "text": "1.4 Where to Go Next\n\n1.4.1 For Researchers\nSo, why should you get involved with N3C? First and foremost, N3C provides an opportunity to participate in impactful team science. Investigators with expertise in multiple domains come together across organizational boundaries to understand and address the impact of COVID-19 across the United States. Dozens of N3C-supported publications span the gamut of research: Pfaff, Girvin, Bennett, et al. (2022) applied machine-learning methods to understand important predictive factors for Long-COVID, and Sharafeldin et al. (2021) identified demographic and clinical factors contributing to mortality risk in cancer patients. Mehta et al. (2021) studied the use of hydroxychloroquine, remdesivir, and dexamethasone over time at multiple sites, revealing how treatment guidelines evolve in response to updated information over time. Sun et al. (2022) studied breakthrough infections after vaccination, Yang et al. (2021) evaluated COVID-19 outcomes in HIV patients, Reese et al. (2023) clustered patients to reveal sub-types of Long-COVID, and Anzalone et al. (2023) found higher hospitalization and mortality in rural communities. These are but a small sample of work produced by researchers participating in N3C.\nThis range of work is only possible by the diversity of interests and expertise researchers bring. Practicing clinicians, biostatisticians, machine-learning researchers, and others collaborate on projects inside the secure N3C Data Enclave. N3C supports team science in a variety of ways. Domain Teams, for example, serve to connect groups with similar interests for peer support, research coordination, and collaboration building. While most Domain Teams are clinically oriented (e.g. the Pregnancy Domain Team), others are more general (e.g. the Machine Learning Domain Team). Chapter 5 covers Domain Teams in more detail. N3C provides a number of training and support venues, including regular office hours, training modules, and of course this book. See Chapter 11 for more information on these topics.\nOf course, N3C brings significant value as one of the largest databases of de-identified patient records in the US, covering drug prescriptions, conditions, procedures, and more, each associated with a corresponding visit and other information. Data are extensively quality-checked and harmonized for consistency with the OMOP common data model (see Chapter 3), which supports sophisticated filtering and querying (Chapters 7 and 8). Other data are available as well, including publicly-available datasets (e.g. from the US Census) and, for some patients, additional mortality, viral variant, or billing data from non-EHR sources (Chapter 3).\nBig data is of little value without powerful analysis tools. Fortunately, N3C’s Enclave supports analyses with SQL, Python, and R, including thousands of popular libraries for the latter two. Backed by the high-performance distributed-computing framework Apache Spark, researchers can include billions of rows of data in a single analysis. Graphical tools are also available for those without coding expertise, and the N3C community generates reusable code and datasets to pave the way for others. All of these tools are cloud-hosted, so researchers only need to bring a web browser. For information on these topics, see Chapter 8.\nFinally, N3C has worked hard to make these resources secure and accessible. Enclave access requires coverage by a Data Use Agreement, which hundreds of institutions across the US and beyond have signed on behalf of all their employees and students. Data itself is accessed via a guided Data Use Request form in the Enclave, and researchers can invite others to their projects at any time. See Chapters 5 and 6 for details.\n\n\n1.4.2 For Institutions\nThere are two primary ways that institutions can participate in N3C: (1) by signing an institutional Data Use Agreement; and (2) by contributing data.\nSigning an institutional Data Use Agreement provides Enclave access to all employees and students at your institution. Hundreds of institutions have done so, connecting their research community to a vast network of data, tools, and expertise. For more information about Data Use Agreements, see Chapter 5.\nInstitutions that contribute data to N3C gain more than recognition–contributing sites get early access to new features and pilot programs. More importantly, N3C provides data partners feedback on their data quality. While contributing sites implement their own data quality checks, N3C has discovered a number of issues that are only apparent in an environment with multiple organizations’ data (Pfaff, Girvin, Gabriel, et al., 2022).\nFinally, N3C’s governance structures may be of interest to other organizations embarking on large-scale, team-science efforts. Chapter 4 introduces these perspectives.\nRegardless of how you think your institution can work with N3C, getting started is as simple as reaching out–either by contacting N3C leadership, submitting a ticket to our Enclave-external help desk, or just stopping by office hours. More information on these latter two options may be found in Chapter 11.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nAnzalone, A. J., Horswell, R., Hendricks, B. M., Chu, S., Hillegass, W. B., Beasley, W. H., Harper, J. R., Kimble, W., Rosen, C. J., Miele, L., et al. (2023). Higher hospitalization and mortality rates among SARS-CoV-2-infected persons in rural america. The Journal of Rural Health, 39(1), 39–54. https://doi.org/10.1111/jrh.12689\n\n\nMehta, H. B., An, H., Andersen, K. M., Mansour, O., Madhira, V., Rashidi, E. S., Bates, B., Setoguchi, S., Joseph, C., Kocis, P. T., Moffitt, R., Bennett, T. D., Chute, C. G., Garibaldi, B. T., & Caleb Alexander, G. (2021). Use of hydroxychloroquine, remdesivir, and dexamethasone among adults hospitalized with covid-19 in the united states: A retrospective cohort study. Annals of Internal Medicine, 174(10), 1395–1403. https://doi.org/10.7326/M21-0857\n\n\nPfaff, E. R., Girvin, A. T., Bennett, T. D., Bhatia, A., Brooks, I. M., Deer, R. R., Dekermanjian, J. P., Jolley, S. E., Kahn, M. G., Kostka, K., McMurry, J. A., Moffitt, R., Walden, A., Chute, C. G., Haendel, M. A., Bramante, C., Dorr, D., Morris, M., Parker, A. M., … Niehaus, E. (2022). Identifying who has long COVID in the USA: A machine learning approach using N3C data. Lancet Digit Health, 4(7), e532–e541. https://doi.org/10.1016/S2589-7500(22)00048-6\n\n\nPfaff, E. R., Girvin, A. T., Gabriel, D. L., Kostka, K., Morris, M., Palchuk, M. B., Lehmann, H. P., Amor, B., Bissell, M., Bradwell, K. R., et al. (2022). Synergies between centralized and federated approaches to data quality: A report from the national COVID cohort collaborative. Journal of the American Medical Informatics Association, 29(4), 609–618. https://doi.org/10.1093/jamia/ocab217\n\n\nReese, J. T., Blau, H., Casiraghi, E., Bergquist, T., Loomba, J. J., Callahan, T. J., Laraway, B., Antonescu, C., Coleman, B., Gargano, M., et al. (2023). Generalisable long COVID subtypes: Findings from the NIH N3C and RECOVER programmes. EBioMedicine, 87. https://doi.org/10.1016/j.ebiom.2022.104413\n\n\nSharafeldin, N., Bates, B., Song, Q., Madhira, V., Yan, Y., Dong, S., Lee, E., Kuhrt, N., Shao, Y. R., Liu, F., Bergquist, T., Guinney, J., Su, J., & Topaloglu, U. (2021). Outcomes of COVID-19 in patients with cancer: Report from the national COVID cohort collaborative (N3C). Journal of Clinical Oncology, 39(20), 2232–2246. https://doi.org/10.1200/JCO.21.01074\n\n\nSun, J., Zheng, Q., Madhira, V., Olex, A. L., Anzalone, A. J., Vinson, A., Singh, J. A., French, E., Abraham, A. G., Mathew, J., Safdar, N., Agarwal, G., Fitzgerald, K. C., Singh, N., Topaloglu, U., Chute, C. G., Mannon, R. B., Kirk, G. D., & Patel, R. C. (2022). Association between immune dysfunction and COVID-19 breakthrough infection after SARS-CoV-2 vaccination in the US. Archives of Internal Medicine (Chicago, Ill. : 1908), 182(2), 153–162. https://doi.org/10.1001/jamainternmed.2021.7024\n\n\nYang, X., Sun, J., Patel, R. C., Zhang, J., Guo, S., Zheng, Q., Olex, A. L., Olatosi, B., Weissman, S. B., Islam, J. Y., et al. (2021). Associations between HIV infection and clinical spectrum of COVID-19: A population level analysis based on US national COVID cohort collaborative (N3C) data. The Lancet HIV, 8(11), 690–700. https://doi.org/10.1016/S2352-3018(21)00239-3"
  },
  {
    "objectID": "chapters/intro.html#footnotes",
    "href": "chapters/intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "These articles provide good overviews of N3C and the surrounding landscape: It took a pandemic, but the US finally has (some) centralized medical data (MIT Technology Review), and The ambitious effort to piece together America’s fragmented health data (The Verge)↩︎\nThe Federal Risk and Authorization Management Program (FedRAMP) is a rigorous, standardized certification program with an emphasis on security and information protection. The Enclave is an installation of Palantir Technologies’ Foundry platform, a FedRAMP-certified data analytics suite.↩︎\nOMOP was originally developed by its namesake, the Observational Medical Outcomes Partnership, but is now stewarded by the Observational Health Data Sciences and Informatics (OHDSI, pronounced “odyssey”) program, an international group of researchers and clinicians. For complete information about OHDSI and OMOP, see the Book of OHDSI.↩︎"
  },
  {
    "objectID": "chapters/story.html#sec-story-onboarding",
    "href": "chapters/story.html#sec-story-onboarding",
    "title": "2  A Research Story",
    "section": "2.1 Onboarding",
    "text": "2.1 Onboarding\nAs is typical for access to patient data outside your own institution, institutional participation in N3C requires legal agreements. The initial time investment is longer for an N3C investigation compared to most, but project-specific work tends to go faster. System access requires your institution to sign a Data Use Agreement (DUA) with N3C, and even with strong institutional support this can take several months in legal and administrative channels.1\nYet after clearing that first (tall) hurdle for your site, data access for each specific project takes only a week or two to be processed by the N3C Data Access Committee (DAC). That’s a remarkably short time considering the scale of available data. It’s likely quicker than initiating a project based on a single EHR from your site, and much quicker than accessing EHRs from 70+ sites individually.\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nThe next afternoon you are chatting with your institution’s navigator.2 She organized the local N3C presentation and invited any interested attendees to contact her.\n\n\n\n\n\n\n\n\nHover over a footnote to see the popup, without jumping to the bottom of the page.\n\n\n\n\nNavigator: I’m glad you think the N3C might help your research. As I wrote in this morning’s email, the agreement between the university and the NIH was established last year, so don’t worry about that.3 There are two remaining steps. First, complete your personal paperwork.4 Second, submit a DUR tailored to your hypotheses.5\nInvestigator: Remind me. What is a DUR?\nN: A Data Use Request describes your upcoming project. Once an NIH committee approves your proposal, your project’s code and data are protected in an associated workspace allotted on the NIH cloud.6 Everyone on your project shares this dedicated workspace. Your collaborators can request to join your workspace by signing a copy of your DUR in the Enclave as well.7\nI: Umm, I think I got it.\nN: It will make sense once you get it into it. Skim the example DUR proposals I’m sending now. Then start filling out this online form. Get as far as you can, and I’ll help with the rest. If there’s something I don’t know, I’ll ask a friend. The DUR application process will take about an hour. Then the proposal will likely be approved within a week or two. In the meantime, we can talk about potential collaborators.\nI: Is there anything else I need to know?\nN: The only other consideration at this point is whether you want your study to use Level 2 or Level 3 data. Level 2 data is fully de-identified, meaning all the dates are shifted within ±180 days. Also, all location identifiers are obfuscated to prevent reidentification, so you won’t have access to 5-digit ZIP Codes or other granular location measures. If you need either of those, you have to submit Level 3 data, which is a limited data set containing these data elements. Some institutions require an IRB protocol to submit a Level 3 DUR, so keep that in mind. Our institutional IRB has provided a letter covering most Level 3 DURs in N3C, so I’ll send that to you.\nI: Ok, that’s very helpful. I may need real dates as I anticipate differences in treatment deliveries based on real-world availability. If I start with Level 2, am I stuck with Level 2?\nN: No. If needed, you can request a workspace to be updated from Level 2 to Level 3 and receive IRB approval.\nI: That’s great. I think we’ll start with Level 2. I’ll work on the research protocol now.\n\nAfter some thought, the investigator develops a research protocol and submits the DUR in Figure 2.2.\n\n\n\nFigure 2.2: Submitted Data Use Request."
  },
  {
    "objectID": "chapters/story.html#sec-story-team",
    "href": "chapters/story.html#sec-story-team",
    "title": "2  A Research Story",
    "section": "2.2 Team Construction",
    "text": "2.2 Team Construction\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nThe next step is to build a team to leverage retrospective medical records. Like most contemporary research teams, heterogenous skills are important. You follow some advice and assemble the following team.\n\n\n\na navigator who has learned the administrative and IRB requirements and can facilitate the investigation,\na subject matter expert (SME) who has clinical experience with the disease of interest and can inform decisions with EHR variables,\na statistician or data scientist who understands the limitations of observational collection and can model retrospective data,\na logic liaison or informaticist who understands the challenges of EHRs and can extract and transform information (also known as a data engineer),\na data liaison who has expertise in medical terminology to help develop concept sets or identify previously validated concept sets, and\na principal investigator who knows the literature and can testable hypotheses and write the manuscript.\n\nFigure 2.3 depicts these roles. Depending on the investigation and the personnel, sometimes one person can serve multiple roles.\n\n\n\nFigure 2.3: Typical N3C Team Composition.\n\n\nN3C teams have some differences from conventional research teams at single sites. Some trends we have noticed are:\n\nMost N3C teams have researchers from multiple institutions. In the experience of the authors and editors, this encourages more diverse opinions and more willingness to express constructive criticism. Researchers from a single institution/lab are sometimes more reluctant to generate contrary views.\nThe role of the navigator is often the most important member of a successful team. Your local investigations are likely guided by someone with years of experience with institutional safeguards and the personnel who can help when something stalls. N3C is bigger and younger than your site’s EHR research team, so an N3C project will benefit when guided by a bright, patient, and persistent navigator.\n\nIf your team needs someone, consider asking a relevant domain team for help identifying and approaching potential collaborators. Note that community-wide data and logic liaisons are available for consultation during regular office hours.8"
  },
  {
    "objectID": "chapters/story.html#sec-story-meeting",
    "href": "chapters/story.html#sec-story-meeting",
    "title": "2  A Research Story",
    "section": "2.3 Initial Meeting",
    "text": "2.3 Initial Meeting\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nThe team is assembled after a few weeks. The first discussion is usually a variation of the following exchange, with the goal to move toward the completion of a research protocol.\n\n\n\nInvestigator: Welcome everyone. We’d like to know if Drug A or Drug B is associated with better outcomes.\nStatistician: No problem. I can longitudinally model the type and amount of each medication received by each patient, relative to their intake date.\nLogic Liaison: Hmmm. I’m happy to produce a dataset with the dose and frequency columns, but you may not find it useful.9 Those two columns are sparsely populated and they look inconsistent across sites.10\nI: Bummer. Then what’s realistic or feasible?\nSubject Matter Expert: Maybe this simplifies the picture… In my clinical experience, a patient rarely switches between Drugs A & B. Based on the initial presentation, their provider will pick A or B, and complete the regimen unless there’s an adverse event.\nSt: In that case, should my initial model have three levels for treatment: A, B, and A+B?\nI: Probably. In the N3C database, can someone tell me how many patients get both during the same visit?\nLL: I’m already logged into the Enclave11. Give me 2 minutes to add this drug to our templated fact tables12 which show same-day events and also summarize events at the patient level.13\nI: Oh my goodness, is that your cat? What a cutie!14\nLL after a few minutes: Ok, I got it. Unmutes himself. Ok, I got it. 40% of patients are Drug A only, 52% are Drug B only, while 8% have at least one administration of both Drug A & B in the same visit.\nSME: Weird. 8% is a lot more than I expected. I was thinking around 1%.\nLL: Hmm, let me check. Give me another minute.15\nLL after a few minutes: I see what you mean. It looks like the bulk of the combo patients were admitted in the spring of 2020. After Jan 2021, only 3% of patients have both Drug A & B.\nSt: I was planning to model the phase of the pandemic (by including the diagnosis date as a covariate). I’ll test if there’s a significant interaction between time and treatment.\nI: I like that as a starting point. Regarding the question about dose and frequency… For now let’s assume the providers were following the current dosing guidelines. Therefore the dose and frequency variables can be dropped from the analyses.\nSt: Phew. I didn’t want to admit this. But I skimmed the dosing guidelines you emailed yesterday. It looked complicated. I wasn’t sure if I could appropriately incorporate those variables in the model.\nI: Well, that’s everything I wanted to cover today. See you in two weeks. Wait. I can’t believe I forgot. Sorry –our Navigator is sick this week and I’m almost worthless in her absence. Is everyone still on the call? For our secondary hypothesis, we want everything to connect to a patient’s diagnoses. …before, during, and after their covid hospitalization.\nLL: Bad news. This is kinda like the dose and frequency situation a few minutes ago. The structure of the OMOP diagnosis table theoretically can connect a patient’s diagnoses across different locations. But the quality of the historical records depends on the site. Some places like Rhode Island leverage their state’s HIE16 to populate their N3C dataset. However other places are not as well connected. If a patient doesn’t have diagnosis records, it’s tough to determine if they are healthy, or if their primary care provider uses a siloed EHR.17\nI: Ugh. Good point.\nLL: But I’ve got good news. All the N3C contributors comprehensively capture all conditions diagnosed during the visit. Furthermore the diagnosis codes are standardized well across sites. That’s because most providers enter ICD codes into the EHR, which eventually can be cleanly mapped to OMOP’s standard concepts.18\nI: Well, that’s fine for this paper. Maybe our next manuscript will follow up with N3C’s death records.19\nSME: Sorry everybody, I have clinic this week, and they’re calling me. I need to drop.20\nSt: Can I go back and ask a question about medications? I see that Drug A has 15 different brand names. I don’t recognize half of them. How should I classify them?\nLL: It’s actually worse than that. Sorry I’m a downer today. Can you see my screen? Drug A has 15 brand names and 200 different…21  So long story short, you’ll probably want to transform the two counts into two booleans.\nSt: Ok. Thanks for simplifying the dataset for me."
  },
  {
    "objectID": "chapters/story.html#sec-story-definitions",
    "href": "chapters/story.html#sec-story-definitions",
    "title": "2  A Research Story",
    "section": "2.4 Protocol, variables, & definitions",
    "text": "2.4 Protocol, variables, & definitions\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nDeveloping a research protocol is both familiar and vague in the context of EHR studies, even when researchers have several years of graduate-level courses and real-world experience. Tradeoffs are inevitable when selecting variables, and an investigator’s first choice is not always available.\n\n\nRealistically an investigation can use only a fraction of the terabytes of information in an EHR. The research team must identify the relevant variables among the qualifying patients to produce a coherent dataset tailored to their hypotheses.\nWhile there are different approaches to developing a Research Protocol, the general steps involved include:\n\nDefine the research question: The first step is to clearly define the research question and the patient population of interest. This will guide the selection of relevant clinical concepts and data sources.\nDetermine the study design: The research team should choose an appropriate study design, such as a cohort study or case-control study, and determine the inclusion and exclusion criteria for the patient population.\nDevelop the analytic plan: The research team should prospectively specify the statistical methods to be used to analyze the data, including any adjustments for confounding variables and any sensitivity analyses.\nDocument the study protocol: The research team should document a detailed study protocol that includes all of the above information, as well as any other relevant information, such as ethical and regulatory considerations, data privacy and security measures, and data sharing agreements.\n\nThe N3C Data Enclave’s Protocol Pad supports the development and documentation of detailed study protocols.22"
  },
  {
    "objectID": "chapters/story.html#sec-story-omop",
    "href": "chapters/story.html#sec-story-omop",
    "title": "2  A Research Story",
    "section": "2.5 OMOP Tools",
    "text": "2.5 OMOP Tools\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAfter you have created a research protocol, you turn to OMOP and N3C tools to define the variables.\n\n\nThe Observational Health Data Sciences and Informatics (OHDSI) program maintains the Observational Medical Outcomes Partnership (OMOP) common data model (CDM). OMOP was funded by the US Food and Drug Administration in 2008, primarily for adverse drug events surveillance, but it has expanded to become the de-facto global research CDM. Detecting a small signal requires a large dataset –larger than any single healthcare database (OHDSI, 2019, Chapter 1). Given its ubiquity and active research community, OMOP is well-suited for N3C. OMOP has extensive tooling to support researchers, including two tools that directly support the curation of concept sets from the OHDSI program and one tool that is specific to N3C:\n\nAtlas provides a user-friendly interface for querying and analyzing data in the OMOP CDM. In the context of N3C, it supports browsing medical terminology and supports the development of concept sets.\nAthena is a centralized repository of standardized clinical vocabularies.\nThe N3C Concept Set Browser is an N3C-specific tool that allows you to explore and modify existing concept sets as well as create new concept sets to fit your exact study needs.\n\nConcept sets, described in detail in Chapter 7 and Chapter 8, are the basic building blocks of an analytic dataset. They contain lists of medical codes, usually restricted to a very specific definition or computable phenotype. In N3C, they are used to identify cohorts or exposures to answer a research question. They point to standardized vocabularies and clinically organized domains in the OMOP CDM (e.g., drug, condition, measurement).\nIn general, the overall process involved in developing concepts is as follows:\n\nDefine the research question: The first step is for the research team to clearly define the research question and population of interest, which will guide the selection of relevant clinical concepts.\nExplore the data: Using Atlas or the N3C Concept Set Browser, the team member filling as the data liaison will explore the data available or existing concept sets to identify relevant clinical concepts.\nRefine the concept set: The data liaison works with the SME to refine the concept set, which is usually an iterative process, to include clinically relevant and exclude clinically irrelevant concepts.\nValidate the concept set: Once the concept set has been defined, the SME and logic liaison validate the concept set and publish it in the N3C Concept Set Browser, which allows for reuse across the N3C community.\n\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAfter determining the need for a concept set defining anemia, which is a common symptom of scurvy, the subject matter expert and data liaison meet to refine the concept set.\n\n\n\nData Liaison: Good morning! I’ve started browsing existing concept sets for anemia and found one potential option.\nSubject Matter Expert: That’s great. Let’s take a look.\nDL pulls up a concept set for anemia: This is one that’s out there. It uses the parent SNOMED CT Code 271737000.\nSME: Hmm…this isn’t quite right. We need anemia caused by blood loss. A lot of these are unrelated.\nDL: Gotcha. Let’s take a look at the hierarchy and see if we can refine it. We can look at the descendants and go from there. Three hours elapse.\nDL: Ok, great. I think we have a working example. I’m going to extract all of these into a spreadsheet. Please go through this one more time and then I’ll share it with the group to review."
  },
  {
    "objectID": "chapters/story.html#sec-story-preparation",
    "href": "chapters/story.html#sec-story-preparation",
    "title": "2  A Research Story",
    "section": "2.6 Data Preparation",
    "text": "2.6 Data Preparation\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nOnce the project team has outlined the study protocol, key definitions, and timing of study elements, the next phase is the curation of a dataset for analysis. This is typically within the purview of the team informaticist or logic liaison. Let’s take a peek into the process…\n\n\nAfter clarifying all the data elements and study protocol, the next step is to curate an analytic dataset. Depending on the study design, this can be organized in multiple ways. In general, an analytic dataset is organized at either the person or encounter level. This means that you’ll have a single analytic dataset per patient or one row per encounter. This topic is covered in greater detail in the Tools and Best Practices chapters.\nUsing the previously defined protocol, the informaticist will use the concept set browser to identify or create relevant concept sets and prepare the analytic dataset in a code workbook or code repository in the N3C Data Enclave. This time-intensive process produces a dataset that the team’s statistician will analyze. N3C Logic Liaison fact tables and templates are available to help build these datasets more quickly using defined pipelines where the custom concept sets can quickly be added as inputs.\nThe N3C Logic Liaisons also provide tools that assess the quality of your derived dataset. These tools inform decisions such as dropping specific sites or variables from the analysis (for example, if a site appears to be systematically missing a variable that is important to your hypothesis).23\n\n\n\nTable 2.1: Scurvy Analytic Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\nSex\nRace Ethnicity\nQuarter of COVID dx\nSmoking Status\nScurvy pre-COVID\nMed A\nMed B\nDiabetes pre-COVID\nCOPD pre-COVID\nLiver Disease pre-COVID\nCancer pre-COVID\nRenal Disease pre-COVID\nCOVID Hospitalization\nECMO IMV\nTime to ECMO IMV\nDeath\nTime to Death\n\n\n\n\n27\nM\nNon-Hispanic White\n2022Q2\nCurrent or Former Smoker\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n28\n0\n28\n\n\n75\nF\nHispanic or Latinx\n2021Q1\nNo Documented History of Smoking\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n28\n0\n28\n\n\n54\nM\nNon-Hispanic Black or African American\n2020Q4\nNo Documented History of Smoking\n1\n1\n0\n1\n0\n0\n1\n0\n1\n0\n1\n1\n1\n\n\n34\nF\nNon-Hispanic White\n2023Q1\nCurrent or Former Smoker\n1\n0\n1\n1\n0\n1\n0\n0\n1\n1\n3\n1\n5\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…"
  },
  {
    "objectID": "chapters/story.html#sec-story-analyses",
    "href": "chapters/story.html#sec-story-analyses",
    "title": "2  A Research Story",
    "section": "2.7 Analyses",
    "text": "2.7 Analyses\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAfter the first draft of the analytic dataset is prepared, the focus shifts from the informaticist to the statistician. This often requires a discussion to communicate the definitions and data structure.\n\n\n\nNavigator: Welcome everyone. Today we’ll review the initial analytic dataset and the next steps in the analysis.\nInformaticist: Yep. I’ve made a new code workbook in your folder in the project workspace. Can you pull it up and we can take a look?\nStatistician: Sure. Did you send me the dataset? I don’t remember seeing an email.\nN: Remember, everything has to be done within the secure N3C platform. So you won’t be doing anything on your local machine. It all has to be done within the platform. Do you know how to get to the project workspace?\nS: …remind me, please?\nA few minutes later after a refresher.\nS: Ok, great. I think I get it. So it’s not too different from using R in RStudio as long as I follow the documentation you sent me. I think I can figure that out. What happens when I want to share results with the rest of the team? Can I email everyone the initial results?\nN: Ehh, there’s a better way to securely share intermediate results. First, create a blank report and add stuff like tables, graphs, summary results, and model output to a report. Next, share the report (within the N3C Data Enclave) with those who have access to our project workspace. Then when it’s time to share results outside the platform, we’ll submit a download review request (DRR) to ensure compliance with the N3C privacy and security expectations. Once N3C leadership approves the DRR, the report can be extracted from the Enclave.\nS: Gotcha. I’ll reach out if I have any questions once I get to that point. Thanks!\n\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nFrom here, the statistician can work directly in N3C using R or Python for data analysis. After some work figuring out the platform, the statistician adds some simple summary statistics to a report. Within a code workbook, they develop the following code to produce a table added to a report.\n\n\ndescriptive_statistics &lt;- function(analytic_dataset) {\n  library(gtsummary)\n  library(dplyr)\n\n  patient_characteristics &lt;-\n    analytic_dataset %&gt;%\n    dplyr::select(\n      Scurvy_Indicator_Before_COVID,\n      Age,\n      Race_Ethnicity,\n      Medication_A,\n      Medication_B,\n    ) %&gt;%\n    gtsummary::tbl_summary(\n      by = Scurvy_Indicator_Before_COVID\n    )\n\n  table1 &lt;- patient_characteristics &lt;- as_tibble(table1, col_labels = FALSE)\n\n  return(table1)\n}\nWhich produces the following Table 2.2.\n\n\nTable 2.2: Characteristics of Patients with and without Scurvy\n\n\n\n\n\n\n\nCharacteristic\nNo HistoryofScurvy\nHistoryofScurvy\n\n\n\n\nPatient Count\n19,871\n2,199\n\n\nAge, Median(IQR)\n50(25, 75)\n51(27, 75)\n\n\nRace/Ethnicity\nNA\nNA\n\n\nNon-HispanicWhite\n7,834(39%)\n883(40%)\n\n\nNon-HispanicBlack or African American\n3,970(20%)\n431(20%)\n\n\nHispanic or Latinx\n3,986(20%)\n430(20%)\n\n\nMissing/Other\n4,081(21%)\n455(21%)\n\n\nMedication A\n3,017(15%)\n318(14%)\n\n\nMedication B\n1,661(8%)\n173(8%)\n\n\n\n\n\nAfter repeating this for the remaining analyses for the project, the statistician creates a report and requests it be reviewed by the N3C Download Review Committee\n\nStatistician: Good afternoon! I just received notification from N3C that my download review request was approved. What happens now?\nNavigator: That’s great! Now that it’s been approved, you can go into the Download Review Dashboard and there’s an area where you can safely download the results. The review comes with a DRR ID, which is required to download the associated results.\nS: Ok. And then I can send it out to the rest of the team?\nN: Yep! Once it’s been reviewed, you can share it with the rest of the team outside of the Enclave.\nS: Great. Thank you! I’ll set up a time to review the results with the team next week."
  },
  {
    "objectID": "chapters/story.html#sec-story-manuscript",
    "href": "chapters/story.html#sec-story-manuscript",
    "title": "2  A Research Story",
    "section": "2.8 Manuscript Preparation",
    "text": "2.8 Manuscript Preparation\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAfter a few iterations, the analyses are complete and the team prepares for the next phase.\n\n\n\nInvestigator: Thanks everyone for making this meeting. I hope you’ve had a chance to review the results that were circulated last week.\nSubject Matter Expert: They looked great. Very unexpected, but intriguing results. I was surprised that outcomes were so much better in those exposed to Medication B than to Medication A.\nTeam reviews the results in detail. Manuscript write-up divided amongst the study team.\nI: Ok, so we should all know what the next steps are for preparing the manuscript. Are there any other requirements from N3C before we can submit the paper to our target journal?\nNavigator: Good question. The manuscript draft has to be reviewed by the N3C Publication Committee. They meet weekly and will review the draft to make sure it’s compliant with N3C policies and adheres to the original research question proposed by us.\nI: Ok, that makes sense. How long does it usually take?\nN: In my experience, it’s a pretty quick turnaround. Usually 1-2 weeks unless there are serious issues with the draft. They are primarily concerned about compliance with N3C policies, such as not having small cell counts and having gone through the download review process. Since we were following these guidelines, it should be a quick review.\nI: Awesome. I hope you’ll still be available to help when we get to the step. Thank you!\nN: Of course. Let’s plan on meeting after the draft is closer to being finished.\nI: Sounds great. See you all in two weeks!\n\n\n\n\n\n\n\nVoice of Narrator\n\n\n\nAt this point, the team divides the remaining work and starts writing. After completing a draft that’s almost ready to be submitted for publication, they submit their draft to the N3C Publication Committee. After a week, they receive approval (see the collapsed callout below), tweak the manuscript, and submit it to a journal.\nTheir research story has finished, but yours is just beginning. The chapters in this book provide details that are useful to N3C researchers. Please tell us if you have ideas how to improve the book for someone in your position. Enjoy!\n\n\n\n\n\n\n\n\nSample Email from N3C Publication Committee\n\n\n\n\n\n\nTitle: Approved to submit MSID:383.23 | Investigator | The Impact of Drug A and Drug B on Patient with Scurvy Post-Acute COVID-19\nDear Investigator,\nThe publication committee has reviewed and conditionally approved your manuscript for submission. The following must be addressed: Please add “on behalf of the N3C consortium” to the end of the author list with an asterisk that says consortial contributors are in the process of being documented.\nSincerely, The N3C Publication Committee\n\n\n\n\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nOHDSI. (2019). The book of OHDSI: Observational health data sciences and informatics. OHDSI. https://ohdsi.github.io/TheBookOfOhdsi/\n\n\nPfaff, E. R., Girvin, A. T., Gabriel, D. L., Kostka, K., Morris, M., Palchuk, M. B., Lehmann, H. P., Amor, B., Bissell, M., Bradwell, K. R., et al. (2022). Synergies between centralized and federated approaches to data quality: A report from the national COVID cohort collaborative. Journal of the American Medical Informatics Association, 29(4), 609–618. https://doi.org/10.1093/jamia/ocab217"
  },
  {
    "objectID": "chapters/story.html#footnotes",
    "href": "chapters/story.html#footnotes",
    "title": "2  A Research Story",
    "section": "",
    "text": "On the other hand, hundreds of institutions have already done so; check the list at https://covid.cd2h.org/duas. This process is described in Section 5.4.↩︎\n This “navigator” role may be called something differently at your institution; the roles are defined below in Section 2.2. If your institution doesn’t support this position, we suggest starting with the N3C Office Hours or other avenues of support.↩︎\nRead about the institutional-level DUA in Chapter 5.↩︎\nSee Chapter 5.↩︎\nProject-level paperwork is discussed in Chapter 5.↩︎\nThe N3C Data Enclave is detailed in Chapter 8.↩︎\nDURs are the topic of Chapter 6.↩︎\nSee Section 11.9.↩︎\n  OMOP tables are designed to represent data from many sources but typically need to be transformed before analysis.↩︎\nConformance is a topic in Chapter 3.↩︎\nSee Chapter 6 for accessing the N3C Data Enclave.↩︎\n See Section 7.3.3.1 and Section 11.9.2↩︎\nRead about SQL, Python, and R transforms in Code Workbooks in Chapter 8.↩︎\nThere is a brief discussion of SME’s cat.↩︎\nThere is a brief discussion of S’s daughter strutting in the background wearing a cowboy hat and waving a fairy wand.↩︎\nAn HIE is a health information exchange.↩︎\nThe benefits and caveats of real-world data are a theme throughout the book, particularly in the best practices discussed in Chapter 9.↩︎\nAuthoring and using concept sets is described in Chapter 7. Mapping an ICD to SNOMED diagnosis code is an example of mapping a “non-standard” to a “standard” concept, discussed in Chapter 7.↩︎\nMortality records are discussed in Section 7.5.3.↩︎\nEveryone says goodbye to the cat.↩︎\n The conversation gets really technical. More technical than the editors want for this chapter.↩︎\n Section 8.4.1 describes the tool and Chapter 9 describes the best practices.↩︎\nSee also Pfaff et al. (2022).↩︎"
  },
  {
    "objectID": "chapters/cycle.html#sec-cycle-intro",
    "href": "chapters/cycle.html#sec-cycle-intro",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nThis chapter introduces the National COVID Cohort Collaborative (N3C) data ingestion and harmonization pipeline and explains the technical and research requirements for the central data repository. N3C is a centralized data repository and accessible Real World Data (RWD) dataset that facilitates COVID-19 research. The COVID-19 pandemic has taken the lives of over 6.8 million people worldwide1 and more than 1.1 million in the United States2.\nN3C is the largest national, publicly available HIPAA-limited dataset in US history, and its innovative data-governance and public-government partnership have made broad sensitive clinical data sharing possible to support COVID-19 research. To date, N3C brings together over 23 billion clinical records from over 18 million patients from 75 medical institutions. It represents a major achievement in overcoming legal, regulatory, and technical barriers to become the largest and first national, publicly available HIPAA-limited data set.\nThe N3C has galvanized sharing of data, methods, and artifacts; reproducibility and transparency; and attribution for all types of contributors. Furthermore, N3C connects to other patient data repositories through Patient Privacy Preserving Record Linkage (PPRL) and integrates with environmental, social, mortality evidence, viral variant genomic datasets, and CMS (Centers for Medicare & Medicaid Services) datasets to create a holistic view of the patient’s healthcare journey. The availability of this data has catalyzed over 300+ institutions involving 3,800+ researchers."
  },
  {
    "objectID": "chapters/cycle.html#sec-cycle-overview",
    "href": "chapters/cycle.html#sec-cycle-overview",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "3.2 Overview",
    "text": "3.2 Overview\nDisparate data sources from over 75 data partners are ingested, harmonized and merged together to create a central electronic health record (EHR) data repository. Manual curation of 2,000 mappings was scaled into a fully automated data pipeline that includes common data model (CDM) schema checks, data conformance checks, correction for missing COVID test measurement codes and data expectation checks within the data ingestion and harmonization process. Each data partner’s ingested data is merged to create a central data repository, which allows for consistent and efficient data quality reviews.\nBeyond well-recognized data quality issues, we discovered heuristics relating to each CDM conformance, demographics, COVID tests and associated results, conditions, encounters, measurements, observations, coding completeness, and fitness for use. The results of the data quality issues were communicated back to the participating sites and the sites improved their data quality after feedback. Furthermore, the N3C central data repository, referred to as the N3C Data Enclave or Enclave, is able to infer missing measurement units as well as correct invalid units of measure for certain laboratory test results through unit inference algorithms. Unified Code for Units of Measure is used to harmonize the measurement units.\nThe N3C’s central data repository also supports the N3C’s data integrity, security, accessibility, and governance objectives. The contributing sites submit data in one of four CDM formats through SSH File Transfer Protocol (sFTP) to the secure cloud platform in a compressed data file zip format. The zip file structure must conform to a predefined payload structure for efficient processing. The zip file is then transferred to the Enclave. All access to the sFTP site is audited and monitored. It is Federal Risk and Authorization Management Program (FedRAMP) compliant. As data is transferred into the Enclave for processing, the governance team receives secure messages regarding this process for audit purposes. Once the data is transferred to the Enclave the automated pipeline is triggered to process the data through the data ingestion and harmonization pipeline.\n\n\n\nFigure 3.1: N3C Data Pipeline. Limited data sets submitted by participating data partners in their source CDMs are ingested into the N3C Data Enclave and harmonized into the OMOP CDM.\n\n\n\n3.2.1 N3C Data Ingestion Pipeline Workflow\nThe contributing sites regularly run scripts written by the N3C Phenotype Workstream to extract COVID and control group patients’ data. Data encoded in four different CDMs (i.e., OMOP, PCORnet, ACT, and TriNetX) from over 232 medical institutions are harmonized to the target Observational Medical Outcomes Partnership (OMOP) CDM with ~50,000 transforms and constituent mappings recorded with full provenance.\nThe N3C Data ingestion pipeline consists of the following five categories of steps:\n\nNative source CDM conformance check after unzipping the payload. This ensures that the submitted data adheres to the source CDM specification.\nCorrection of COVID-19 laboratory tests with missing LOINC codes. The COVID-19 TestNorm utility (Dong et al., 2020) is used to rescue COVID lab tests with missing LOINC codes.\nSource data terminology translation. The terminology translation step involves standardizing the terminology used in the data so that they are consistent across data sources, and includes conversion of terms used in the source dataset to those used in the target data model.\nOMOP CDM conformance check which structures and organizes the data into common domains using common terminology.\nData quality check where the source data terminology is converted to OMOP concepts using the OMOP concept relationships to ensure that the data is consistent and accurate across the domain.\n\n\n\n\nFigure 3.2: N3C Data Pipeline Workflow. Data submitted by participating data partners undergo multiple steps prior to data release, including multiple transformations, harmonizations, data quality checks, filters, and data linkages.\n\n\nSource terminology is largely classified into two groups; a defined series of valued sets permissible within each source CDM and a varied dynamic list of codes and respective code systems that exist in the source CDM, e.g. UCUM, LOINC, HCPCS, ICD10CM, ICD10PCS, CPT4 and RxNorm. We created a shareable set of crosswalk mappings that are used to convert the value sets, such as gender and race concept sets, from all source CDMs to OMOP concepts along with associated OMOP domain mapping information. Code map service is used to generate site-specific dynamic lists of terminology crosswalk mapping tables to accurately map the source terminology into OMOP concept ids. The OMOP relationship table is referenced to identify the codes or terms found in the source data with standard OMOP concept ids. This terminology translation is a critical component of the data ingestion process as it allows the data from different sources with varying terminology to be used together, enabling more consistent, comprehensive and accurate analyses and insights. The following sections will describe the data ingestion steps in more detail.\n\n\n3.2.2 Validate Receipt of the Data\nSites that are contributing data to N3C are required to include a manifest file with every submitted data payload that contains metadata describing the submitted dataset. This manifest file includes detailed information about the submitted dataset, such as the local CDM name, site name, run date, information on date shifting that the site may perform prior to submission, the site’s contact information, and the OMOP vocabulary version number used at the site if the site uses the OMOP CDM. In addition to submitting the zipped data files, sites must also provide the expected row counts of the submitted datasets. The row count information is used as part of a handshake protocol for the pipeline to confirm the correct receipt of the datasets sent by the participating site. If the parsed data counts in the Enclave do not match the data counts sent by the site, then an issue is generated for follow-up by the data triage team. This metadata helps to ensure that the submitted datasets are properly parsed through the automated ingestion pipeline and can be used effectively by N3C researchers.\n\n\n3.2.3 Check Conformance with the Source Common Data Model\nThe N3C data conformance check is an important process that is used to verify the quality and consistency of incoming datasets. This process ensures that the data meets the source CDM constraints and is formatted and structured correctly according to the CDM version specification set by the CDM model. During the data conformance check, the data files are parsed and any empty strings are replaced with null values. The columns are also cast to the expected data types as defined by the CDM model specifications.\nThere are several types of checks that are included in the data conformance process, such as the presence of required tables and primary key checks. Table checks are used to ensure that all of the required tables (such as demographics, patient_dimension, and patient) are present in the dataset. Primary key checks verify that there are no null values or duplicates in the primary key fields, which helps to ensure the integrity and reliability of the data. For example, if the primary key field contains duplicates, the build will fail as a result of a failed data health check, and an issue is logged for the triage team to contact the site with reference to the source data that raised the error. The integrity of the foreign key reference is also checked and the row is excluded and removed from the ingestion pipeline.\nCertain data health checks will fail the build and prevent the data from getting reviewed for release. By performing native CDM conformance checks, N3C can ensure that data is fully prepared to be ingested by the next step in the data pipeline.\n\n\n3.2.4 COVID Lab Correction\nAmong different types of clinical data ingested into N3C, COVID-19 diagnostic testings are extremely important for research analyses, as they are the primary means to identify the confirmed COVID-19 cases. To address the urgency of the pandemic, individual institutions have created local names and local codes for those new COVID-19 testings in their EHRs. Meanwhile, LOINC, a widely used international standard for lab tests, has responded quickly by developing a new set of standard codes for COVID-19 testings to guide standard coding of COVID-19 testings in clinical settings. The Enclave repository contains a supplementary data mapping table to map COVID-19 lab measurement data with missing LOINC codes.\nThe COVID-19 TestNorm tool developed at UT Houston was used to initially prepare the mapping table. This tool is available as an open-source package for developers and as an online Web application for end users. In addition to the curated mapping table, missing LOINC codes for COVID-19 test measurement data are carefully reviewed by the Data Ingestion & Harmonization (DI&H) team. This correction process is based on the source lab test names submitted in the data and is carried out using a strict string-matching approach. The new mapping entries are manually reviewed by vocabulary experts and clinicians to ensure their accuracy.\n\n\n3.2.5 Source Data Vocabulary / Terminology Translation\nThe incoming data sets submitted by contributing sites use collections of vocabulary codes to describe the managed concept collections from which to draw value set content. Each CDM defines a list of codes / vocabulary that are native to its CDM and internally unique to represent a concept. The CDM-defined static sets of common data elements (CDE) do not change from payload to payload and are uniform across different sites and data payloads.\nTechnically, terminology, vocabulary, and code system are not synonyms, but the community often uses these phrases interchangeably. The incoming data sets use well-defined terminology to encode clinical meanings in the data. For example, the International Classification of Diseases, 10th Revision, Clinical Modification (ICD-10-CM) is used for diagnoses which have complex ideas that include multiple, nuanced sub-elements. SNOMED CT is also used to encode clinical diagnoses which have internal hierarchies built upon increasing specificity (IS-A) and may also include relationships among the concepts (e.g., caused-by or finding-site). Logical Observation Identifiers Names and Codes (LOINC) are used to represent laboratory tests. ICD-10-PCS, HCPCS, and CPT4 codes are used for procedures. CVX codes are used for vaccines and RxNorm codes are used to specify the medications prescribed and administered. The string text used to indicate the code system categories are not uniform across the CDM models and some are case sensitive, i.e. “RxNorm” vs. “RXNORM”.3\nAll of the coded data found in the submitted payload are collected and translated into OMOP concept ids using the OMOP vocabulary, concept and concept relationship tables. Two categories of crosswalk mapping tables are generated to perform the translation; a static list of common data elements that are specific to CDM tables and a dynamic list of codes/code systems that can contain a varied list of values from payload to payload.\nStatic crosswalk common data element mappings are based on permissible values for a given column within a CDM table and they are well defined by the CDM specifications. CDE are uniform across different sites and data payloads. These CDE mappings include categories such as race, ethnicity, and gender. CDM-specific CDE values are reviewed and the corresponding list of OMOP concept ids is identified to create a crosswalk mapping table that can be used in the vocabulary transformation step in the pipeline. The curated crosswalk mapping table is available in the N3C GitHub repository.\n\n\n\nFigure 3.3: Static ValueSet N3C Mapping Table. A static value set mapping table is utilized to map values that are well-defined by CDM specifications.\n\n\nDynamic crosswalk mapping tables, on the other hand, are created during the transformation process in the pipeline based on the existence of codes that are found in the sites’ submitted payload. These mappings are created for each data payload to maximize the efficiency of the transformation process. For example some sites only submit a handful of distinct COVID-19 lab tests and would only need a short list of entries in the mapping table for the laboratory tests. The code used to generate the crosswalk mapping table is also available in the N3C GitHub repository.\n\n\n\nFigure 3.4: Dynamic N3C Mapping Table. A dynamic code and code system mapping table is utilized for mapping values that are not defined by the CDM specifications, and code and code system can vary by data partner.\n\n\n\n\n3.2.6 OMOP domain mapping\nThe OMOP CDM is person-centric. OMOP organizes the patient’s temporal EHR records in multiple domains; condition, procedure, drug, device, and measurement. OMOP domain mapping is the process of aligning data from different sources to the OMOP framework, using OMOP standard terminology and concepts, and ensuring that the data are populated in the correct OMOP domain table. This process is important because it helps to ensure that the data are consistent and can be easily understood and analyzed. By mapping data to the OMOP framework, researchers and analysts are able to compare and combine data from different sources, enabling more comprehensive and accurate insights into healthcare outcomes and trends.\nOMOP domain mapping typically involves the creation of mapping tables that translate the source code from different CDM data sources to the OMOP standard concept terminology and concept ids used in the OMOP vocabulary framework. The OMOP vocabulary dictates which source code should be placed in which target domain after it is translated into OMOP concepts. This vocabulary transformation using the OMOP concept relationships arranges the data into a well-organized and consistent format that can be easily analyzed and queried.\n\n\n3.2.7 N3C Global ID generation for all primary key fields\nThe incoming data sets submitted to N3C may or may not include their own primary keys. For those tables that do not include a primary key, one is generated such that each row of incoming data can be uniquely identifiable. Furthermore, in order to ensure the integrity and reliability of the data at scale, N3C automatically generates new unique IDs for all of the primary key fields as one of the steps in the data transformation pipeline. The data partner’s id is encoded in the key generation step to minimize primary key conflicts. For those rare occasions where primary key conflicts do occur, they are resolved by the primary key conflict resolution step in the transformation pipeline. This ensures that all of the primary keys are unique, even when data from multiple sites are combined and released as a whole. In addition, the data provenance is preserved so that the newly generated key can be traced back to the originating site’s row of data. The automated N3C DI&H pipeline is able to handle large volumes of data efficiently and effectively and ensures the quality and reliability of the data for research and analysis.\n\n\n3.2.8 Sensitive Information Screening\nThe N3C works with domain experts to screen sensitive information to protect patient privacy. N3C datasets are cleaned again in the pipeline before the data is released. The pipeline contains a process to remove any possible privacy-breaching data. This process includes tribal zip code truncation and Agency for Healthcare Research and Quality (AHRQ) code removal.\nTribal zip code truncation is a process that is used to protect the privacy of individuals who live on tribal lands in the United States. In order to protect the privacy of these individuals, all zip codes that represent a region where the majority of residents are American Indian (AI) or Alaska Native (AN) have been truncated to three digits. This announcement can be found in the N3C Training Area .\nFurthermore, data is scanned to remove any “Never Events” codes that may be left in the submitted data. The term “Never Events” was first introduced in 2001 by Ken Kizer, MD, former CEO of the National Quality Forum (NQF), in reference to particularly shocking medical errors—such as wrong-site surgery—that should never occur. AHRQ codes are “Never Events” that are used by AHRQ to identify specific healthcare treatments, procedures, and diagnoses in data sets. “Never Events” consists of 29 “serious reportable events” grouped into the following seven categories:\n\nSurgical or procedural events, such as surgery or other invasive procedure performed on the wrong body part.\nProduct or device events, such as patient death or serious injury associated with the use of contaminated drugs, devices, or biologics provided by the health care setting.\nPatient protection events, such as patient death or serious disability associated with patient elopement (disappearance).\nCare management events, such as patient death or serious injury associated with a medication error.\nEnvironmental events, such as patient or staff death or serious disability associated with an electric shock in the course of a patient care process in a health care setting.\nRadiologic events, such as death or serious injury of a patient or staff associated with introduction of a metallic object into the MRI area.\nCriminal events, such as any instance of care ordered by or provided by someone impersonating a physician, nurse, pharmacist, or other licensed health care provider or abduction of a patient/resident of any age.\n\nBy removing AHRQ codes indicating data elements deemed to contain or represent sensitive information, it becomes more difficult to identify specific individuals and their healthcare treatments, procedures, and diagnoses. The N3C has a list of AHRQ codes to screen out sensitive items, and these records have been removed from the datasets before the weekly releases. More detailed information about the list of the serious Never Events can be found here. Note that different healthcare organizations may classify incidents in different ways."
  },
  {
    "objectID": "chapters/cycle.html#sec-cycle-workflow",
    "href": "chapters/cycle.html#sec-cycle-workflow",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "3.3 N3C Data Harmonization Workflow",
    "text": "3.3 N3C Data Harmonization Workflow\nData harmonization is an essential step in the process of analyzing and understanding COVID-19 patient data, as it helps to bring data from different sources into a consistent format and structure. With the unprecedented amount of data being collected during the pandemic, it is especially important to ensure that this data is accurate, reliable, and comparable. N3C data is harmonized using the OHDSI standardized vocabularies and the clinical data is organized into OMOP CDM structures, which enables standardized analytics that leverage the OHDSI knowledge base when constructing exposure and outcome phenotypes and features within cohort characterization. Using common data standards and organizing data in common data structures allows researchers and analysts to gain a rapid comprehensive understanding of the health impact of the SARS-CoV-2 pandemic, and to use this information to inform decision-making and policy development.\nIn addition, unit harmonization is performed to rescue possibly unusable data where measurement units are missing. With over 11 billion rows of measurement data within N3C, the DI&H pipeline was built to maximize the usefulness of this valuable resource for researchers and analysts through the unit inference and harmonization pipeline step. Rows of patient data that are missing measurement units as well as rows of data that contain invalid laboratory measurement units undergo unit inference to rescue otherwise unusable rows of data.\n\n3.3.1 Unit Harmonization\nN3C has compiled one of the largest secure collections of clinical health data with over 23 billion rows of patient data from over 18 million patients spread across 75 data partners, including over 11 billion rows of laboratory measurement data, all harmonized in the OMOP CDM. In order to facilitate COVID-19 research, N3C has grouped ontologically similar OMOP measurement concepts together for 53 measured concept categories and developed a unit-harmonization pipeline to ensure that the data is consistent and comparable. Unit inference is performed in the following cases; cases where the measurement units are missing or in cases where the units are provided by the site and valid for the given lab, but the value distribution indicates that it is the wrong unit.\nThe massive amounts of measurement data in N3C enable the process of correcting units via inference based on the distribution of the existing data values for the same laboratory results. Furthermore, the same measurement data in different units are also converted to use the canonical units using the unit conversion table.\nThis unit-harmonization pipeline consists of several steps:\n\nSelect a canonical unit for each measurement variable.\nDevelop a formula for converting data values to this canonical unit.\nObtain a clinical review of each formula.\nApply the formula to convert data values in each unit into the target canonical unit.\nRemove any rows containing harmonized values that fall outside of accepted value ranges for the measurement.\n\nFor data with missing units for all the results within a lab test for a particular data partner, N3C compares the values to pooled values from all data partners using the Kolmogorov-Smirnov test.\n\n\n\nFigure 3.5: Unit conversion example for degree Fahrenheit to degree Celsius.\n\n\nIn order to help sites locally leverage the centralized information on measurement units from N3C, a useful unit harmonization and inference tool is published in the N3C GitHub repository. For more details please refer to the UHI-tool-for-sites page.\n\n\n3.3.2 Data Quality Checks\nThe N3C Data Enclave performs two types of data quality checks: automated data health checks during the ingestion phase, and manual data quality assessment checks with the help of the Data Quality Portal tool after the data ingestion pipeline steps are complete.\n\n3.3.2.1 Automated Data Health Checks\n\n\n\nFigure 3.6: A visual representation of the many ingestion and harmonization steps with data health checks.\n\n\nWhen the data ingestion pipeline is deployed for a new site, it includes a set of automated data health checks. The DI&H pipeline is run every time a data payload is submitted to N3C, and if the new data doesn’t meet the desired data health expectations, the pipeline automatically logs an issue and the pipeline administrators are immediately notified. The logged issues are triaged by the DI&H team, and if the issue requires communication with the site to correct, a site buddy is assigned to review the issue details for communication with the site to aid in correcting the site data that raised the data expectation failure.\n\n\n\nFigure 3.7: Automated Data Health Checks. Issues identified by the automated data health checks are resolved through issue tracking and communication with data partners.\n\n\n\n\n\nFigure 3.8: Data Health Checks and Code Templates. Templates are utilized to easily deploy ingestion logics and data health checks for new sites upon submission of data to N3C. Data provenance for over 5000+ transformation across 76+ sites datasets are automatically tracked.\n\n\nWithin N3C, template technology is utilized to deploy the data ingestion pipeline. The CDM-specific DI&H pipeline is deployed for a new site using the same data ingestion template written for each source CDM. Once deployed the data checks run automatically and are tracked within the Enclave. The data pipeline is refreshed within roughly 30 minutes whenever source data is updated at the sFTP site. The data ingestion pipeline templates are utilized to ingest over 75 data partners’ data. The ingested data that pass the data quality checks are available to be merged to the limited dataset (LDS) where they then become available to the research community.\n\n\n3.3.2.2 Manual Data Quality Checks through Data Quality Portal\nThe N3C Data Quality Portal (DQP) is a comprehensive tool and interface developed by the N3C Data Ingestion and Harmonization workstream to review the quality of the transformed data once the data ingestion pipeline is finished but prior to final data release to the N3C research community. The DQP is only accessible to the ingestion and harmonization team. It covers all aspects of data quality across different OMOP domain tables.\n\n\n\nFigure 3.9: Data Quality Portal. The Data Quality Portal provides a graphical user interface for assessing data quality of data partner data submissions.\n\n\nThe figure above shows a screenshot of the Data Quality Portal (showing the checks for the measurement domain).\nThe N3C DQP is based on the OHDSI OMOP Data Quality Dashboard (DQD), which provides a range of features and tools for identifying and addressing data quality issues. Through queries implemented in the N3C DQP, we are able to quickly assess after every new data build each site’s number of COVID-19 tests and their results, demographic counts by gender, race and ethnicity, counts of visit types, counts and percentages of non-standard concepts found in each domain, implausible death and birth dates, and visits with negative duration.\nThe DQP is used to determine the coding completeness of the site’s data and its overall fitness for use by the research community and whether further data improvement is required before the data can be released for research. The combination of both automated and continual assessment of manual data quality checks through DQP enables the scale and rigor required for the N3C research community. In addition to the capabilities provided by the DQD, the N3C DQP includes a set of COVID-19-specific data quality checks that focus on the coding of COVID-19 labs, patient encounters, and other data relevant to the SARS-CoV-2 pandemic. These checks help to ensure that the data is accurate, reliable, consistent, and support more comprehensive and informed analysis of the data.\n\n\n\nFigure 3.10: DQP Metrics. The Data Quality Portal also allows for comparison of data quality metric results between data partners.\n\n\nOne unique feature of the DQP is the ability to check distributions and patterns across different sites, which can help to identify and fix any abnormal patterns or anomalies in the data. This is particularly important for the N3C centralized data repository, as it enables the team to quickly identify and correct the data issues.\n\n\n\n3.3.3 Site Scorecard\nThe DI&H team has created site feedback reports called scorecards that could not only be sent to all sites but could also be used by the DI&H team in conjunction with the DQP to monitor and maintain optimal data quality for research across the Enclave. The scorecards were developed for sites by utilizing the DQP as a starting point for the development of analyses. Figures and tables were created using Palantir’s Contour application to provide indicators of site data quality, and to give sites greater visibility into their data.\n\n\n\nFigure 3.11: Site Scorecard. Site scorecards utilize charts and graphical representations of data partner’s submitted data to provide up-to-date data quality metrics that can be deployed in response to a successful ingestion of data from a data partner’s data submission.\n\n\nSite scorecards provide reports to all sites that are released in the Enclave. These reports are sent on a biweekly cadence, via the sFTP outgoing folder. They are created in an automated fashion and based on the latest data available for the site in the Enclave.\nEach site receives two scorecards: a site-specific report and a site-comparison report. The site-specific scorecard provides data quality information specific to the site’s latest payload. The site-comparison scorecard provides useful context via comparisons between the site and other anonymized sites on various metrics. Sites submitting data in the OMOP or PEDSnet CDM receive scorecards with comparisons to other OMOP sites, while sites submitting data in ACT, TriNetX, or PCORnet CDM receive scorecards with comparisons to sites with these non-OMOP data models.\nScorecards focus on several data quality areas including demographics, COVID-19 metrics, visits/encounters, measurements/labs, non-plausible dates such as dates in the distant past or future, use of non-standard concepts, and N3C-specific data enhancements such as social determinants of health and oxygen supplementation device data. Sites are also provided with contact information should they have further questions about their scorecard or need help troubleshooting data discrepancies.\n\n\n\nFigure 3.12: Inpatient Length of Stay Distribution Scorecard. This line graph displays the length of stay for inpatient visits in days using a log scale across all OMOP and PEDsnet sites.\n\n\nThe scorecards allow sites to be directly involved in the data quality improvement process by highlighting areas for targeted improvement in their local source data, which would ultimately result in data quality improvements in their N3C payloads. Additionally, the scorecards allow the DI&H team to monitor and maintain data quality across subsequent N3C data submissions and prevent any regression on those metrics. If the scorecards reveal that a released site is no longer passing key data quality metrics, then the site is unreleased until they are able to remediate their data quality issues."
  },
  {
    "objectID": "chapters/cycle.html#sec-cycle-releases",
    "href": "chapters/cycle.html#sec-cycle-releases",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "3.4 N3C Data Releases",
    "text": "3.4 N3C Data Releases\n\n3.4.1 De-Identification and release to LDS (L3)\nThe N3C Data Enclave hosts patient data at multiple levels of patient data completeness\nwith corresponding levels of restricted access. The most complete data set available within the Enclave is the limited data set (LDS), which is a type of dataset that has had certain protected health information (PHI) removed or de-identified in accordance with the Health Insurance Portability and Accountability Act (HIPAA). However, some data elements, such as dates (date of birth/death, admission, discharge, service, etc.), location data (city, state, five-digit zip code), and age are still available.\nThe DI&H workstream meets bi-weekly to review potential data quality issues found in site data submissions. Based on various data quality metrics of the submitted data from each site, the “release” or “no release” status flag is set for each data partner. Based on this release status flag, the pipeline determines whether or not to merge the site’s payload data to the LDS. The dimension of data quality checks includes internal and external validations, data conformance, and completeness that are based on the OHDSI data quality framework (Pfaff et al., 2022).\nGenerate SafeHarbor DataSets (L2)\nA Safe Harbor dataset is a type of limited data set (LDS) that has had certain identifying information removed or de-identified using the Safe Harbor method. The SafeHarbor dataset (L2) is generated by applying the following list of procedures to LDS (L3):\n\nThe zip code is truncated to the first 3 digits.\nAll patient-level date and datetime fields are shifted by keeping two sets of random ±180 numbers. One number is used to shift the time part and the other number is used to shift the date part. (Please note that a number of sites do pre-shift the date fields prior to submitting the data to N3C. The data partners who pre-shift the date field can be identified by the maxDateShift column of the manifest table.)\nAfter creating the randomized date/time shifting numbers for a patient new to the Enclave, the patient-level date shift is stored for each subsequent payload, such that the same number is used for the patient from the patient’s initial data load and every data load thereafter.\nTimes are shifted ±180 minutes from the actual time. All times are shifted by a consistent base time shift for all patients. However, an additional secret fuzz factor is applied to each time shift by an additional ±30 minutes and/or seconds, and this secret fuzz factor is patient-specific.\nThe date part is shifted ±180 days after the time shift is performed, as the time shift can cause the date to rollover to the next or previous date.\n\nFor more details on generation of the SafeHarbor dataset, please refer to the GitHub page."
  },
  {
    "objectID": "chapters/cycle.html#sec-cycle-enhancements",
    "href": "chapters/cycle.html#sec-cycle-enhancements",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "3.5 N3C Data Enhancements",
    "text": "3.5 N3C Data Enhancements\nAs part of N3C’s efforts to support COVID-19 and long COVID research, N3C requested sites that are able to add new data elements to their common data models (CDMs). These data elements help to create a more comprehensive and rich dataset that can support more in-depth and informative analyses of the SARS-CoV-2 pandemic. By adding these data elements, sites can contribute to the collective knowledge about COVID-19 and help to inform the development of effective treatments, policies, and strategies to address the pandemic. N3C is committed to working closely with participating sites to ensure that the data collection process is efficient and straightforward and that the data is accurately and reliably captured and integrated into the N3C dataset.\n\n\n\nFigure 3.13: N3C Data Enhancements. N3C has also sought to expand the clinical data provided by data partners by requesting data enhancements to data partner data submissions, including long COVID clinic visits, ADT transactions, oxygen devices, NLP, and SDoH datasets. In addition to N3C data sets, PPRL linked external sources such as CMS and mortality evidence datasets have been added and currently available for research.\n\n\n\n3.5.1 Long COVID Clinics\nWhile many patients recover from COVID-19 within a few weeks or months, a significant proportion of patients continue to experience ongoing symptoms, such as fatigue, shortness of breath, and mental health problems, for months or even years after the initial infection. This phenomenon, known as long COVID, can have a significant impact on a patient’s quality of life and ability to function normally.\nBy collecting data from long COVID clinics, researchers and healthcare providers can better understand the prevalence, severity, and course of long COVID, as well as the risk factors and protective factors that may influence the likelihood and severity of long COVID.\nLong COVID clinic data is collected in the N3C Data Enclave. For more details, please refer to this GitHub page.\n\n\n3.5.2 ADT Transactions\nADT transactions (also known as Admission, Discharge, and Transfer transactions) encompass any event, transition, or change in a patient’s location, department, level of service, or accommodation that would be recorded in a Health System’s Admission, Discharge, and Transfer hospital information system. ADT Transactions are recorded when a patient presents to a health care system, and are tracked during a patient’s stay until departure. This information is used to update patient records, track patients’ movements within the facility, and support other administrative and clinical functions.\nThough specifics vary by the data model, the following apply to all sites completing the ADT transaction data enhancement.\n\nADT transaction data does not require changes to the visit occurrence/encounter tables.\nADT transaction data must be mapped to one of three designated codes to represent ICU, Emergency Department, and all other inpatient transactions.\nSites should include raw/source values in the appropriate fields in their CDM. These fields will be used for validation and harmonization purposes and will not be exposed to end users in the N3C Enclave.\nIf the site has observation visits captured in ADT transaction data, the related transactions do not need to be included in the data submitted to N3C. Should the site choose to include them, an appropriate code, other than the three chosen for N3C’s ADT data, should be chosen.\n\nThe data is stored in the VISIT_DETAIL table in the Enclave. For more information, please refer to this GitHub page.\n\n\n3.5.3 NLP\nWhether they are clinical notes, radiology notes, or some other note type, notes that are present in EHR systems potentially contain a wealth of patient information that may not be present in standardized or structured fields. Although there may be some structured data elements present in these notes that are actionable within the EHR, most of the note information will be free-text that must be processed in order to fit in a structured data model like OMOP. In order to process the note data, it is necessary to use Natural Language Processing (NLP) techniques. Once processed, the data is available to researchers in the OMOP NOTE_NLP table.\nFor more details about the N3C NLP data, including the NLP submission process and GitHub repository containing tools for processing notes, please refer to this GitHub page.\n\n\n3.5.4 O2 Supplement Devices\nO2 device data is critical for N3C research, as a significant number of hospitalized COVID-19 patients have required oxygen supplementation and/or ventilation support using O2 devices or ventilators. However, the current common data model (CDM) does not capture oxygen supplementation with sufficient granularity, which can limit the usefulness of the data for research purposes.\nTo address this issue, N3C is working to improve the data on oxygen supplementation and ventilation support in the CDM, including by collecting more detailed data on the types and modes of O2 delivery and ventilation support that patients receive. This will help to provide a more comprehensive and nuanced understanding of the role of oxygen supplementation and ventilation support in the care of hospitalized COVID-19 patients.\nFor more details about the N3C O2 Device data please refer to this GitHub page.\n\n\n3.5.5 Social Determinants of Health\nSocial Determinants of Health (SDOH) refer to the social, economic, and environmental factors that can impact an individual’s health and well-being. These factors can include things like income and education level, housing and transportation, and access to healthcare and other resources. While N3C has made available several external datasets containing SDOH data that can be incorporated into a research project (See the Public/External Datasets section of the Understanding The Data chapter), N3C has also requested that data partners that routinely collect SDOH data provide that data as part of their data submissions, if they are able.\nFor more details about the N3C SDOH data please refer to this GitHub page.\n\n\n3.5.6 PPRL\nAdditional datasets beyond those submitted by N3C clinical data partners are available within N3C and linked through Privacy Preserving Record Linkage (PPRL). More information about PPRL and the datasets available through this linkage can be found in the Understanding The Data chapter.\n\n\n3.5.7 CMS\nThe Enclave is a continuously refreshed data resource following the Observational Medical Outcomes Partnership (OMOP) Common Data Model. However a patient’s data from the EHR may be missing data from the patient’s healthcare journey outside of hospital settings. The missing portion of health care received by the patient may include visits to the pharmacy, visits to the outpatient clinic as well as the non-hospital institutional care facilities. It may also be missing telehealth visits and the purchasing of equipment or devices to be used at home. Often data from these outside-of-hospital visit settings are not captured via the EHR. Using PPRL technology, we linked N3C patients with CMS patients and supplemented N3C EHR information using a comprehensive CMS claims dataset. For the PPRL-linked patients we provide additional datasets that can be referenced to render a more holistic view of the patient’s healthcare journey. This work is currently ongoing. The approach and methods to construct the claims data with encounters will be updated soon. The CMS data is used to complement and augment the N3C OMOP datasets.\nAdditional details on the CMS datasets can be found in the Understanding The Data chapter.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nDong, X., Li, J., Soysal, E., Bian, J., DuVall, S. L., Hanchrow, E., Liu, H., Lynch, K. E., Matheny, M., Natarajan, K., et al. (2020). COVID-19 TestNorm: A tool to normalize COVID-19 testing names to LOINC codes. Journal of the American Medical Informatics Association, 27(9), 1437–1442. https://doi.org/10.1093/jamia/ocaa145\n\n\nPfaff, E. R., Girvin, A. T., Gabriel, D. L., Kostka, K., Morris, M., Palchuk, M. B., Lehmann, H. P., Amor, B., Bissell, M., Bradwell, K. R., et al. (2022). Synergies between centralized and federated approaches to data quality: A report from the national COVID cohort collaborative. Journal of the American Medical Informatics Association, 29(4), 609–618. https://doi.org/10.1093/jamia/ocab217"
  },
  {
    "objectID": "chapters/cycle.html#footnotes",
    "href": "chapters/cycle.html#footnotes",
    "title": "3  Data Life Cycle - From Patients to N3C Researchers",
    "section": "",
    "text": "See https://covid19.who.int/↩︎\nSee https://covid.cdc.gov/covid-data-tracker/#datatracker-home↩︎\nSee https://mmshub.cms.gov/sites/default/files/Codes-Code-Systems-Value-Sets.pdf.↩︎"
  },
  {
    "objectID": "chapters/governance.html#sec-governance-preamble",
    "href": "chapters/governance.html#sec-governance-preamble",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.1 Preamble",
    "text": "4.1 Preamble\nN3C is more than simply a data N3C Data Enclave. It is a research community dedicated to facilitating collaborations and the rapid generation and dissemination of knowledge to help combat COVID-19. There were numerous innovations along the pathway to N3C. The novel governance procedures and structures described here may benefit other collaborative research efforts beyond the pandemic emergency response."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-leadership",
    "href": "chapters/governance.html#sec-governance-leadership",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.2 N3C Leadership",
    "text": "4.2 N3C Leadership\nN3C is a public-private-government partnership among the Clinical and Translational Science Awards (CTSA) Program hubs supported by the National Center for Advancing Translational Sciences (NCATS) as the overall data steward; the National Center for Data to Health (CD2H); the Institutional Development Award Networks for Clinical and Translational Research (IdeA-CTR); data contributing organizations (e.g., public health Health Information Exchanges, hospitals/health systems, academic medicine, and clinical research); distributed clinical data networks (PCORnet, OHDSI, Act, TriNetX); multiple commercial partners; and a large community of researchers.\nFour co-leads (two from the researcher community and two from NCATS) developed the vision for N3C and led its implementation with extensive community consultation. Their vision was to bridge the traditional silos of biomedical research to rapidly enable team science while respecting the rights of data subjects, institutions, and researchers. To accomplish this at record speed, the N3C leadership established five community workstreams. Five Community workstreams were rapidly organized: Data Partnership & Governance, Phenotype & Data Acquisition, Data Ingestion & Harmonization, Collaborative Analytics, and Synthetic Data. focusing on specific N3C components. The workstreams are supported by dedicated administrative staff, with virtual workspaces and communication channels (Slack) to share materials and reduce barriers to participation. The workstreams meet regularly, and meetings are open to anyone interested in participating or monitoring activities. Early on, the N3C conducted general webinars and Questions and Answers sessions to inform potential partners and created a Welcome Partnership Packet to help initiate participation."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-culture",
    "href": "chapters/governance.html#sec-governance-culture",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.3 Establishing a collaborative governance culture- The Partnership and Governance Workstream",
    "text": "4.3 Establishing a collaborative governance culture- The Partnership and Governance Workstream\nAchieving the goals of a shared data resource involves deciding how the resource may be produced, operated, used, and sustained. Critical considerations for establishing a data-sharing governance framework include:\n\nDeveloping a data-sharing strategy (i.e., deciding what data will be shared and whether sharing will be bilateral or multilateral, whether the data will be centralized or distributed /decentralized),\nUnderstanding the legal, regulatory, and organizational compliance requirements and establishing the data sharing agreements (i.e. identifying potential restrictions on data sharing and use due to laws, regulations, or ancillary agreements, and understanding party’s legitimate interests)\nSelecting the technical environment where sharing will occur (i.e. selecting the technical infrastructure and the security measures to be implemented to protect data integrity, assessing risks and managing data access)\nPreparing the data for sharing, retention and disposition (i.e. optimizing data usability, selecting data processing methods and adopting FAIR principles)\nMonitoring data sharing (i.e. establishing a monitoring and reporting plan, enforcing policies and procedures, and resolving disputes)\n\nThe Data Partnership and Governance Workstream established by the N3C PIs with participation from NCATS and the scientific and ethics community is responsible for (1) establishing and refining the principles, policies, and procedures to support N3C, (2) determining the roles and functions to support the governance strategy, and (3) making recommendations to NIH for their implementation. The workstream convenes representatives of contributing organizations, researchers, and institutions that support them, NCATS, ethicists, and anyone interested in advancing N3C.\nAt the start, the Partnership and Governance workstream met twice weekly to rapidly align on an N3C governance vision and establish the universal conditions governing the use of N3C data. These conditions are that data protection is adopted by default, contributing sites are not identified, and workspaces must be provisioned on a per-project basis only for COVID-19-related research. Also, data cannot be extracted or downloaded from the data N3C Data Enclave, with the exception of publishable summary results or figures, each of which must be reviewed prior to export (see Chapter 10 for details on this process).\nThe workstream split into a smaller subgroup to draft the supporting governance documents while continuing to meet with the whole Workstream for ideation, context, and feedback weekly. Onward, the workstream adjusted the frequency of the meetings to the work cadence."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-bridges",
    "href": "chapters/governance.html#sec-governance-bridges",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.4 N3C Governance Bridges Individual Oversight Responsibilities",
    "text": "4.4 N3C Governance Bridges Individual Oversight Responsibilities\nThe N3C Governance is a set of behavioral norms, policies, and procedures supported by technology/security measures and oversight mechanisms. The N3C Governance structure relies on a variety of collaborative approaches, from hierarchical to consultative to consensus-based. The Data Partnership and Governance Workstream advocates a partnership approach that builds on collective competencies where governance aligns with community values, and final sign-off is obtained according to leadership roles and responsibilities. This governance approach values input from stakeholders and helps develop a shared understanding while recognizing the different risks and obligations of the scientific community and NCATS. Governance policies are posted on Zenodo to promote transparency and elicit community feedback. This also provides a resource for other researchers facing similar governance needs.\nThe N3C Governance framework was developed with guidance from the NIH Office of the General Counsel. As data steward, NCATS has fiduciary responsibility for the N3C Data Enclave and adjudicating access. As such, NCATS is responsible for establishing the terms for data contribution and use, user training requirements, and Enclave usage accountability mechanisms. Participating in N3C necessitates both organization-level agreement(s) and user commitments to the N3C User Code of Conduct and other N3C governance requirements.\nAdditionally, the scientific community representatives in the Participation and Governance Workstream, set the behavioral and ethical expectations and norms, including the N3C Community Guiding Principles to support collaborative research and the Attribution and Publication Policy to clarify the acknowledgment and authorship expectations for disseminating N3C-enabled research. The N3C Community Guiding Principles include a Diversity and Inclusion statement highlighting the commitment to create a safe and welcoming environment for all participants to N3C. It also has an Ethics statement highlighting the community’s core values. The scientific community takes a holistic approach to enforcing the Community Guiding Principles. A Community Response Team follows a conflict resolution process emphasizing dialog and mutual respect to adjudicate complaints and solve disagreements.\nSeven principles summarize the Community Guiding Principles:\n\nPartnership: N3C community members are trusted partners committed to honoring the N3C community guiding principles and N3C User Code of Conduct\nInclusivity: N3C is open to any organization that wishes to contribute data, code, and ideas, as well as anyone who registers to use N3C data to conduct COVID-19-related research, including citizen/community scientists\nTransparency: Open processes and reproducible research is the hallmark of N3C and good scientific practice. Access to data is project-based and focused on COVID-19 research questions. Descriptions of projects are posted publicly and are searchable to promote collaborations.\nReciprocity: Contributions are acknowledged and results from analyses, including provenance and attribution, are expected to be shared with the N3C community.\nAccountability: N3C community members take responsibility for their activities and hold each other accountable for achieving the N3C objectives and acting through good scientific practices.\nSecurity: All activities are conducted in a secure, controlled access cloud-based environment and are recorded for auditing and attribution purposes.\nMutual respect: Communications should be professional, concise, clear, and relevant. Follow proper communication etiquette. Avoid excessive conflict, unprofessional arguments, ad hominem attacks, and/or ridicule over chat and in messaging.\n\n\n\n\nFigure 4.1: N3C shared Governance initiatives with sign-off responsibility represented."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-ethics",
    "href": "chapters/governance.html#sec-governance-ethics",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.5 Ethical oversight",
    "text": "4.5 Ethical oversight\nThe ethical oversight of N3C is two-fold. First, the IRB at John Hopkins University (JHU’s IRB) approved the N3C protocol and methodologies employed to create N3C data resources. Institutions must obtain relevant authorizations, including applicable IRB approvals or waivers, before contributing data to N3C. Institutions can select their own IRB or rely on JHU’s IRB as the Central IRB for N3C. Relying institutions must use the SMART-IRB online reliance system and execute a Master Reliance Agreement to delegate IRB oversight to JHU’s IRB for their contribution to N3C. While it is not necessary to use JHU’s IRB, the reliance process is simple and efficient, and relying on a single IRB reduces the burden associated with contributing data and speeds up IRB review.\nSecond, NCATS obtained IRB approval for the N3C Data Enclave from the NIH IRB since NIH houses the Enclave under NCATS oversight. NCATS also received a Certificate of Confidentiality to resist legal requests to disclose identifying information from the people represented in N3C data."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-contractual",
    "href": "chapters/governance.html#sec-governance-contractual",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.6 Contractual Agreements: Separating Data Transfer and Data Use",
    "text": "4.6 Contractual Agreements: Separating Data Transfer and Data Use\nThe N3C Community contributed to the two types of contractual agreements: ones that delineate data contribution (or data transfer) and others that define data access (or data use) conditions. Both types of agreements clarify confidentiality, intellectual property, warranties and liabilities, conflict resolution, and expiration or termination. The data transfer and use agreements are intentionally separated since contributing data is not required to be eligible to access the data. All contractual agreements are made between NCATS and signing officials from home institutions\nAn N3C Data Transfer Agreement (DTA) must be executed between NCATS and a contributing organization before they can submit health record data.\nN3C leaders saw the value of enhancing the N3C research capability and developing the governance to enable privacy-preserving record linkage (PPRL) between N3C data and other data sources. PPRL enables linking records across multiple datasets without revealing the identity of the data subject. Data-contributing organizations willing to participate in this data linkage pilot must execute a separate Linkage Honest Data Broker Agreement(LHBA) with NCATS and the Linkage Honest Broker. Again, separating the DTA and LHBA provides the flexibility to contribute data without enrolling in the PPRL.\nA separate Data Use Agreement (DUA) must be executed between NCATS and signing officials from an institution whose investigators wish to access N3C data. To improve efficiency, instead of executing a traditional pair-wise agreement each time a researcher needs access to N3C, a single DUA is executed between NCATS and an organization to render individual researchers eligible to request access to N3C content."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-access",
    "href": "chapters/governance.html#sec-governance-access",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.7 Data Access by Researchers",
    "text": "4.7 Data Access by Researchers\nFirst-time users wishing to access N3C must verify that their institution has executed a DUA with NCATS. Then users must register an account with N3C, including agreeing to the Community Guiding Principles and complete the Information Security and Management Refresher course of the NIH Information Security and Information Management Training series. Only then can users submit a Data Use Request (DUR). NCATS established a Data Access Committee (DAC) responsible for reviewing and approving DURs and addressing user questions. Users must submit a new DUR for every separate study. (See Chapters 5 and 6 for details.)\nThere are three levels of data access commensurate with the sensitivity of the data. See Chapter 6 for more details about these levels.\n\n\n\nFigure 4.2: Data Tiers.\n\n\nAccess to each data level requires approval from the Data Access Committee. In addition, investigators requesting access to level 3 data must submit proof of IRB approval from their institution or an independent accredited IRB for their intended use. Note that PPRL data is only available by special request in level 3 projects with appropriate additional IRB approval.\nN3C governance includes considerations for protecting special populations and communities, and respect for tribal sovereignty. N3C contains no Tribal affiliation. Using ZIP code information to make assumptions about Tribal affiliation is not valid or appropriate.\n\n\n\nFigure 4.3: Data Access Governance Process. To lower the burden on participation, user authentication and eligibility are established once during account registration. Eligible users can then submit their Data Use Request for evaluation by the Data Access Committee. A new request must be submitted for each specific project."
  },
  {
    "objectID": "chapters/governance.html#sec-governance-incident",
    "href": "chapters/governance.html#sec-governance-incident",
    "title": "4  Governance, Leadership, and Operations Structures",
    "section": "4.8 Incident Notification and Escalation procedure",
    "text": "4.8 Incident Notification and Escalation procedure\nIt is essential to create an Incident Notification Policy to ensure that the right people are notified about incidents at the right time and that problems can be addressed rapidly. Incidents include interpersonal interactions or disputes, suspected terms/policies violations, technical issues such as systems interruption or process delays, a potential data breach, and attribution or publication situations. For each incident type, an escalation path clarifies the first line of response and any further process or action for escalating it based on severity.\nUsers can report interpersonal conflicts to the “Report Conduct Concerns” page on the website. All concerns reported are taken seriously and handled with discretion. A Community Response Team of 2-3 individuals nominated by the community follows a conflict resolution process to help parties solve disagreements in good faith with constructive dialog and escalate to NIH if relevant.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page."
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-eligibility",
    "href": "chapters/onboarding.html#sec-onboarding-eligibility",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.1 Researcher Eligibility",
    "text": "5.1 Researcher Eligibility\nCitizen scientists, researchers from foreign institutions, and researchers from U.S.-based institutions are all eligible to have access to the N3C Data Enclave. Everyone with an Enclave account has access to the tools and public datasets that are available in the Enclave.\nThere are several levels of Electronic Health Record (EHR) data that are available within the Enclave. (For more information about the levels of data, see the section ‘Description of Levels 1, 2, 3’ in the ‘Getting & Managing Data Access’ chapter. LINK NEEDS TO BE ADDED HERE)\nCitizen scientists are only eligible to access synthetic data (Level 1). This data is artificial but statistically comparable to, and computationally derived from, the original EHR data.\nResearchers from foreign institutions are eligible to access synthetic data (Level 1) and patient data that has been deidentified by removing protected health information (PHI) (Level 2). (PHI includes 18 elements defined by the Health Insurance Portability and Accountability Act (HIPAA).)\nResearchers from U.S.-based institutions are eligible to access synthetic data (Level 1), deidentified patient data (Level 2) and patient data that includes dates of service and patient zip code (Level 3). (The latter data set is referred to as a limited dataset because it contains only 2 of the 18 PHI elements.)"
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-registration",
    "href": "chapters/onboarding.html#sec-onboarding-registration",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.2 Registration",
    "text": "5.2 Registration\n\n5.2.1 ORCiD; InCommon vs Login.gov\n\n\n5.2.2 NIH IT security training\n\n\n5.2.3 Human Subjects Training\nDue to the secure nature of the data that is available in the N3C Data Enclave registration of users is key. There are two options in which users can log in and create an account, InCommon or Login.gov. The InCommon pathway is available to select institutions that participate in that identity management service. You can click the link to confirm if your organization participates. If your institution does not participate with InCOmmon, you will need to create a login.gov account. Use the link to Login.gov and complete the required fields to create an account. Once you know which pathway you will use to create an Enclave account there are other security measures that are put in place, you will need to have an ORCiD, complete NIH security training, and also human subjects training.\nORCiD, which stands for Open Researcher and Contributor ID, is a unique identifier free of charge to researchers.\nThe Enclave is hosted by National Center for Advancing Translational Sciences and all researchers must complete the “Informational Security, Counterintelligence, Privacy Awareness, Records Management Refresher, Emergency Preparedness Refresher” course. The course can be accessed at https://irtsectraining.nih.gov/public.aspx. The course takes approximately 60-90 minutes to complete and you should print your certificate of completion. Users need to complete Human Subjects training that aligns with their institution’s guidelines. You will need to provide the date of completion as part of Enclave creation.\nOverall, users will need to confirm if they use the InCOmmon or Login.gov pathway, register for an ORCiD, have completed NIH Security Training, and completed institution human subjects training."
  },
  {
    "objectID": "chapters/onboarding.html#data-use-agreements",
    "href": "chapters/onboarding.html#data-use-agreements",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.3 Data Use Agreements",
    "text": "5.3 Data Use Agreements\nThe data use agreement (DUA) establishes the permitted uses of the data in the N3C Data Enclave. By signing the agreement, an institutional official is assuring that users from their institution will abide by the terms defined in the agreement.\nA DUA must be executed by National Center for Advancing Translational Science (NCATS) and a research institution. The DUA must be signed by authorized institutional officials who have the authority to bind all users at their institution to the terms of the DUA. (A citizen scientist who is not affiliated with an institution must execute a data use agreement with NCATS.) A DUA will be in effect for five years from the DUA Effective Date.\nEvery individual who has access to the N3C Data Enclave must be covered by a DUA. This DUA must be in place before an account for the N3C Enclave is requested. If your institution has an active DUA, there is no additional action required with regard to the DUA. A list of institutions with DUAs in place can be found at List of DUA Signatories: (https://covid.cd2h.org/duas).\nThe Institutional Data Use Agreement form is available at:\nhttps://ncats.nih.gov/files/NCATS_N3C_Data_Use_Agreement.pdf\nFor more information see:\nhttps://ncats.nih.gov/n3c/resources/data-access"
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-enclave-access",
    "href": "chapters/onboarding.html#sec-onboarding-enclave-access",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.4 Enclave Access",
    "text": "5.4 Enclave Access"
  },
  {
    "objectID": "chapters/onboarding.html#research-project-teams",
    "href": "chapters/onboarding.html#research-project-teams",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.5 Research Project Teams",
    "text": "5.5 Research Project Teams\n\n5.5.1 Project Lead vs Collaborations\n\n\n5.5.2 Common roles and expectations (PIs, PMs, SMEs, Analysts, …)\n\n5.5.2.1 Expertise needed"
  },
  {
    "objectID": "chapters/onboarding.html#sec-onboarding-dt",
    "href": "chapters/onboarding.html#sec-onboarding-dt",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.6 Domain Teams",
    "text": "5.6 Domain Teams\nThe N3C Data Enclave is built for multi-site collaboration and aims to bring together researchers of different backgrounds with similar questions using domain teams. Because N3C is multi-site, it can be difficult to collaborate with researchers of different backgrounds from different sites. Domain Teams exist to alleviate this difficulty. Some collaboration examples could be collecting pilot data for grant submission, sharing methodology and cohort logic, or learning how to use tools for large-scale data like machine learning.\nFor example, let’s say your institution just signed the DUA and you have some questions about the relationship between rurality and COVID treatments. You can look at the list of domain teams to see rural health. Then you can get in contact and go to the next upcoming meeting. At the meeting, you can find out whether your questions are already part of an existing project within the domain team, or if a new project should be created.\nIf you don’t see your type of question belonging to any existing domain teams you can create a new one here:\nhttps://n3c-help.atlassian.net/servicedesk/customer/portal/2/group/3/create/58\nSee here for a list of existing domain teams:\nhttps://covid.cd2h.org/domain-teams"
  },
  {
    "objectID": "chapters/onboarding.html#browsing-researchersprojectsinstitutions",
    "href": "chapters/onboarding.html#browsing-researchersprojectsinstitutions",
    "title": "5  Onboarding, Enclave Access, N3C Team Science",
    "section": "5.7 Browsing Researchers/Projects/Institutions",
    "text": "5.7 Browsing Researchers/Projects/Institutions\n\n5.7.1 Object Explorer, Public Dashboard\nOnce you have an Enclave account you can log in and use the object explorer to browse researchers and research projects. The object explorer can be found on the left-hand side of your view on the Enclave homepage. Click on Object Explorer and there are several object-type groups, to search researchers and projects, click on N3C Admin, from there you can search Data Use Requests, N3C Researchers, and Research Projects. If you are looking for a particular N3C researcher, you can click on that box and in the search bar type in their name and hit enter. A new box will be displayed and you can click on that researcher’s name and from there you can see the research projects that are lead of or a collaborator on. You can go back to the Object Explorer, object type groups, click N3C Admin again, and search research projects by clicking that box. Using the search bar at the top of the page you can search by keyword. Type in the keyword and click enter and a results box will be displayed. You can view all results to find the project in which you are interested in joining. From that screen, you can select the title of the project that you are interested in joining or reading about. There is a public-facing version of searching projects in addition to using the Enclave as a search method. Users can search https://covid.cd2h.org/projects or https://covid.cd2h.org/dashboard/exploration#projects and search for title, lead investigator name and also the institution. There are many features that are available to search using the public-facing dashboard. There are four categories of.\n\n\n\n\n\n\nNote\n\n\n\nThis chapter is still being written."
  },
  {
    "objectID": "chapters/access.html#sec-access-background",
    "href": "chapters/access.html#sec-access-background",
    "title": "6  Getting & Managing Data Access",
    "section": "6.1 Background: N3C Protected Data Levels",
    "text": "6.1 Background: N3C Protected Data Levels\nNot all data in the N3C Data Enclave requires an approved DUR to access–mock datasets and publicly-available data (e.g., US census data) are accessible by everyone with Enclave access. These low-risk data are covered more in Chapters 7 and 10.\nThe harmonized EHR data that do require an approved DUR to access are made available in three different “levels,” each with different amounts of data obfuscation, and correspondingly different access requirements. Deciding which level of data is appropriate for your study is important, because accessing Level 3 data is more work and restrictive than accessing Level 2 data. On the other hand, some studies can be accomplished with only Level 3 data. Note that if you start with a lower level of data, it is possible to “upgrade” a project’s access level, though all participants in the project will need to complete another DUR for the new level.\nIn addition to the primary Level 1, 2, and 3 datasets are “PPRL” data. PPRL data includes extra non-EHR sources of information such as obituary-based mortality records and viral variant sequencing information. These are available alongside only Level 3 data as an optional add-on; we’ll discuss PPRL in more detail below.\n\n6.1.1 Level 3, Limited Data Set (LDS)\nLevel 3, or LDS data is the most complete and protected (the term “limited data set” is defined by HIPAA and may contain a limited set of potentially identifying information). This dataset contains two pieces of Protected Health Information (PHI) defined by HIPAA: full, 5-digit patient zip codes,1 and accurate dates of events and services (except for dates of birth which are limited to month and year).2 Level 3 data are in the OMOP common data model, with some N3C-specific additions and conveniences, and are versioned as releases as described above. (OMOP and N3C-specific additions are covered in Chapter 7.)\n\n\n6.1.2 Level 2, De-Identified\nLevel 2 data, also known as De-Identified data, contains nearly the same information as the Level 3 data, but the two PHI-containing fields are further anonymized. Zip codes available in Level 3 are truncated to just the first 3 digits, and all recorded dates are shifted randomly, where the range of the random shift is ±180 days. This is not as dramatic a research limitation as it may seem, because the random shift amount is determined per patient: all dates for a given patient are shifted by the same (unknown, random) amount, allowing identifying sets of patients who had, for example, a positive COVID-19 PCR test and within 14 days received a given drug treatment.3 Level 2 data would not be appropriate for studies considering absolute timing, such as whether a patients’ primary COVID infection occurred during the Delta wave. Such questions are best answered by the LDS data.\nThe Level 2 data are also in OMOP format and versioned as releases. We’ll forgo examples of notional data because the format is exactly the same as for Level 3, LDS data.\n\n\n6.1.3 Level 1, Synthetic\nLevel 1, or Synthetic,4 data provide the most anonymous view of the harmonized data, and are quite different from the Level 2 and 3 datasets in both format and content. Rather, Level 1 data were generated from a statistical model of a researcher-defined subset of the Level 3 data. This means Level 1 data contain no real patient records at all, but only a synthetic derivative designed to be statistically similar. The generation process is handled by a private company, MDClone, whose proprietary algorithms also look for resulting information that is too similar to real patient information, potentially resulting in a loss of patient privacy. Such records are masked with “censored” values in the resulting dataset.\nBecause this statistical-modeling approach doesn’t scale to the entirety of an EHR database, it is generated on subsets of data of interest to researchers, with the assistance of MDClone representatives. To take an example, consider a research team interested in outcomes of COVID-19 in diabetic vs. non-diabetic individuals. Inside the Enclave, the team initiates a request with an MDClone liaison, suggesting that they would like to collect a set of patient information (with one row per patient in the resulting table), with columns for patient_age (at the date of their first COVID-positive PCR test result), has_diabetes (indicating if their record contains a diabetes diagnosis), days_until_ventilation (number of days elapsed between their COVID-positive test and subsequent mechanical ventilation, or null if they were not ventilated within 30 days), and several other potential confounding variables. The MDClone liaison then collects this requested information from the Level 3 data, develops a representative statistical model, and delivers a table of synthesized rows from the model with the columns requested to the team.\nLevel 1 datasets are generated from the most recent Level 3 release at the time of generation, and not automatically updated as new primary N3C data arrives. Notes for each Level 1 dataset describe the date of generation and information included for future reference.\n\n\n\n\n\n\nUpdate: As of mid-2022, N3C is no longer able to generate new synthetic datasets at researcher request. The previously-generated datasets are still available for use however, and gaining access to Level 1 data provides access to all generated synthetic datasets.\n\n\n\n\n6.1.3.1 Example N3C OMOP Data Format\nTo give a sneak preview of the primary N3C data format, here’s a snapshot of a few columns of notional (fake) data in OMOP format from the condition_occurrence table:\n\n\n\nFigure 6.1: A subset of rows and columns for mock condition_occurrence data. Notice the N3C-added data_partner_id, these are anonymized identifiers for sites contributing data, labeling each record with a source.\n\n\nNotice in particular the columns for data_partner_id (a pseudo-random identifier assigned to each data partner), condition_concept_id (the OMOP identifier uniquely describing the condition used for data filtering and analysis), and condition_concept_name (the human-readable version of condition_concept_id, used for convenience but not for data filtering). Other OMOP tables link records by shared columns; here are a few columns from the person table describing basic demographics of patients:\n\n\n\nFigure 6.2: Example data for the OMOP person table.\n\n\nSome other tables available with Level 3 data are specific to N3C; here are a few columns from a notional manifest table providing information about data partners, including the source CDM used by the data partner, the dates of data extraction and submission to N3C, and whether the data partner performs random date shifting prior to submission to N3C:\n\n\n\nFigure 6.3: An example of the N3C-specific manifest table. This table provides information about data partners, such as their source common data model (cdm_name), whether they pre-shift dates (see Chapter 3), and when their last submission was.\n\n\n\n\n\n6.1.4 PPRL Data\nPPRL, short for “Privacy-Preserving Record Linkage,” is a strong cryptographic data handling technique allowing for the matching of records about individuals from different data sources, without revealing to any party except the data sources themselves any identifying information about the individual. While we won’t describe how the process works here, researchers with Enclave access can read more about it in Chapter 7 and the Introduction section of the PPRL training module  in the Training Portal (see Section 11.4)."
  },
  {
    "objectID": "chapters/access.html#sec-access-availability",
    "href": "chapters/access.html#sec-access-availability",
    "title": "6  Getting & Managing Data Access",
    "section": "6.2 Level 1/2/3 (and PPRL) Availability",
    "text": "6.2 Level 1/2/3 (and PPRL) Availability\nLevel 3, LDS data are available only to researchers affiliated with US-based organizations such as universities and medical schools. As we’ll discuss below, the most significant access requirement for Level 3 data is a letter of determination from the researchers’ local Institutional Review Board (IRB), so this level of data is also available to other US-based organizations with the ability to work with an Institutional Review Board, or IRB (e.g., pharmaceutical companies).\nLevel 2, De-Identified data are available to researchers at both US and approved foreign organizations (those that can sign an institutional Data Use Agreement (DUA), see Chapter 3).\nLevel 1 Synthetic data are available to researchers at US and approved foreign organizations (though note the update about Level 1 data generation above), as well as individuals without any affiliation with a research organization, i.e., “citizen scientists.” This level of data thus provides opportunities for public outreach and engagement with topics in clinical science. Citizen scientists must nevertheless complete some training requirements and sign the legal Data Use Agreement document. Minors may apply for access, but only via their parents’ or legal guardians’ consent and with explicit N3C consent.\nSince the registration process is non-trivial for those not affiliated with a research institution, citizen scientists hoping to work with Level 1 Synthetic data should reach out to one of the N3C support venues to get started (see Chapter 11).\nPPRL data can be accessed alongside only Level 3 (LDS) data, and as such has the same access availability. As we’ll discuss below, there are special procedures for gaining access to PPRL data as part of a Level 3 access request."
  },
  {
    "objectID": "chapters/access.html#sec-access-workspaces",
    "href": "chapters/access.html#sec-access-workspaces",
    "title": "6  Getting & Managing Data Access",
    "section": "6.3 Workspaces, Permissions, and the Data Catalog",
    "text": "6.3 Workspaces, Permissions, and the Data Catalog\nThe N3C Data Enclave uses a fine-grained permissions model to manage researcher access to protected data. While researchers are not able to modify these permissions themselves, understanding them will help in navigating the DUR process and subsequent work.\nData management in the Enclave is centered around “project workspaces” which act like folders–workspaces are indicated by a small filing-drawer icon and are listed under the “Projects & files” link in the left navigation menu.\n\n\n\nFigure 6.4: Project workspace browser. Note that the “Request access” buttons are not active and clicking one will suggest visiting the DUR dashboards described below.\n\n\nWorkspaces are used for multiple purposes. Some store the Level 1/2/3 data; the “LDS Release” workspace for example stores the tables for the Level 3 LDS data. (The Data Catalog provides a more efficient way to access these tables however, more on that below.) Some are used to store “external” datasets such as publicly-available US Census data (see Chapter 7). The “N3C Training Area” workspace can be accessed by anyone as a place to practice on notional data (see Chapter 10). Most, however, house research projects, and these are indicated with an RP-XXXXXX prefix.\nWork in one project workspace can access only data or files from another project workspace if a “reference” has been added from the former to the latter. Researchers do not have permission to add such references. Thus, access from a research project workspace to protected datasets is possible only if the appropriate references have been added by administrators, which is done after a corresponding Data Use Request has been approved. Said another way, rather than providing researchers access to data, researchers are provided access to project workspaces, and these are in turn provided access to data.\nThis reference-based permission scheme supports a number of useful features for N3C. Naturally, restricting workspace access to a subset of data affords the possibility of different levels of data access with correspondingly different access requirements. A single researcher may be involved with multiple research projects of different levels but cannot share data or files across them, thus it is impossible for a researcher with access to Level 3 data to share it with their colleagues in another project with Level 2 data."
  },
  {
    "objectID": "chapters/access.html#sec-access-dur",
    "href": "chapters/access.html#sec-access-dur",
    "title": "6  Getting & Managing Data Access",
    "section": "6.4 The DUR - Data Use Request",
    "text": "6.4 The DUR - Data Use Request\n\n6.4.1 Project Roles and DUR Types\nIn the simplest view, each research project is associated with a single ‘DUR’ listing the project title and abstract, and governing the level of data accessible to the project workspace (and thus also to all researchers with access to that workspace). DURs are reviewed by the N3C Data Access Committee (DAC), and when approved administrators configure the workspace and researcher access to it.\nIn practice the situation is a bit more complicated. The researcher who submits the initial DUR defining the title, abstract, and data access level is known as the project lead. Other researchers who sign on to the project submit “collaborator DURs.” A collaborator DUR is a copy of the original DUR submitted by the lead, but the title, abstract, and data access level are fixed. Both leads and collaborators must enter other requirements depending on the data access level (e.g., date of most recent Human Subjects Research Protection training or local IRB approval), and both lead and collaborator DURs are reviewed by the DAC before access is granted. If a lead’s DUR is rejected by the DAC, they will have the opportunity to appeal by updating the information provided.\nThe lead has certain abilities and responsibilities, most importantly configuring whether the project is open so that other researchers may request to join as collaborators, and if so periodically reviewing those requests. These cannot be delegated, so the lead should be someone with that decision-making authority. Aside from these aspects however, leads and collaborators all have the same permissions in the project workspace and can review and share work.\nSince DURs expire after one year, there is also a “renewal” DUR type, which both leads and collaborators are prompted to fill in the weeks prior to their DUR expiry via email. Renewal DURs don’t allow editing the title, abstract, or data level, and so are usually expedited by the DAC.\nLastly, there are “revision” DURs, where the lead requests to change some major feature of the DUR (most commonly to upgrade the data access level) for an existing project workspace. Once approved by the DAC, administrators reach out to the lead to plan the revision, because collaborators will also need to submit new collaborator DURs prior to the change or they will lose access to the workspace.\nNote that while we’ve described these different types of DURs, they all use the same DUR form, with the differences boiling down to what fields can be changed by the researcher filling it out.\nTheoretically a single researcher may have multiple outstanding DURs for the same project. For example, the lead for a project nearing its expiry may submit a renewal DUR, and while that renewal is pending DAC approval the lead may also decide to submit a revision DUR to access a higher level of data. Most likely the renewal will be approved first, followed by the revision. If it so happens that the revision is approved before the renewal, the renewal request will then be moot as revision also resets the expiry date. Such cases may require intervention by administrative support, so it is wise to be judicious about DUR submissions to expedite the process."
  },
  {
    "objectID": "chapters/access.html#sec-access-dashboards",
    "href": "chapters/access.html#sec-access-dashboards",
    "title": "6  Getting & Managing Data Access",
    "section": "6.5 DUR Dashboards",
    "text": "6.5 DUR Dashboards\nThere are three primary dashboards used to create and manage DURs; all three are linked from the N3C Data Enclave homepage via buttons titled “Data Use Request (DUR)”, “My Projects (DURs)”, and “Explore Projects (DURs).”\n\n\n\nFigure 6.5: Enclave homepage. Highlighted are the three DUR-related dashboards.\n\n\nAlthough there is some functionality overlap, the primary uses for these three dashboards are:\n\nData Use Request (DUR) is used to submit a new Data Use Request for a new project. The submitter provides the project abstract and title and selects the data level needed, and will become the lead for the project.\nMy Projects (DURs) shows the status of your submitted requests and projects. This interface also provides some project management functionality for leads, such as linking a project to an N3C domain team, reviewing requests from potential collaborators to join the project, appealing rejected DUR submissions, and accessing the download request dashboard. \nExplore Projects (DURs) lists all N3C research projects, and for those that allow it, provides a link to request to join a project by submitting a collaborator DUR."
  },
  {
    "objectID": "chapters/access.html#sec-access-request",
    "href": "chapters/access.html#sec-access-request",
    "title": "6  Getting & Managing Data Access",
    "section": "6.6 Data Use Request (DUR): Initiate a New Research Project",
    "text": "6.6 Data Use Request (DUR): Initiate a New Research Project\nThis first dashboard is really just the DUR form itself, and is used to request the creation of, and access to, a brand-new research project with access to one of the levels of protected N3C data. The form has a number of sections, and some of these are dynamic, depending on choices to earlier questions. The left side of the DUR form provides quick links to other dashboards or actions; “Create a New Project” simply refreshes the page (since we are already creating a new project DUR), “Request to become a collaborator on an existing project” opens the Explore Projects dashboard (discussed below), and Public Health Proposal opens the application form for the N3C PHASTR initiative (which utilizes a specialized variant of the DUR process discussed here).\n\n\n\nFigure 6.6: New Data Use Request (DUR) form.\n\n\nMoving to the main content of the DUR form, the first three questions prompt for a Title, Abstract, and Research Project Rationale. (The green Submit button is always visible, and will be clickable when all fields for the DUR are properly completed.) Title and Abstract are self-descriptive, but know that these entries will be listed for others to see, both inside the N3C Data Enclave via the Explore Projects dashboard, and outside at https://covid.cd2h.org/dashboard/.\nThe Research Project Rationale deserves special attention. This information will be visible only to you (the submitter) and the Data Access Committee (DAC) who will approve or deny the request. The project rationale should describe for the DAC why data access is being requested in light of the goals of the research project. The rationale should include a justification for the level of data being requested, especially if requesting the Level 3 Limited Data Set (LDS). If your project doesn’t require the accurate dates or full patient zip codes provided by Level 3 data, it will be rejected, or approved for the Level 2 data even though Level 3 was requested. Similarly, if requesting access to any PPRL (see Section 6.2) datasets the rationale should include why. (You will also need to have the PPRL data mentioned in your IRB letter of determination - see below.)\n\n\n\nFigure 6.7: DUR metadata fields.\n\n\nThe next questions pertain to potential collaborations. The “Please associate your project with an existing N3C Domain Team” prompts you to select a domain team that best fits your project. Domain Teams are N3C working groups encompassing multiple research projects and are used to facilitate collaboration. Linking your project to a domain team this way in the Enclave allows others to navigate domain teams and browse linked projects dynamically. It is possible to modify this linkage later, so if you aren’t sure feel free to check the “need help choosing” check box.\n\n\n\nFigure 6.8: DUR Domain Teams and collaborator fields.\n\n\nThe next question, “Allow other researchers to join this project,” configures the project so that others can request to join via the Explore Projects dashboard. If this is enabled, other researchers will be able to fill out a collaborator DUR (a copy of this DUR but with the title, abstract, and data level fixed). As the lead submitting this primary DUR, you will have the opportunity to approve or deny such requests (via the My Projects dashboard); if you approve, their request will be forwarded to the DAC for final approval of the workspace and data. This option can be changed later.\nThe Collaborators section allows you to send invites to potential collaborators to join the project. Added collaborators will receive an email notification (with instructions to register for N3C in case they don’t already have access), and will see the invite in their My Projects dashboard with a link to fill out a collaboration DUR (much like those who request to join described above). These invites are sent regardless of the earlier choice to Allow/Do not allow others to join the project.\nNext is the choice of data level to request. Levels 2 and 3 require additional information, so we’ll start with the Level 1 data and return to see the differences for Levels 2 and 3.\n\n\n\nFigure 6.9: DUR data level request.\n\n\n\n6.6.1 Level 1 DUR Requirements\nFor Level 1 access, the first requirements are that you have read and attested to the Data Use Agreement, and that you have completed the required NIH IT Security training course within the past year. Both of these are also required as part of onboarding, so we won’t cover them here (see Chapter 5).\n\n\n\nFigure 6.10: DUR attestations.\n\n\nFinally, you will need to attest to having read the N3C Code of Conduct (with the text provided above), that you have read and understood the N3C download policy (a link is provided via the more information icon), and that if you choose to use any additional data sources in connection with N3C protected data that you are aware of your institutions’ policies around doing so. (Some institutions restrict how and when EHR data can be linked to other datasets, including publicly available data ingested for use by N3C researchers.) \n\n\n\nFigure 6.11: DUR attestations and acknowledgments.\n\n\nWith all of this information provided, you will be able to click Submit and send the request to the Data Access Committee for review.\n\n\n6.6.2 Level 2 DUR Requirements\nRequesting access to Level 2, De-Identified data requires additional information. First, when selecting the Level 2 request option, you will be asked to confirm that you have taken Human Subjects Research Protection training within the last three years and provide the date of completion. Most research institutions provide specific training courses to satisfy this requirement, for example via the CITI Program. The specific courses required vary from institution to institution, so check with your research office or equivalent authority for guidance. If your institution does not offer or recommend specific courses, you can utilize the free course provided by Health and Human Services.\n\n\n\nFigure 6.12: DUR Human Subjects Research Protection (HSRP) training attestation. HSRP training must have been completed within the last three years.\n\n\nIn addition to human subjects research protection training, you will need to consider whether your institution requires a letter of determination from your local IRB when using De-Identified data. If so, you’ll be prompted to upload the letter as a PDF before you can submit the DUR.\n\n\n\nFigure 6.13: DUR IRB letter upload. For a Level 2 DUR, this is required only if your local institution requires IRB approval for de-identified data access.\n\n\n\n\n6.6.3 Level 3 DUR Requirements\nThe Level 3, Limited Data Set (LDS) offers the most complete view of N3C data. Unlike Level 2 De-Identified data, dates are not shifted (for most contributing data partners) and 5-digit patient zip codes are available (for most zip codes). See Chapter 3 for more details\nLevel 3 DURs require the same attestations and human subjects research protection training as Level 2 DURs. A Level 3 DUR also allows researchers to request access to one or more supplemental PPRL datasets providing additional information about patients. These provide additional mortality information, viral variant records, and claims data from the Centers for Medicare and Medicaid Services, but are available only for subsets of N3C patients (see Chapter 7 for details about PPRL).\n\n\n\nFigure 6.14: DUR Privacy-Preserving Record Linkage (PPRL) options for Level 3 requests.\n\n\nA Level 3 DUR additionally requires a letter of determination from the submitter’s local IRB:\n\n\n\nFigure 6.15: The lead (submitting) investigator must supply an IRB letter of determination from their local institution. Collaborators from the same institution as the lead aren’t required to, but collaborators from other institutions must also obtain and submit a determination letter from their own local IRB.\n\n\nNote that when requesting access to PPRL datasets, both the Research Project Rationale (reviewed by the DAC) and the IRB letter of determination should address the need for the PPRL datasets requested. Failure to adequately justify the use of PPRL datasets may result in denial of the DUR, or approval with lack of PPRL dataset access.\n\n\n6.6.4 Project Workspace Creation and Data Access\nThe Data Access Committee reviews all DURs. After a new project DUR is approved, a workspace is created inside the Enclave with access to the approved datasets (see Workspaces, Permissions, and the Data Catalog above), and the submitting lead is given access to this project workspace. An email is also sent to the submitting lead with links to useful learning resources.\nThe full review and access process varies depending on the level of data requested and workload of the DAC, but most DURs are evaluated within 2 weeks. The status of your DUR will be shown in the My Projects dashboard (below). If a submitted DUR has not received any update for longer than two weeks, you should submit a request for a follow-up to the Enclave-external ticket support system."
  },
  {
    "objectID": "chapters/access.html#sec-access-explore",
    "href": "chapters/access.html#sec-access-explore",
    "title": "6  Getting & Managing Data Access",
    "section": "6.7 Explore Projects (DURs): Browse and Join Projects",
    "text": "6.7 Explore Projects (DURs): Browse and Join Projects\nAlthough all N3C research projects are listed in the public dashboard, the Explore Projects dashboard within the N3C Data Enclave allows researchers to request to join projects that allow for it as collaborators.\nFirst, let’s explore the dashboard. The initial list of projects shown is “Projects to join” - these have been configured by the project lead to allow any N3C researcher to request access to the project for collaboration. Below this list are two expandable sections, “Projects to Explore” listing projects that do not allow requests to join, and “Operational Projects (N3C Technical Development)” listing DURs used by the data ingestion and harmonization teams and others.\nThe “Search Projects” field is helpful in finding one of the many N3C projects and supports searching in any of the shown text columns. The “Projects by N3C Domain Team” button shows the project list organized by domain team, which can be useful in identifying potential overlapping research. Finally, “Closed Projects” lists non-active projects, for example those whose 1-year expiry has passed without renewal.\n\n\n\nFigure 6.16: The Explore Projects dashboard.\n\n\nTo request to join a project, simply click its Request to Join link from the list. This opens up a collaborator DUR: a copy of the DUR submitted by the project lead, but the Title, Abstract, selected data level, and domain-team linkage are not changeable. You will need to complete the other requirements for the data level however (see above), including submitting a letter of determination from your local IRB if needed. If the DUR indicates that PPRL data are requested, your IRB letter should also reference these datasets. IRB determination letters are required for all Level 3 DURs, or if the DUR is Level 2 and your institution requires IRB review. (Note that the DUR form will not ask for an IRB letter if you are registered with N3C as being from the same institution as the project lead; in this case it is assumed that the lead’s already-submitted letter covers you as well.)\n\n\n\nFigure 6.17: Requesting to join a project from the Explore Projects dashboard opens a collaborator DUR for the project, where metadata fields such as title and abstract are fixed.\n\n\nCollaborator DURs must first be approved by the project lead (via their My Projects dashboard, see below). You may thus want to reach out to the lead prior to requesting access to their project so they can expect the request. Once approved by the lead the DUR is sent to the DAC for final approval. After DAC approval you will be given access to the project workspace (see Workspaces, Permissions, and the Data Catalog above). Collaborator DURs are generally faster to approve than new project DURs, but as with new project DURs if no progress is made in two weeks you should submit a request for follow-up to the Enclave-external ticket support system.\nAs with new project DURs, the status of your submitted collaborator DURs can be found in the My Projects dashboard."
  },
  {
    "objectID": "chapters/access.html#sec-access-my",
    "href": "chapters/access.html#sec-access-my",
    "title": "6  Getting & Managing Data Access",
    "section": "6.8 My Projects (DURs): DUR Invitations, Status, and Management for Leads",
    "text": "6.8 My Projects (DURs): DUR Invitations, Status, and Management for Leads\nThe My Projects dashboard, linked from the N3C Data Enclave homepage, shows the status of your Data Use Requests and allows leads to configure some parameters of their projects.\n\n\n\nFigure 6.18: The My Projects dashboard.\n\n\nAlong the left are the various projects you’ve requested access to, either as a lead who’s submitted a new project DUR, or as a collaborator who’s requested to join a project. Notice along the bottom the “Invitations to Submit a Project Collaborator DUR” - this section may be expanded, and if any leads have added you to their DUR as invited collaborators, you will see a link to fill out a collaborator DUR here. (This is the same process as requesting to join a project as described above.) A quick link to the Download Requests Dashboard is also provided near the top, which we won’t discuss further here. Nearby are two buttons for project leads, “Review Collaborator Requests for My Projects,” and “Manage My Projects,” both discussed below.\nHaving selected a project from the list, the interface shows a variety of information about the project, including the data access level, project lead, and a direct link to the project workspace. If you are a collaborator on the project rather than the lead, you will also see a button to voluntarily withdraw from the project, which will remove your access to the project workspace but leave intact your work within that workspace for your collaborators.\nThe table below shows the DURs you have submitted pertaining to the project. Usually this will list a single DUR, but as noted at the beginning of this chapter in theory you may have multiple outstanding DURs pertaining to the same project. The “DUR Review Status” column shows the state of the DUR, and may contain one of several values indicating its progress: “Pending Lead Investigator Approval” if the DUR still needs to be approved by the project lead (“Denied by Lead Investigator” if they deny the request), “Pending DAC Review” for DURs that have been approved by the lead and are awaiting DAC review (“Denied” for those DURs that have been denied by the DAC), “Reviewed and Pending Workspace Access” for DURs that are fully approved but awaiting permissions to be applied for project workspace access, and “Complete” for fully approved DURs with enabled data access.\nIf we scroll further to the right we see other important columns, particularly the Action and Renewal columns.\n\n\n\nFigure 6.19: Scrolling rightward when reviewing a project in the My Projects dashboard reveals more information and options.\n\n\nN3C DURs are valid for one year–continued access to the project workspace requires submitting a renewal DUR before the expiration date listed. The Renew DUR link in the Renewal column opens up a renewal DUR: a copy of the DUR with almost all information uneditable, but requiring re-attestation of information like the Data Use Agreement, Code of Conduct, and human subjects research protection training. Renewal DURs are processed more quickly than other types, but if you miss the renewal deadline and you are a collaborator (not the lead) you will need to resubmit a collaborator DUR for a more thorough review. Leads who miss the renewal deadline will have their projects closed and will need to submit an Enclave-external support ticket for help re-opening it.\nIf you are the lead for the project you will have the option to “Appeal” the DUR under the Action column to appeal a DUR rejected by the DAC. The appeal DUR is a copy of the submitted DUR, but all fields are editable, including Title, Abstract, data access level, and research project rationale. When appealing a rejected DUR, you will likely want to update at least the research project rationale. Finally, for active projects there will be an “Update DUR” option to request a significant change to an approved DUR requiring DAC review, such as updating a project to a higher level of data access.\n\n6.8.1 Project Management for Leads\nThe My Projects dashboard provides some additional functionality for project leads not available to collaborators in addition to the Appeal link. First is the dashboard accessed via the “Review Collaborator Requests for My Projects” button. The projects for which you are a lead are listed along the left, and the requests to access that project are listed along the right. You can choose to approve or deny each request by selecting it and clicking the large blue “Approve Selected Collaborator Request” button. Collaborator access cannot be revoked this way, however.\n\n\n\nFigure 6.20: Reviewing collaborator requests to join my projects.\n\n\nBack in the My Projects dashboard, we can select the “Manage My Projects” button to see more project configuration options. The main configuration options are “Configure N3C Domain Teams” to modify the listed linkage between the project and domain teams, “Review Collaborator Requests for this Project” which opens the collaborator approval dashboard above, and “Allow or Disallow Collaborator Requests.” This last option enables or disables the “Request to Join” link for the project in the Explore Projects dashboard.\n\n\n\nFigure 6.21: Adjusting other project settings.\n\n\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page."
  },
  {
    "objectID": "chapters/access.html#footnotes",
    "href": "chapters/access.html#footnotes",
    "title": "6  Getting & Managing Data Access",
    "section": "",
    "text": "There are some exceptions where five digit zip codes are not visible in Level 3 data: zip codes represented by fewer than 20,000 patients are removed altogether; sometimes only the first three digits of the zip code are displayed (such as zips with a predominantly American Indian/Alaskan Native population and zip codes from participating institutions who send only the first three digits).↩︎\nIn actuality, a few data partners also perform small amounts of date shifting–randomly shifting all dates on a per-patient basis to further protect patient privacy–prior to sending their data to N3C. The manifest information (described below) indicates which data partners do so and the range of potential shift so researchers can remove data from these partners if they need highly-accurate date information.↩︎\nLike Level 3 data, birth dates are available only at the year and month level, and these are similarly shifted along with other patient-associated dates. In Level 2 data birth date information is removed entirely for patients who are more than 90 years old, and a separate is_age_90_or_older column identifies these individuals as a group.↩︎\nThe Synthetic data discussed here should not be confused with the notional (fake) datasets described in Chapter 10. These happen to have similar sounding names: SynPuf and Synthea.↩︎"
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-intro",
    "href": "chapters/understanding.html#sec-understanding-intro",
    "title": "7  Understanding the Data",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe purpose of this chapter is to introduce the core building blocks for performing analyses in the N3C Data Enclave (see The N3C Data Enclave and Data Access in the Introduction).\nWe start with the Observational Medical Outcomes Partnership (OMOP) vocabulary, since all data are represented in the OMOP common data model (CDM) format. This chapter will describe the difference between the OMOP concept id unique identifier and the source value contributed by a site, the notion of hierarchy across concepts, and the OHDSI tools outside the Enclave that can help. Because we are working in a secure NCATS Enclave environment, we cannot rely on OHDSI tools outside the Enclave, so knowing the basic applications available within the Enclave and how these data are organized is crucial for the proper conduct of research.\nWe next present the notion and management of concept sets. These are concept ids that are taken as synonymous, for a given research purpose, and have their own set of tools.\nWe turn next to constructs—derived variables and facts—derived from concept sets and the raw data, including N3C OMOP additions that build on the OMOP data.\nWe round out this discussion with an introduction to external datasets that have been brought into the Enclave and the process to bring further datasets in.\nLater chapters will give more details on how to use the resources and tools described here."
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-basics",
    "href": "chapters/understanding.html#sec-understanding-basics",
    "title": "7  Understanding the Data",
    "section": "7.2 OHDSI Basics",
    "text": "7.2 OHDSI Basics\nThis Observational Health Data Sciences and Informatics (OHDSI) basics section introduces the OHDSI community and provides an introduction to OMOP vocabularies, and vocabulary search tools such as ATHENA and ATLAS. For further OHDSI collaborative knowledge and exploration, refer to The Book of OHDSI which serves as a central knowledge repository (OHDSI, 2019). The Book of OHDSI is a living document, community-maintained through open-source development tools, and evolves continuously. The online version, available for free, always represents the latest version.\n\n7.2.1 OHDSI Community\nOHDSI is an international, interdisciplinary research collaborative that promotes open-source research. Its purpose is to create open-source solutions that bring out the value of observational health data through large-scale analytics. OHDSI has established an international network of researchers and observational health databases with a central coordinating center housed at Columbia University. More information about OHDSI can be found at https://www.OHDSI.org.\n\n\n7.2.2 OMOP Vocabulary\nThe Book of OHDSI states:\n\nThe OMOP Standardized Vocabularies, often referred to simply as “the Vocabulary”, are a foundational part of the OHDSI research network, and an integral part of the CDM. They allow standardization of methods, definitions and results by defining the content of the data, paving the way for true remote (behind the firewall) network research and analytics. Usually, finding and interpreting the content of observational healthcare data, whether it is structured data using coding schemes or laid down in free text, is passed all the way through to the researcher, who is faced with a myriad of different ways to describe clinical events. OHDSI requires harmonization not only to a standardized format, but also to a rigorous standard content.\n\nN3C relies heavily on the OMOP common data model which is part of the heart of the N3C harmonization process. OMOP, along with other data partners from ACT, TriNetX, and PCORI, ingest their data feeds into a central OMOP N3C repository that is then available within the N3C Data Enclave for researchers.\nThe OMOP CDM is an open-source, community standard for observational healthcare data and consists of clinical tables describing conditions and events from patient records and vocabulary tables describing clinical concepts. The central table in the OMOP vocabulary system is the table, concept.\n\n\n\nFigure 7.1: Sample rows from the 6.9-million row table of concepts imported from OMOP.\n\n\n\n7.2.2.1 Vocabulary Concepts\nThe meaning of every data element in the OMOP vocabulary, and many of the values of the data themselves, are represented by codes in the source data that get mapped to OMOP concept ids. Suppose a physician diagnoses a patient with diabetes and types “I10” into the EHR (which uses the ICD-10 vocabulary). The “I10” value is stored in OMOP as the source_value, and is then translated via the OMOP Vocabulary to the standard concept id, “320128”.1 N3C analyses use standard concept ids, as opposed to specific codes that aren’t necessarily consistent across hospitals.\nThe Book of OHDSI has a detailed chapter on Standardized Vocabularies. The OHDSI vocabularies are in a standardized CDM structure that houses existing vocabularies used in the public domain. This CDM compiles standards from disparate public and private sources as well as some OMOP-grown concepts.\nConcepts are related to each other. For instance, hypertrophic cardiomyopathy (concept id 4124693) is a type of cardiomyopathy (concept id 321319). The concept_relationship table stores these relationships. While analysts are welcome to use this table in their queries, there are other approaches to doing the same. (See concept sets.)\n\n\n\nFigure 7.2: A sample from the 54.4 million-row table imported from OMOP, focused on the cardiomyopathy concept id, 321319.\n\n\nEvery relationship type connecting two OMOP concepts has a converse. In the concept_relationship table, this means a parent-child relationship will be represented (redundantly) by two records. Hypertrophic cardiomyopathy (concept id 4124693) is a type of cardiomyopathy (concept id 321319):\n\n\n\nFigure 7.3: The converse representation of Figure 7.2 row 47.\n\n\nThe hierarchical relationship between the two concepts is represented once (above) with a “Subsumes” record and again (below) with an “Is a” record. concept_relationship contains only direct relationships between pairs of concepts. For hierarchical relationships, more distant relationships are found in the concept_ancestor table.\nThe concept_relationship table contains many other relationship types beyond hierarchical relationships: “Mapped from”, “Maps to”, “Has brand name of”, “Consists of”, etc.\nATHENA is an OHDSI tool for exploring OMOP vocabularies and even downloading them; its primary use in N3C is for looking up individual terms. More details on using ATHENA to search terms are documented on OHDSI’s publicly-viewable GitHub repository. ATLAS is an OHDSI tool to explore concepts and their relationships. See the section below on concept sets that describes further uses of the ATLAS tool.\nThe OMOP vocabulary is updated regularly to keep up with the continual evolution of sources. Its vocabulary maintenance and improvement is an ongoing activity that requires community participation and support. Releases and release notes are published via Git Releases on the OMOP Vocabulary GitHub repository. The most recent specification documentation can be found at OMOP Common Data Model. New versions of the OMOP vocabulary are downloaded monthly by the Enclave managers.\nThe OMOP CDM assembles these relationships in the Standardized Vocabulary section of the CDM. These tables are available to N3C researchers, along with the Standardized clinical data tables that store the patients’ data (expressed using the concept ids). Data are available as well in the Standardized health system, Payer_plan_period (not cost), and Standardized derived elements tables.\n\n\n\nFigure 7.4: OMOP Data model.\n\n\nTo learn more about OMOP and get acquainted with the CDM used in the Enclave and its functionality, see OMOP 101: A Crash Course in OMOP Standard Vocabulary  and Chapter 11 in this guide.\nThe figure below shows all OMOP tables within the Enclave.\n\n\n\nFigure 7.5: Note that this folder contains tables (e.g., conditions-to-microvisits) that are unique to N3C. The SafeHarbor release has a comparable folder.\n\n\n\n\n7.2.2.2 Domains\nTables may contain data from more than one “domain”. There are currently 36 Domains in OMOP; the 5 primary clinical Domains are Drug, Condition, Observation, Procedure, and Measurement, plus others. They classify concepts in the concept table.\n\n\n\nFigure 7.6: Listed in decreasing order by the number of concepts for which we have data in N3C.\n\n\n\n\n7.2.2.3 Vocabulary Updates\nAs stated above, source vocabularies change. For instance, at the start of the pandemic, there were no ICD-10 codes for “COVID-19 infection”, and therefore, there were no OMOP vocabulary codes for it and, therefore, no N3C codes. When we need a code, but there is none in the OMOP vocabulary, we create a local code. When the OMOP vocabulary team releases a newer, relevant code to replace the local code, the Enclave attempts to contact everyone who has employed the local code, notifying them of the change.\nMore subtle issues are when the source vocabulary managers change or decommission existing codes or add new codes (that do not have a local code equivalent), and the OMOP managers incorporate those changes in the vocabularies brought into the Enclave every month. Another subtlety is when the relationships between codes are changed. The Enclave attempts to contact those who have used codes related to the changed code. Generally, the analyst has used those codes in concept sets, to which topic we now turn."
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-sets",
    "href": "chapters/understanding.html#sec-understanding-sets",
    "title": "7  Understanding the Data",
    "section": "7.3 Concept Sets",
    "text": "7.3 Concept Sets\nAny study or analysis performed in the N3C Data Enclave will start with identifying meaningful clinical conditions and events in patient data. We attempt to determine the presence or absence of clinical phenomena in patient history through the presence or absence of certain concept codes in patient records (Gold et al., 2018, 2021). For any given phenomenon used in an analysis, however, a single concept code will seldom be sufficient, and concept sets are used.\nThere are many reasons to specify a concept set:\n\nVariables in your study are implemented using concept sets.\nHaving dozens or hundreds of concept ids in a single concept set makes analysis much easier.\nIt makes the analysis easier to peer review.\nIt makes your research coherent with the research of others who use the same concept set(s).\nChunking concept-ids together into concept sets with different names and intentions makes your own (research) intentions clearer.\n\nIf you are publishing your results, concept sets are considered a publicly viewable part of your work. Such public viewing is important.\n\nIt is required by the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) guidelines for publishing observational research.\nOpen Science expects reproducibility, and reproducibility in EHR-based research requires knowing how variables (concept sets) were defined.\nIt enables other N3C researchers to use your concept sets, which is what makes research coherent across N3C projects.\n\nConcept sets that are “finalized” within N3C will have information about them posted to our publicly-viewable GitHub repository. Such posting is part of the process known as making research FAIR (findable, accessible, interoperable, reusable). This repository is functionally our Shared Variable Library. Other variable definitions beyond concept sets are posted there (formulas, harmonized values, cohort definitions).\nThe following concept set content will be made public in Zenodo and/or GitHub:\n\nConcept set name, version number, date of finalization\nIntention\nLimitations (of the concept-ids within the concept set)\nIssues (with respect to data in the Enclave, so analysts are forewarned)\nProvenance (how the concept set was assembled, which communicates how trustworthy the concept set is)\nAttributions (who did what)\nThe list of codes (in JavaScript Object Notation (JSON) format, which can be imported directly into Observational Health Data Sciences & Informatics (OHDSI)’s Atlas tool)\nZenodo persistent document object identifier (DOI)\n\n\n7.3.1 What is a concept set?\nThe Book of OHDSI explains concept sets as:\n\nA concept set is an expression representing a list of concepts that can be used as a reusable component in various analyses. It can be thought of as a standardized, computer-executable equivalent of the code lists often used in observational studies. A concept set expression consists of a list of concepts with the following attributes:\n\n\nExclude: Exclude this concept (and any of its descendants if selected) from the concept set.\nDescendants: Consider not only this concept, but also all of its descendants.\nMapped: Allow to search for non-standard concepts.\n\nWhile ATLAS concept sets may contain standard and non-standard concepts, Enclave concept sets may include “standard” (SNOMED-based) concepts only. Thus, the “Mapped” attribute is rarely helpful, as it applies only if using source data.\nFor example, a concept set expression could contain two concepts as depicted in The Book of OHDSI’s Cohort chapter. Here we include concept 4329847 (“Myocardial infarction”) and all of its descendants, but exclude concept 314666 (“Old myocardial infarction”) and all of its descendants.\n\n\nTable 7.1: Representation, from Book of OHDSI, of the Myocardial Infarction subtree of standard concepts\n\n\n\n\n\n\n\n\n\nConcept Id\nConcept Name\nExcluded\nDescendants\nMapped\n\n\n\n\n4329847\nMyocardial infarction\nNO\nYES\nNO\n\n\n314666\nOld myocardial infarction\nYES\nYES\nNO\n\n\n\n\nAs shown in Table 7.1, this will include “Myocardial infarction” and all of its descendants except “Old myocardial infarction” and its descendants. In total, this concept set expression implies nearly a hundred Standard Concepts. These Standard Concepts in turn reflect hundreds of source codes (e.g. ICD-9 and ICD-10 codes) that may appear in the various databases.\n\n\n\nFigure 7.7: Representation, from Book of OHDSI, of the Myocardial Infarction subtree of standard concepts.\n\n\nATLAS is used by the OHDSI community to construct concept sets. Best practice is for each research institution to have its own CDM instance. The Enclave, for technical reasons, cannot implement ATLAS. In the future, N3C hopes to provide a public tool but it is unclear when this release will occur. Users without their own local ATLAS may explore its utility using a publicly available one on OHDSI.org. Do beware all public tools are intended to be a demonstration site to showcase the power of the open-source tool stack. They are not intended for active research projects. These deployments are not running on a real OMOP CDM. They use synthetic data like the CMS Synthetic Public Use Files (SYNPUF) in the OMOP CDM format. The vocabulary schema will reflect whatever version of the vocabulary was in place when that OMOP CDM ETL was run. It will not reflect the most recent version of the OMOP vocabulary, which may be problematic for research projects that use newly approved vaccines or medicines. You should not make the habit of saving content within the public instance. It can be reset and wiped at any point in time. If you enjoy using ATLAS, consider spinning up your own instance via Broadsea, OHDSI in a box, or other OHDSI Community resources to jump-start users with their own ATLAS deployment. It is very easy to do and runs effectively, even on synthetic data. Resources are available via the OHDSI ATLAS workgroup. While it is a handy complement to the Enclave, do focus on the N3C products: The N3C concept set library and the concept set editor. The Enclave builds its own OMOP vocabulary releases by downloading the vocabulary tables from ATHENA.\nBecause of these limitations, also do beware that ATLAS “record counts” (the number of patients with data expressing the given concept id) are not linked to N3C and do not represent the counts or distribution of concept ids within the Enclave.\n\n\n7.3.2 Concept Set Metadata\nThe work on concept sets does not take place in a vacuum. It is assumed that you are part of a project team, which has the following Subject Matter Expertise (SME): Research/Clinical, Vocabulary, Informatics, and Statistical/Analytic. Each of these SMEs will be called upon in your work.\nIntention, Limitations, and Provenance comprise concept set metadata.\n\n7.3.2.1 Intention\nIntention communicates in sentences more than the name can. For instance, is the concept set intended to be “broad”, and sensitive, to capture as many cases as possible, leaving downstream analysts to winnow the set of patients down? Or is it “narrow”, and specific? Is it meant to be definitional (these codes “mean” diabetes) or indicative (the codes tell me that you have diabetes, e.g., “retinopathy due to diabetes mellitus”; the codes suggest chronic lung disease, e.g., “infertility due to cystic fibrosis”). “Intention” can also indicate whether the concept set has clinical manifestations or not (e.g., “Sickle Cell” indicates a genetic condition, but unclear if it’s homozygous or heterozygous, the former being clinical and the latter often not). It is true that a later analyst will look through the list of codes, but having an explicit intention helps that analyst screen potential concept sets.\n\n\n7.3.2.2 Limitations\nLimitations communicate edge cases and caveats to the analyst. “Issues” communicates performance with the Enclave data. This performance could include the number of codes contributing the majority of the data (e.g., from Term Usage) or the distribution of values across sites, in the case of lab tests.\n\n\n7.3.2.3 Provenance\nProvenance communicates how the concept set was assembled. It should include any authorities consulted (the literature, the Value Set Authority Center, the domain team members, etc.) and a sense of what modifications, if any, were made from that authoritative beginning. (Of note, “ATLAS” is not an authoritative beginning, unless there is annotation on the ATLAS site of the authoritative source for that specific concept set.)\n\n\n\n7.3.3 N3C Concept Set Library\nThe concept sets already created in the Enclave can be browsed in the concept set browser , illustrated in Figure 7.8. This is the first step for deciding whether a new concept is needed or if an existing concept set can be chosen for inclusion into an analysis as a building block for an analytic variable or cohort definition. We recommend that you use or build from existing concept sets, especially the N3C Recommended concepts sets, if they serve your research question. A workflow for performing this series of tasks is shown in Figure 7.8.\n\n\n\nFigure 7.8: An illustration of the Myocardial Infarction concept set and container and versions in the concept set browser.\n\n\nWhen created from scratch, a container is created, so multiple versions of the concept sets can be supported. New versions are created as the team gets better facility with the tool, as the team understands better from domain experts subtleties in the intention of the concept set, or as changes are made to the vocabularies, by OHDSI or the source vocabulary stewards. Containers are indicated by blue icons; versions, by brown icons, as illustrated in Figure 7.8.\n\n\n\nFigure 7.9: Workflow for finding, extending, and creating concept sets. *New container name should be similar to source container, but different enough to communicate altered intention. MS=Manuscript\n\n\nThe browse screen allows you to filter to concept sets relevant to you. Once you have identified a concept set that you might use, drilling into it provides data about that concept set: versions, metadata, concept expressions, concepts. In reviewing the concept set (generally by drilling to the most recent version), it may be worth exploring similar concept sets using the concept set Overlap tab. At this point, you may be satisfied that an existing concept set serves your needs. If not, you can create a new one — either an entirely new concept set, or a new version of an existing concept set, modifying it to meet your requirements. If you are creating a new version within an existing container, please contact the concept set owner to ensure that your update is in the right spirit. If not, clone the existing version into a new container, naming the container with a name that connotes the difference between the existing and the new concept sets.\nIf you use a concept set in a published manuscript, make it publicly available. See the next section, Published Concept Sets.\nAdditional documentation on the concept set browser and editor can be found on the Enclave. A tutorial  is available that explores these tools further.\n\n7.3.3.1 Published Concept Sets\n“Publication” includes 2 ways of making a concept set available to other researchers. Provisionally Approved and N3C Recommended are automatically submitted to Zenodo, to generate a permanent DOI URL that can be used in publication or in sharing concept set contents with analysts outside the Enclave. Any study manuscript for publication should ensure that any concept set used is either Provisionally Approved or N3C Recommended. The Zenodo URL should appear in the resulting publication.\nThe N3C Data Liaisons and Logic Liaisons, in partnership with the Domain Teams, have selected common medical features often required for broad use in research that support the analytics pipelines. The original N3C Recommended concept sets were selected based on\n\nthe comorbid conditions identified by the US Centers for Disease Control (CDC) definitions for COVID-19 as increasing risk of severe COVID-19\nthe comorbid conditions identified in the Charlson Comorbidity Index, and\nothers as requested by N3C leadership for use in the Logic Liaison templates.\n\nThe N3C Recommended concept sets have been vetted by clinical experts (typically N3C Domain Team leads) and the Data and Logic Liaison teams according to the below refinement and finalization process. The latest version of the listed concept sets should always be used, as earlier versions represent iterations and have not passed the final review.\nThe clinical experts and the N3C Data Liaisons and Logic Liaisons perform the following steps in establishing an intentional N3C Recommended concept set:\n\nSelect relevant parent concepts and their descendants from OMOP standard codes, using Charlson Comorbidity description or the relevant section in the referenced CDC page to establish intention.\nUtilize the OHDSI Atlas tool to explore the OMOP hierarchy to look for other potential parent codes and also to remove child codes that are non-specific to the intended scope of the concept set.\nCompare the draft concept set to other related concept sets, using the Concept Set Overlap feature of the Concept Set Editor. This facilitates a review of any codes that do not overlap with those in value sets from reputable stewards such as the National Library of Medicine’s Value Set Authority Center (VSAC) and the Healthcare Cost and Utilization Project (HCUP) and from existing similar concept sets within the Concept Set Browser. Add and remove concepts (and their descendants) as per recommendations from the clinical experts.\nReduce the intentional concept set expression as parsimonious as possible, retaining all the approved concepts collected in prior steps.\nIterate as needed.\nDocument metadata (properties) in the Concept Set Editor for each created concept set to include: Intention, Limitations, Provenance, and at least one vocabulary and one clinical review.\nPresent for final vetting at the Data Liaison meeting for N3C Recommended designation.\n\nThese concept sets are in queue to be published as pdf (Properties) and json (concept ids) in Zenodo, to be available to researchers outside the Enclave. Once published, their Zenodo DOI will be posted to the Zenodo Property of the version that was published. The N3C Recommended concept sets are currently published to the N3C GitHub and are available to researchers outside the Enclave.\nThe N3C Recommended concept sets listed below are used as the default concept sets in the Logic Liaison templates COVID-19 Diagnosed or Lab Confirmed Patients  and All Patients  which then populate the N3C Phenotype Explorer  and N3C Public Health Dashboards.\n\n\n7.3.3.2 Concept set reviews and validation\nWhy should anyone trust a concept set? This trust is needed both within the project team and outside the team. The project team needs to trust a concept set in an analysis; someone outside the team needs to assess its trust in deciding whether to (re)use it in their own analysis.\n“Trust” in a concept set derives from a number of factors. First, do we trust the people who assembled the concept set and its process? Second, do we believe that the concept set encodes the concepts we want it to encode, and excludes those we do not? Third, does it perform within the Enclave data the way we expect it to?\nRe “the people” and “the process”: The concept set meta-data, especially the identity of the author and reviewers, and the Provenance, provide the necessary information.\nAnother means of deciding on a level of trust is the Reviews. The Enclave supports two types of reviews.\n\n“Vocabulary review”: Here, a terminologist or informatician familiar with the OMOP vocabulary has reviewed the process documented (in Provenance) or in real-time (as part of work with the Domain Team) and documents satisfaction with what codes are included and excluded.\n“Clinical review”: Here, a clinical member of the Domain Team or project has reviewed the codes to confirm that they match the “Intention” of the concept set.\n\nA third means of deciding on a level of trust is looking to see what research projects have used the concept set. The more authoritative (and published) those projects are, the more authority they have. Of course, in some cases, a concept set “going viral” may simply be a popularity contest and not an indicator of correctness (false positives). At the same time, linking a concept set to a research project is a manual process, which is often not done (false negatives).\nNote that this entire scheme of authority and validity depends on adequate documentation of properties. There are many enticing concept sets that do not have properties filled, reviews performed, nor projects linked. Please ensure that your concept sets are not missing these key components.\nNote, as well, that, if you publish a paper, using a concept set, that concept set should be published as well. The Properties and Reviews will be published, whether missing or not."
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-ehr",
    "href": "chapters/understanding.html#sec-understanding-ehr",
    "title": "7  Understanding the Data",
    "section": "7.4 EHR-Based Data Beyond OMOP",
    "text": "7.4 EHR-Based Data Beyond OMOP\nData tables of use or interest to analysts found in either the LDS or Safe Harbor Release are the following:\n\n7.4.1 Manifest\nThis table, found in the folder along with the OMOP tables, provides data about each data partner that analysts should find helpful when trying to understand variation in the data: CDM type (ACT, OMOP, PCORNet, TriNetX) and version, whether dates are shifted (and how much), and date of last contribution.\n\n\n7.4.2 Control Map\nFor every “case” generated locally by the Phenotype algorithm, the algorithm also identifies 2 “controls”, that are matched by age, sex, race, and ethnicities, including matched by missing values. While it may be tempting to use this map to identify controls for your own study, too many patients who were initial “controls” may have become “cases”, or their true status is unknown, because of home or other non-site testing. Other matching methods (e.g., propensity scores) are more trustworthy.\n\n\n7.4.3 Microvisit to Macrovisit Map\nWhile admissions to the hospital are recorded in the Visit_Occurrence table, the end date is not always so recorded. Now, in many hospitals, procedures performed during a hospitalization may be recorded in the EHR as an “encounter”. So an admission may be represented in the visit_occurrence table as a string of such “encounters”. We define a macrovisit as a merge of chronological, overlapping inpatient and other longitudinal facility visits, to which we add any other types of visits (outpatient, telehealth, etc) that occur during the merged interval. See the N3C Data Enclave .\n\n\n7.4.4 Harmonized values\nIt is no surprise that, across over 70 data partners, and the many included sites, different institutions measure the same analyte in different ways and report the results with different units of measure. N3C has spent a fair amount of effort to harmonize units (and therefore values; think degrees Fahrenheit and degrees Celsius), so analysts have a level playing field for their analyses. In addition, many units of measure are missing, so, as part of that effort, an algorithm has been developed to use the pooled data across the Enclave to help us infer units (and therefore, values). This strategy has led to the “rescue” of over 78% of records where units of measure were missing; see Bradwell et al. (2022). This harmonization was done for about 70 measurements so far. The results are found in the Measurement table, as harmonized_value_as_number, harmonized_unit_concept_id and unit_concept_id_or_inferred_unit_concept_id.\n\n\n7.4.5 Phenotype and cohort\nA phenotype is a grouping of related terms, or observable characteristics that could be applied to a person, disease trait, medical condition or events (Richesson et al., 2013). Examples would be, “has Acute COVID”; “treated with ampicillin”.2\nA cohort is a set of persons who satisfy one or more inclusion criteria for a duration of time. Data ingestion is the process of importing data meeting a phenotype definition into a database which can then be used for research. In this section, we will discuss existing tools and methodologies for identifying codes and terms, creating phenotypes, managing vocabulary concept sets, and how to ingest codes into the Enclave database that will enable research using the N3C basic and supplemental datasets.\n\n\n7.4.6 Derived variables/facts\nFacts are designations that a patient falls into a category (at a point in time). Thus, confirmed_covid_patient or TOBACCOSMOKER_indicator is set as “1” (true) if certain conditions are met in the template Logic Liaison Template All Patients Facts . Other Logic Liaison Templates include many derived facts of value to analysis. See Chapter 8."
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-pprl",
    "href": "chapters/understanding.html#sec-understanding-pprl",
    "title": "7  Understanding the Data",
    "section": "7.5 Beyond EHR: PPRL-Based Data",
    "text": "7.5 Beyond EHR: PPRL-Based Data\n\n7.5.1 Introduction to PPRL\nPrivacy Preserving Record Linkage (PPRL) is a cryptographically-secure method to link information about individuals from different sources of data, without revealing personal information about those individuals. PPRL enables N3C to collect information such as EHR data from clinical data partners, viral variant information from sequencing centers, mortality information from government and private sources, and government Medicare and Medicaid data, all into a single unified database for researcher use.\n\n\n7.5.2 PPRL Data Access\nAccess to datasets that incorporate the PPRL are available through the N3C Data Use Request process. PPRL requests are treated as level 3 / Limited Data Set requests and as such require a signed institutional Data Use Agreement (DUA), a local letter of determination, investigator attestation to the N3C Code of Conduct, IT security training, and human subject training. Requests for use of datasets linked by the PPRL require approval by the National Center for Advancing Translational Sciences (NCATS) federally staffed Data Access Committee (DAC).\n\n\n7.5.3 Mortality\n\n7.5.3.1 Mortality Data Within N3C\nTo provide more complete information on mortality, N3C has collected additional mortality data from multiple sources. Because the data sources themselves are sensitive information, supplemental PPRL mortality records are broken down into three source categories:\n\nGovernment Mortality: Government data sourced from death certificates and person reporting.\nPublic Obituary: Obituary data sourced from funeral homes, newspapers, and other online obituary sources specifically from https://www.obituarydata.com/ (a private obituary aggregator).\nPrivate Obituary: Obituary data sourced from funeral homes, newspapers, and other online obituary sources sourced from other private sources.\n\nPPRL mortality data are harmonized against the existing Level-3 (LDS) OMOP tables in the N3C Data Enclave. Once access is granted via a Level 3 DUR requesting mortality data, PPRL tables may be found in the Data Catalog under the PPRL Datasets collection. The primary mortality data are available in a table simply called mortality.\n\n\n7.5.3.2 Mortality Data Completeness and Caveats\nFirst, note that mortality information is available for only those data partners who have opted in to linkage of their records against the mortality data. As a result the data_partner_id column will only represent a subset of the data partner IDs found in other N3C OMOP tables. See the Intro to PPRL documentation  for more information.\nSecondly, these data sources should not be considered comprehensive in the sense of providing full information on all deaths in the US. As a result, there may well be mortality records in the OMOP death table that are not represented in the supplemental PPRL data. (And certainly there are mortality records in the PPRL data that are not present in the OMOP death table–that’s why N3C has collected this data in the first place!)\nDifferent data sources lag in inclusion of mortality information from the actual date of death (e.g. if someone’s date_of_death is 2022-04-05, that record may not show up in the mortality source data until 2022-04-18). This lag varies by data source type: government sources tend to lag longer than private sources. For detailed information on mortality data latency, data completeness, and other considerations, see the N3C PPRL Mortality Data Guide  and the N3C Mortality Data FAQs.\n\n\n\n7.5.4 Viral Variant\n\n7.5.4.1 What Are Viral Variants?\nSeveral coronavirus variants have emerged as the virus, SARS-CoV-2, continues to mutate and evolve. For general information about COVID viral variants, see COVID Variants: What You Should Know\n\n\n7.5.4.2 Viral Variants Data Within N3C\nThe collection and linkage viral variant PPRL data within N3C is planned in two phases; in the first phase (completed), N3C links patient summary information about sequenced variants. In the second phase (in development), N3C will provide information on the viral variant’s genome sequence via connection to NCBI sequence databases.\nFor the latest information about viral variant data within N3C, see the N3C PPRL Viral Variants Guide  and N3C Viral Variant FAQs.\n\n\n7.5.4.3 Which sites\nThe initial viral variant data set includes data from a small number of sites. The number of viral variant data sets will increase as more sites from Clinical and Translational Science Awards (CTSA) and Clinical Trials Research (CTR) participate.\n\n\n7.5.4.4 Viral Variants Completeness and Caveats\nThe summary data currently available in N3C includes:\n\nSample Date\nWHO label (e.g. alpha, delta, omicron)\nPANGO lineage (e.g. B.1.1.7)3\n\n\n\n7.5.4.5 Viral Variant Data in OMOP\nSummary PPRL viral variant data is available as an enriched OMOP measurement table available in the PPRL Datasets data. The following columns are used:\n\nmeasurement_concept_id contains OMOP concept_id 36033667, “SARS-CoV-2 (COVID-19) variant [Type] in Specimen by Sequencing”, for all viral variant samples.\nvalue_as_concept_id will be encoded with the OMOP concept representing the WHO label (e.g. concept_id 4228611 for “Omicron”).\nA new column outside the standard OMOP schema includes the PANGO lineage.\n\n\n\n\n7.5.5 Centers for Medicare and Medicaid Services (CMS)\n\n7.5.5.1 Introduction to CMS\nThe Centers for Medicare and Medicaid Services (CMS) is a federal agency that is part of the U.S. Department of Health and Human Services (HHS) that administers the nation’s Medicare Program. The agency also works with state governments to administer programs that include Medicaid and the Children’s Health Insurance Program (CHIP).\nMedicare is the federal health insurance program created in 1965 to provide health coverage for Americans aged 65 and older. The program was expanded in 1972 to cover people younger than 65 who have permanent disabilities. More than 64 million Americans are currently covered by Medicare.\nMedicaid was signed into law in 1965 alongside Medicare. All states, the District of Columbia, and the U.S. territories have Medicaid programs designed to provide health coverage for low-income people. Although the Federal government establishes certain parameters for all states to follow, each state administers its Medicaid program differently, resulting in variations in Medicaid coverage across the country.4\nFor a full description of CMS, see the HHS website.\n\n\n7.5.5.2 CMS Claims Data\nMedical providers (including pharmacies) submit claims for services or drugs for reimbursement of the enrolled participants. The CMS claims provide an audit trail of billing and reimbursement for healthcare services provided to a patient enrolled in an insurance benefit package no matter where the services were rendered. CMS data contain claims from all sites a patient obtained billable services while enrolled and all medications a patient had filled. CMS claims do not contain clinical depth of EHR data and may not contain non-billable (unbillable) services.\n\n\n7.5.5.3 CMS Data in N3C\nThe N3C CMS dataset contains de-identified billing data from the Centers for Medicare and Medicaid Services, formatted into the OMOP common data model for N3C researcher use. The initial CMS data within N3C contains over 240K COVID-19 patients and is updated on a monthly basis. The amount of CMS patients available in the Enclave is dependent on sites participating in the NCATS PPRL linkage initiative, and as additional sites implement the tokenization process the number of patients will increase.\nCMS records available cover the period from Jan 1, 2017 to the most recently available (three months ago observation period). Although N3C refreshes its CMS data feed on a weekly basis, CMS data has a significant lag, with most records taking six months or more to become available in the system. All seven types of MS claims are included: Part B (PB), Durable Medical Equipment (DME), Inpatient (IP), Outpatient (OP), Skilled Nursing Facility (SNF), Home Health Agency (HHA), Hospice. The records contain masked providers of service and masked care sites linkable to other characteristics.\nThe Master Beneficiary Summary File (MBSF):\n\nProvides enrollment period and reasons for enrollment\nIdentifies patients with dual enrollment in Medicare and Medicaid\nCan be used to distinguish non-users from non-eligible\nIdentifies periods of continuous enrollment\nCan be used to perform annualization\n\n\n\n7.5.5.4 CMS into OMOP CDM Structure\nThe N3C Data Ingestion and Harmonization (DI&H) team worked with partner Acumen to identify CMS data to be ingested. DI&H then created the mapping from native CMS data elements into the OMOP data model. DI&H created a Code Map Service and a Data Transformation Pipeline, which involved combining sometimes up to 45 rows of data into a single patient record and creating constructs, such as coherent visits across claims. The pipeline also required deidentification of patient and provider identifiers.\n\n\n7.5.5.5 CMS Data Completeness and Caveats\nCMS claims data provides slightly different information compared to EHR data; for example, rather than providing medication prescription information, CMS provides information on medication dispensation from the pharmacy. Enrollment in Medicare and Medicaid are well defined and thoroughly tracked, providing insight into patients healthcare utilization vs eligibility that may be lacking in EHR data.\n\n\nTable 7.2: Differences between EHR and CMS Claims Data\n\n\n\n\n\n\nEHR Data\nCMS Claims Data\n\n\n\n\n\nUsually contains data only about care received in that healthcare system\nReflects orders or intent\nContains deep clinical data (labs, vitals, notes, surveys, history)\nMay not contain billing data\nUsually does not contain eligibility or enrollment\n\n\nHave little clinical data but have data (Dx, Px, etc) from all healthcare sites a patient received services\nReflect performed services\nReflect filled prescriptions\nContain amounts billed and paid\nPaid claim indicates enrollment\nEHR elements may get altered or added by billing teams or automatically by software to optimize reimbursement (sometimes described as up-coding)\n\n\n\n\n\n\n\n7.5.5.6 Additional CMS Resources\nFor a detailed investigation of the usefulness of EHR vs administrative claims data, see Kharrazi et al. (2017).\nFor detailed information on the N3C CMS data, see N3C PPRL CMS Data Guide .\nCMS Research Data Assistance Center (ResDAC) contains data dictionaries, code books, and enormous amounts of information about Medicare claims and enrollment. It is a good place to figure out what values represent. Look for dictionaries for the CMS Standard Analytic Files (SAF) or Limited Data Sets (LDS).\nA detailed CMS training webinar is available on YouTube."
  },
  {
    "objectID": "chapters/understanding.html#sec-understanding-public",
    "href": "chapters/understanding.html#sec-understanding-public",
    "title": "7  Understanding the Data",
    "section": "7.6 Public/External Datasets",
    "text": "7.6 Public/External Datasets\nBecause our data are not representative of the geographic locations whence they come, it is important for many analyses to attempt to “correct” the results due to this selection bias. Data sets, called external datasets, are available outside the N3C Data Enclave that provide information about such locations. There are datasets about the locations themselves (e.g., zip code distances ), about the demographics in those locations (e.g., zip code census ), or about covid, in those locations (e.g., covid hesitancy by county, state policy by date ). There are also datasets to help in mapping from zip codes (data available in Level 3 Enclave datasets), such as Mapping Zip codes to states and geolocations . All available datasets are available at The Data Discovery Engine (outside the Enclave) and in The Knowledge Store  (inside the Enclave; filter on “External Dataset”).\nNote that most of these depend on 5-digit zip codes for linking between a patient’s record and the external dataset, and so are useful only in the context of a Level 3 DUR.\nImporting such an external dataset entails a process of review that examines the need, the risk to reidentification, and the license. The External Datasets page gives more detail, including how to request a new dataset.\nThis concludes an exciting chapter in the never ending story of N3C. To a cinema near you. Real soon.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nBradwell, K. R., Wooldridge, J. T., Amor, B., Bennett, T. D., Anand, A., Bremer, C., Yoo, Y. J., Qian, Z., Johnson, S. G., Pfaff, E. R., et al. (2022). Harmonizing units and values of quantitative data elements in a very large nationally pooled electronic health record (EHR) dataset. Journal of the American Medical Informatics Association, 29(7), 1172–1182. https://doi.org/10.1093/jamia/ocac054\n\n\nGold, S., Batch, A., McClure, R., Jiang, G., Kharrazi, H., Saripalle, R., Huser, V., Weng, C., Roderer, N., Szarfman, A., et al. (2018). Clinical concept value sets and interoperability in health data analytics. AMIA Annual Symposium Proceedings, 2018, 480.\n\n\nGold, S., Lehmann, H., Schilling, L., & Lutters, W. (2021). Practices, norms, and aspirations regarding the construction, validation, and reuse of code sets in the analysis of real-world data. medRxiv, 2021–2010.\n\n\nKharrazi, H., Chi, W., Chang, H.-Y., Richards, T. M., Gallagher, J. M., Knudson, S. M., & Weiner, J. P. (2017). Comparing population-based risk-stratification model performance using demographic, diagnosis and medication data extracted from outpatient electronic health records versus administrative claims. Medical Care, 55(8), 789–796. https://doi.org/10.1097/MLR.0000000000000754\n\n\nOHDSI. (2019). The book of OHDSI: Observational health data sciences and informatics. OHDSI. https://ohdsi.github.io/TheBookOfOhdsi/\n\n\nRichesson, R. L., Hammond, W. E., Nahm, M., Wixted, D., Simon, G. E., Robinson, J. G., Bauck, A. E., Cifelli, D., Smerek, M. M., Dickerson, J., et al. (2013). Electronic health records based phenotyping in next-generation clinical trials: A perspective from the NIH health care systems collaboratory. Journal of the American Medical Informatics Association, 20(e2), e226–e231. https://doi.org/10.1136/amiajnl-2013-001926"
  },
  {
    "objectID": "chapters/understanding.html#footnotes",
    "href": "chapters/understanding.html#footnotes",
    "title": "7  Understanding the Data",
    "section": "",
    "text": "This translation is automated with software, as opposed to manually performed by a human.↩︎\nA phenotype is general and a cohort is specific. Thus, a new-onset diabetes phenotype might say, “Must have some number of outpatient visits without a diabetes diagnostic code, followed by at least one visit with such a code.” A related cohort would be, “Must have _2 _outpatient visits without diabetes since Jan 1, 2017 without a diabetes diagnostic code, followed by at least one visit with such a code before Jan 1, 2020.” However, in OHDSI the two terms are used interchangeably. See the Cohorts chapter in OHDSI (2019).↩︎\nNote that while WHO label should always be available, PANGO lineage may only be available for a subset of samples↩︎\nSee https://www.medicareresources.org/glossary/centers-for-medicare-and-medicaid-services/.↩︎"
  },
  {
    "objectID": "chapters/tools.html#sec-tools-intro",
    "href": "chapters/tools.html#sec-tools-intro",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThis chapter introduces tools in the N3C Data Enclave used to analyze data, view results, track project progress, and obtain shared data and code developed in the N3C community. The focus is on accessing and using each tool, the skill level needed, as well as what types of analyses each tool is geared toward. It is expected that you know how data are organized in the Enclave, including the OMOP data model, vocabulary, and concept sets (see Chapter 7 for details).\n\n\n\nFigure 8.1: High-level overview of an N3C project.\n\n\nDue to the complexities of analyzing large clinical datasets, such as that compiled in the Enclave, it is common, and many times necessary, to work in multidisciplinary collaborative teams to answer a research question. Figure 8.1 provides a high-level overview of the process behind forming a team and performing research using the Enclave, along with the recommended field expertise needed during each phase. It is important to note that while certain team members may take the lead at various stages, a project benefits if all team members are engaged to some degree at all phases. Managing these collaborative and multi-faceted projects requires good recordkeeping. The N3C Protocol Pad is designed specifically for N3C research and to aid teams in designing, implementing, reporting, and publishing their work in a Findable, Accessible, Interoperable, and Reusable (FAIR) manner (Wilkinson et al., 2016) (see also https://www.go-fair.org/fair-principles/). Thus it is recommended that you utilize this tool throughout the implementation of your project.\nA research project in N3C starts with organizing a team with the required expertise (clinical, informatics, statistical, etc), followed by defining clinical questions around COVID-19 and characterizing the cohorts needed to answer each, i.e. clinical phenotyping. Because the N3C contains real-world EHR data that is harmonized from multiple data models and dozens of institutions, some information needed to identify an ideal clinical phenotype may be missing or incomplete. Thus, it is important to assess what information is needed to create an N3C computational phenotype for your cohorts. This could include using conditions, labs, or medications as proxies to identify a cohort if some information is not available. Generally, clinicians or other subject matter experts are leading this process with informaticians/data scientists providing guidance on what information is included in the Enclave, what is missing or sparse, and overall data quality (see Chapter 9). Some data quality aspects can be easily obtained through the use of Logic Liaison Templates accessible through the N3C Knowledge Store. The Enclave application Contour can be utilized at this stage, along with Code Workbooks for quick querying and visualizing of the data. Additionally, Fusion can be utilized to keep track of developed concept sets and utilized to easily input them into Logic Liaison Templates.\nThe generation of a computational phenotype overlaps with the generation of concept sets (see Chapter 7 for details), and is often a cyclical process. Well-vetted concept sets are key to obtaining robust cohorts, thus, having a team member familiar with the organization of data and the OMOP vocabulary, such as a data liaison, who can work closely with a clinician is beneficial. Concept set generation can be done using the Enclave Concept Set Browser, or externally through OHDSI ATLAS.\nInformaticians and data scientists then utilize the computational phenotype and vetted concept sets to generate fact tables (i.e. datasets containing information about each patient like demographics, comorbidities, lab results, etc) for the cohorts of interest using the raw OMOP tables, which requires specific knowledge of how to work with large datasets in a Spark environment. Fact tables include all the information needed to characterize a cohort and perform downstream analyses to answer your research questions. Facts can include patient demographics, socioeconomic status, COVID status/severity, medications, comorbidities, etc. Logic Liaison Fact Table Templates can provide you a boost by allowing fast and robust generation of commonly used facts using N3C-vetted concept sets and peer-reviewed code as a starter table. You can then append this base fact table to include project-specific facts needed for analyses. Figure 8.6 and Figure 8.7 in the N3C Knowledge Store section of this chapter provide a more detailed view of how Logic Liaison Templates can be integrated into a project to expedite fact table generation. The generation of the original fact tables from raw OMOP tables can be done using Code Workbooks (Section 8.4.3) or Code Repositories (Section 8.4.4).\nData scientists and statisticians can then analyze the extracted and formatted fact tables. This includes statistical tests, summary tables, visualizations, and reports for the team to discuss. Data analysis is also a cyclical process with all team members engaged in assessing results and circling back to further refine the computational phenotype and concept sets if needed. Depending on the type of analysis needed, Code Workbooks or Contour can be utilized at this step, followed by Foundry’s Notepad for reporting results for secure team dissemination within the Enclave environment.\nOnce you obtain results that you wish to share with others, all tables, figures, and other data needed for reporting in publications, conference submissions, presentations, or any other activity outside the Enclave environment must be submitted as a Data Download Request for a download review by NCATS (see Chapter 10). The download request is meant to ensure no prohibited data is being downloaded as per the N3C Data Download Policy summarized in the Publishing and Sharing Your Work chapter. After approval, your results can be included in research outputs, such as publications, and then submitted to the Publication Review Committee (see Chapter 10). This step is necessary to ensure data are being reported properly in the context of the research project and that proper attribution is being given to all those who contributed to the success of the research, either directly or indirectly. Upon approval, you are free to submit to the venue of choice and freely present the approved data to anyone at any time. Data download requests are performed within the Enclave environment, followed by submitting a Google Form to the Publication Review Committee.\nThe following sections of this chapter discuss each of the features and applications needed to perform research in the Enclave, and include links to external Foundry documentation, as well as direct the reader to other chapters of this book that contain a deeper dive into various N3C topics, such as the organization of data and best practices. This chapter is best utilized alongside the information provided in the next chapter, Best Practices for the Research Life Cycle (see Chapter 9), which includes information on recommended data workflows, such as scheduling automatic data builds, to keep your research current, managing your projects using the Protocol Pad, and much more."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-concepts",
    "href": "chapters/tools.html#sec-tools-concepts",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.2 Using Concept Sets",
    "text": "8.2 Using Concept Sets\nAs discussed in the previous chapter Understanding the Data (see Chapter 7), the electronic health information coded in the various vocabularies used across the country are mapped to the OMOP common data model. By leveraging the hierarchical structure, parent codes and descendants can be captured in one fell swoop to create intensional concept sets for use in analysis.\n\n\n\nFigure 8.2: Concept Set Browser Homepage.\n\n\nThe Concept Set Browser shown in Figure 8.2 is an N3C-specific tool that allows you to explore and modify existing concept sets as well as create new concept sets to fit your exact study needs. For more details around the process of concept set creation, read the Concept Sets section in Chapter 7. It is recommended that, if you are a new researcher, you start your search for a concept set with the list of N3C Recommended concept sets. These concept sets have been frozen in their validated state by the Logic Liaisons and the Data Liaisons after obtaining clinician and informatic reviews (see Chapter 7). They are also the ones used to identify common comorbidities and other facts on the Phenotype Explorer and in the Logic Liaison Fact Table templates. The recommended other method of finding commonly used concept sets is by exploring bundles within the Concept Set Browser. These are groups of concept sets that are often used together. Further exploration of all other concept sets in the Concept Set Browser is also an option, though only advised when proceeding with the understanding that many of the existing concept sets that are not part of the N3C Recommended or bundles are crafted to be specific to one particular study team’s requirements.\nOnce concept sets have been identified for use in your analysis through the Concept Set Browser, the concept set members table becomes the link between concepts and the encompassing concept set as shown by Figure 8.3. You then have to “point” the code to the concept set members table to access this linkage. Once this has been accomplished, the choice becomes using the most recent version of a concept set (concept_set_members.concept_set_name where most_recent_version = TRUE) or using a specific version of the concept set (concept_set_members.codeset_id = {codeset id value}). While there are alternative ways to utilize concept sets and concept ids, the method described above is highly recommended primarily for the ability to quickly update a concept set without having to find and change hard-coded concept ids in a data processing pipeline.\n\n\n\nFigure 8.3: concept_set_members table.\n\n\nReferring to a concept set by name and using the most recent version is often the preferred method for concept sets marked as N3C Recommended since these concept sets can only be updated by N3C core contributors after they have gone through a validation process which has been described in the previous chapter, Understanding the Data.\nFor concept sets that have not undergone the validation process and have not been marked as N3C Recommended by the N3C core contributors, it is recommended that the research team performs their own validation on an existing concept set or creates a new concept set with the input of a clinician. The concept set should then be referenced using its codeset ID when you are performing your data analysis. This will allow your team to perform their analysis from start to finish without worrying about unvalidated modifications to the concept set. However, the codeset id being referenced in the code may need to be updated if the team chooses to modify the concept set once starting the analysis.\nIn constructing phenotypes from concept sets, concept sets may also need to be joined together; these actions are best done in SQL/R/Python Code Workbook transforms with the use of the Logic Liaison’s Combined Variable template  or in Code Repositories."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-store",
    "href": "chapters/tools.html#sec-tools-store",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.3 N3C Knowledge Store",
    "text": "8.3 N3C Knowledge Store\nThe N3C Knowledge Store is an application where you, as an N3C Data Enclave user, can discover shared code templates, external datasets, reports, cohorts, and Python libraries (collectively also known as Knowledge Objects or KOs) and share similarly re-usable Knowledge Objects of your own with other Enclave users, regardless of the specific project from which the resource originated. Most Knowledge Store (KS) objects come about as core contributors and researchers alike develop resources they believe may be useful for others either within or outside of their research project team and wish to share them with the broader community. If you find yourself in this situation, you can easily create, submit, and share a KS resource by following this Code Workbook Template Quick Start Guide . Otherwise, more specifics on how to navigate the KS can be found in this Knowledge Store Guide  within the Enclave.\n\n8.3.1 Datasets\nOf the many types of Knowledge Objects, the most common are datasets and code templates. Datasets in the Knowledge Store can be internal or external. Internal datasets are generated from data inside the Enclave, typically by researchers as part of their project, and are often of patient or row-level granularity. As described in the previous chapter Understanding the Data, external datasets found in the Knowledge Store provide a wealth of information from public datasets that have been brought into the Enclave along with the crosswalks necessary for joining these aggregate data to person-level data at various levels of granularity (see Chapter 7). Either type of dataset can be imported into a workbook or code repository of the appropriate data access level to be used as a starting point for further transformation or analysis.\n\n\n8.3.2 Code Templates\nDepending on the author’s intended use, some code templates can be applied to your custom input dataset while other code templates produce a dataset that can be joined to your study dataset. The code templates themselves can also be imported and customized to produce a dataset for defining a study cohort with key information to use during analysis or simply used as example logic if you are newer to coding. Code templates, in general, are often meant to help transform the massive amount of raw data to smaller, more digestible, and more readily applicable datasets and facts. A few helpful starter templates are those produced by the Logic Liaisons (see the Logic Liaison Services section in Chapter 11), some of which can be seen in Figure 8.4.\n\n\n\nFigure 8.4: N3C Knowledge Store Homepage.\n\n\n\n\n8.3.3 Logic Liaison Fact Tables and Templates\nThe Logic Liaison Fact Tables and Templates are specifically designed to provide a validated and community agreed-upon method for calculating particular facts at the date level and/or the patient level. Through surveying the Domain Team Leads to establish a list of commonly derived variables and continuous feedback from the N3C Community to refine and update this list, the Logic Liaisons have developed, disseminated, and maintain two main fact table templates and an additional fact table template for SDoH variables. These two main fact table templates each produce day-level and person-level data frames of commonly used derived base variables for all N3C patients  as well as a subset who have an index date for their acute COVID-19 infection, the confirmed COVID-19 positive patients  (PCR/AG positive or U07.1 COVID-19 diagnosed). These day-level and person-level datasets are commonly referred to as the Logic Liaison Fact Tables and can be imported directly into a workbook for use without modifying the template. These fact tables are produced using a default set of concept sets from the N3C Recommended Concept Sets list and a set of default values for the template’s parameters.\nThe main fact table template KOs include not only the shared logic for importing/customizing the template for both data access levels, but also a detailed README, example datasets (aka default Logic Liaison Fact Tables), and example code workbooks as exemplified by Figure 8.5. It is recommended you first open the README and example code workbooks to see how the default fact tables are generated and then decide whether you would like to use the fact tables as they are or import the template to customize concept sets and/or template parameter values to generate your project-specific version of the fact tables. The SDoH Variables ALL PATIENTS  template provides you with a curated set of 70 geographically-based SDoH measures tied to each patient. Because joining this data requires a valid five-digit zip code, these fields are only available for patients with a five-digit zip code in Level 3 (LDS) data.\n\n\n\nFigure 8.5: Example Logic Liaison Fact Table Template Knowledge Object.\n\n\nThe Logic Liaisons have also developed, disseminated, and maintained a handful of overall data quality templates, ancillary fact table, and ancillary data quality templates in the Knowledge Store. Figure 8.6 depicts how one could apply the set of Logic Liaison Templates to generate augmented fact tables with a goal of asking one or more research questions within that project.\n\n\n\nFigure 8.6: Example Application of Logic Liaison Overall Quality, Fact Table, and Ancillary Fact Templates.\n\n\nThe data quality templates provide a variety of data tables and visualizations. The first two of which provide a method for evaluating overall quality of the harmonized data ingested from sites.\n\nData Density by Site and Domain : Calculates the Standardized Density, Median Absolute Deviation (MAD), and Directional Median Deviations (DMD) with respect to the number of unique patient/concept/days for each of the major OMOP tables (i.e. condition_occurrence, drug_exposure, etc) and uses them to create a heatmap displaying how many MADs each site is from the median for each OMOP table. The template also scores the site’s date-shifting practices.\nWhitelist Filtering : Creates a bar plot showing whitelisted data partners that have, at minimum, a certain percentage of COVID patients associated with a specified measurement, condition, drug, procedure, etc. Sites not meeting the minimum requirement are removed from the whitelist. These tables can be used in downstream filtering to keep only sites meeting the user-defined minimum data quality.\n\nOnce the main fact table templates mentioned earlier this section have been applied to generate the base fact tables, the ancillary fact templates utilize the day-level and person-level datasets of the base fact templates to efficiently generate additional derived variables based on broadly requested and applicable logic such as:\n\nVaccine Fact : Creates a vaccine fact table at the person level that summarizes their vaccination information\nStudy Specific Fact Indexing : Summarizes the indicators of a visit-level patient fact table with respect to whether they were present pre-, post-, or in a user-defined window surrounding an index date corresponding to a study-specific event. This template is similar to the All Patients Facts Tables with the exception that it is organized around a study-specific event rather than COVID diagnosis.\nCombined Variables ALL PATIENTS : Allows you to combine two variables (variable 1 and variable 2) in a visit-level table by creating a new “same-day occurrence variable” indicating that both variables appear that day for that patient. You can also choose to make an “either/or” variable in the visit-level table that combines two variables (variable 3 and variable 4) into a new variable that flags days where at least one of the input variables is recorded for that patient.\nCombined Variables COVID PATIENTS : Same as above except for patients with facts found based on their covid index date.\nCCI Score : Provides an all-time score or a before-or-day-of-covid score (depending on your selection in the template) per patient based on the CCI weights (Charlson et al., 1987).\n\nFigure 8.7 is a continuation of Figure 8.3 to demonstrate how you could continue to apply the Logic Liaison Templates to the augmented fact table created in order to answer their specific research question within a project.\n\n\n\nFigure 8.7: Example Application of Logic Liaison Ancillary Quality Templates.\n\n\nThe ancillary data quality templates are intended to be applied to fact tables after cohort creation and initial variable creation to stratify question-specific facts by site.\n\nSystematic Missingness by Site and Study Variable : Produces a final visualization that has a binary indicator for whether or not a site is systematically missing meaningful data for the study variables in the input dataset\nFact Density by Site Visualization : Calculates the Standardized Density, Median Absolute Deviation (MAD), and Directional Median Deviations (DMD) with respect to the numerical values in each column of the input table (any non-numerical field is converted to a binary value using the isNotNull() function) and creates heatmaps to visualize the metrics.\n\nOnce you obtain results you wish to share outside of the Enclave for a project, the results must undergo a Data Download Request before being available to export outside of the Enclave. Additional details around exporting results can be found in the Publishing and Sharing Your Work chapter (see Chapter 10). Training materials for getting started with Logic Liaison Templates are available here  within the Enclave. Outside of the main confirmed COVID+ template and a subset of columns in All Patients, these templates could be generalized to apply to research projects outside of the Enclave where studies do not necessarily need to fall within the scope of COVID-19.\nWhile it is not necessary to utilize Knowledge Store resources when conducting your research project, it does allow you to get a jump-start on gathering and understanding the data by avoiding effort duplication and providing a general starting point. You can then build upon this fact table using the ancillary templates that allow self-definition of the index event, combining variables, generating a CCI score, and associating SDoH variables based on zip code crosswalks. The Logic Liaison ancillary data quality templates provide the same structure for analyzing data missingness, density, and contribution quality by site. Further explanation as to why these Knowledge Store objects are highly applicable can be found in Best Practices for the Research Life Cycle (Chapter 9)."
  },
  {
    "objectID": "chapters/tools.html#sec-tools-apps",
    "href": "chapters/tools.html#sec-tools-apps",
    "title": "8  Introducing Enclave Analysis Tools",
    "section": "8.4 Enclave Applications",
    "text": "8.4 Enclave Applications\nThis section will cover the usage of various applications made available in the N3C Data Enclave, including Protocol Pad, Contour, Code Workbooks, and more (a complete list of Foundry applications can be found here). Before designing and running an analysis utilizing data in the Enclave, it helps to understand the concepts of a “data pipeline” and a “data transform” as well as how the data are stored and accessed via Apache Spark on a distributed file system. \n\n8.4.1 N3C Protocol Pad\nN3C Documentation: Quick Start Guide  and Detailed Instructions .\n\nElectronic lab notebook\nPromote collaboration, organize work, and translation to final manuscript\nMethodology checklist\n\n\n\n\nFigure 8.8: N3C Protocol Pad Homepage.\n\n\nBefore diving into an analysis, it is highly recommended that you use N3C’s Protocol Pad shown in Figure 8.8 to organize your thoughts and path forward. Research studies can span many months and pass through the hands of many team members before reaching a stage where you may want to share the results through publication or other approved means. The Protocol Pad serves as an electronic lab notebook to help organize tasks, track progress, and document results in a cohesive format throughout the process of reaching a study’s final state. As a result of this organization and tracking, the tool facilitates easy translation of the work, decisions, and contributors to producing a final manuscript if that is the goal of the protocol.\nProtocol Pad is the foundation for performing reproducible and repeatable science within the Enclave. The templates, checklists, and links to key resources provided within the tool guide you along the path of a well-structured division of labor and use of best practices when performing research using the observational data in the Enclave. The tool also assists your team in keeping the work in alignment with the project. A more in-depth explanation of how Protocol Pad can facilitate research will be available in the future.  Documentation on the tool’s functionality can be found in the QuickStart Users’ Guide to Protocol Pad  as well as the Protocol Pad White Paper and Detailed Instructions .\n\n\n8.4.2 Contour\nFoundry Documentation: Contour Overview\n\nProgramming-free analysis interface\nPoint-and-Click data analysis pipeline development\nData summary and visualization\nAllows customization with Contour’s expression language\nDashboard development\n\n\n\n\nFigure 8.9: Example Contour Analysis.\n\n\nThe Contour application is a programming-free interface to the Enclave that allows those with limited knowledge of Python, R, and SQL to create top-down analysis pipelines in a point-and-click fashion, as well as generate dynamically updated dashboards. A Contour analysis starts by specifying the path to the input dataset as shown in Figure 8.9. Subsequent transforms of that dataset are then specified, which can include adding/calculating new columns, filtering rows or columns, joining with other datasets, creating summary figures and charts, summarizing tables through pivoting, and more. Contour’s expression language also allows for more complex querying and data aggregation beyond the default operations provided. Once an analysis is complete, resulting datasets can be saved to the Enclave for use in additional Contour pipelines or other Enclave applications, like Code Workbooks. Contour can also quickly create summary figures from source tables without code, and has a variety of visualization options, including bar charts, histograms, and heatmaps.\nFigures or summary tables created in Contour can be exported to a dashboard within Contour. A Contour Dashboard allows chart-to-chart filtering, and an easy drag-and-drop interface to build the dashboard while iterating on an analysis. A dashboard is dynamic and interactive, allowing the reader to adjust the graphs to better explore analysis results in a guided and structured way. Figures generated in a Contour analysis can also be exported to other Enclave applications like Notepad for reporting results. The main difference between Notepad and a Contour Dashboard is that Notepad provides a static report with figures that cannot be dynamically changed by the reader. A detailed orientation to Contour can be found in the Foundry Documentation here.\n\n\n8.4.3 Code Workbooks\nTutorial: Intro to Code Workbook \nFoundry Documentation: Code Workbook Overview\n\nGraphical organization of logic\nSimplification of code\nEasy reuse of pre-authored logic\nAdd visualizations to reports\nSupports Python, R, SQL, PySpark, and SparkR\nConsole debugging and development\nBranching facilitates collaboration and reproducibility\nWorkspace to reuse templatized logic\nExporting code to GitHub\n\nCode Workbook is a GUI-based application for you to apply code-based transformations to datasets for the purpose of creating new datasets and visualizations. The explicit goals of the application are to facilitate a collaborative environment in which you can quickly iterate over logic to produce artifacts interoperable with the suite of Enclave applications. The default Code Workbook interface is structured as a directed graph in which nodes represent either datasets or transformations that output datasets. Edges represent the flow of data through the graph such that upstream datasets are inputs for logical operations performed by downstream code transforms. Figure 8.10 shows an abstract version of a Code Workbook, while Figure 8.11 is an actual screenshot in the Enclave where data flows from the left to the right. Any dataset which you have access to within the workspace where the Code Workbook is located can be imported as an input to the various types of transformations. For each transformation one or more tables are specified as the input and are transformed into a single output table (Figure 8.10). Multiple transformations can also be strung together (Figure 8.11) to create an analysis pipeline (see Foundry’s Anatomy of a Data Pipeline module for more detailed information, as well as Chapter 9). In Code Workbook, the primary and required output of a transformation is always a single table; however, visualizations such as graphs and charts can also be generated and saved along the way. Note that the code performing the transformation and the resulting output table are always represented as a single node in the Code Workbook interface.\n\n\n\nFigure 8.10: Abstracted view of data transforms.\n\n\n\n\n\nFigure 8.11: Example Data Transformation Pipeline in Code Workbook.\n\n\n\n8.4.3.1 Types of Transforms\n\nManual Entry transforms allow you to manually populate a custom dataset directly in the specific Code Workbook you are currently working in as a “quick and dirty” alternative to manually populating importable or referenceable datasets with Fusion).\nPython/PySpark Code transforms contain a Python function that takes one or more datasets as input parameters and returns a single dataset as output.\nR/SparkR Code transforms contain an R function that takes one or more datasets as input parameters, and outputs a single data frame.\nSQL Code transforms let you write a Spark SQL query to create a new dataset from the available input datasets.\nTemplate transforms are parameterized blocks of reusable code which you can configure from a point-and-click interface. Many code templates are available in the Knowledge Store, but you can create your own templates scoped to their workspace. Multiple single-node templates can be combined to create a multi-node template to allow reuse of entire configurable pipelines.\nVisualize transforms offer a point-and-click interface for you to create figures and pivot tables. Note this transform is only available for use on saved datasets; it cannot accept unsaved transformations as an input parameter.\n\nBoth Python and R transforms can optionally return a single dataset and produce visualizations using Python libraries/R packages. Any visualization produced in a Code Workbook can subsequently be embedded in a Notepad document. Datasets returned by a transform are ephemeral by default, that is, the transform must be recomputed each time the dataset is used as a downstream input, but options exist to conserve compute power by either caching or saving the output. Caching stores the output temporarily, while saving the dataset stores it permanently in the Enclave. It is recommended that transforms in a pipeline requiring significant compute time be saved as datasets to reduce iteration time during development. In addition to manually clicking “Run” on a transform, a build schedule can be defined to recompute it at regular intervals based on a trigger. You can choose the trigger to be the successful update of an input dataset or simply the elapsing of a specified period of time. This capability can best be visualized using Data Lineage.\nThe Console, Global Code, and Logs panels facilitate quick iteration and code development and debugging. The Console allows you to interactively execute and check syntax in Python, R, or SQL outside of a transform node. Global Code allows you to define custom functions available to all transforms in the Code Workbook. Each code transform includes a Logs panel to view console output generated by code and to view detailed stack traces when transforms fail due to an error.\nMany Python and R transforms rely on external libraries and packages which can be made available via the Environment Configuration. The Enclave provides default configurations tailored to common use cases. For instance, the default profile includes common packages like pandas and tidyverse for routine analysis, whereas the profile-high-memory profile includes packages like foundry_ml for machine learning. You can create your own Environment Configurations which include packages meeting your project’s specific needs.\nNot all libraries and packages are included in the list of options, but you can submit a ticket within the Enclave by clicking on Help & Support &gt; Help Center &gt; Report an Issue to request additional packages be made available. The Enclave maintains instances of the default configurations on warm standby, allowing them to be quickly initialized when you request a new environment. Custom configurations require more time for initialization as instances of these must be started from scratch rather than merely assigned. For this reason, it is recommended to use default configurations when possible.\nFollowing best practices for collaborative software development, Code Workbook allows for branching of the logic within a workbook. As with other popular version control technologies (i.e. Git), branching allows you to make copies of a workbook which your team members can develop independently of the source workbook. Once the development in a particular branch is deemed complete, it can be merged back into the originating branch. Prior to the merge, you can preview both line-level differences within each node, as well as node-level differences of nodes that have been added/removed. Good practice dictates that you perform all development on individual branches, which are then merged back into a common master branch.\nBecause the master branch can change in the interval between you creating a branch and merging it back in, it is important to preview merge changes to ensure that the branch’s contributions are both correct and compatible with the current state of the master branch. However, note that your branch will be automatically deleted after it is merged, which is an important difference from the normal Git behavior. Another prime use case for code branching is to ensure the reproducibility of a given dataset used in a research project. Because the OMOP and N3C-curated datasets are also versioned, you can create a code branch in which all input datasets are set to the same version release (as shown in Figure 8.12) to effectively freeze a dataset used in a specific analysis for later reproducibility while still allowing the possibility of adding additional features. User-generated datasets are set to the same branch as the Code Workbook in which they were created. Finally, Code Workbooks allow you to collect and download all coded transformations within a single workbook into a Git repository that can be easily uploaded to GitHub for public dissemination.\n\n\n\nFigure 8.12: Pinning to an input dataset’s release version.\n\n\nPalantir has created extensive documentation of the Code Workbook application including tutorials. N3C has also published training materials .\n\n\n\n8.4.4 Code Repositories\nFoundry Documentation: Code Repository Overview\n\nProduction pipelines\nCode reuse across projects\nBuilt-in version control\n\n\n\n\nFigure 8.13: Example Code Repository.\n\n\nCode Repositories shown in Figure 8.13 are available within the Enclave and should be used when you wish to share code across multiple Code Workbooks or projects, or need to develop a robust production pipeline. Code Repositories do not support data visualization, coding in R, or point-and-click templates. Additional differences between Code Repositories and Code Workbooks can be found in the Foundry Documentation. Palantir has also compiled several tutorials on how to create, publish, and maintain Code Repositories .\nFor any large analytic project, there are many pieces of code and other artifacts that should be shared between different components within the project. Some code is useful enough that it should be shared across projects. Code Repositories provide a standard mechanism for encapsulating reusable components and allowing them to be reused within projects.\nUnderlying the Code Repository is the Git version control system. You can edit and maintain code using all of the capabilities of Git including comparing versions, branching, and pull requests. Versions that are tagged in Git will be automatically published in the Enclave as a shared library, which allows you and others that have access to your project space to import that code into other Code Workbooks (note this requires you to create a customized environment that imports your code specifically, which may increase workbook initialization time). If you want to make your repository public so others outside of your project workspace can use it you can do one of the following:\n\nPackage your code into a properly structured Python package. This allows other researchers from different project spaces to run it from their Code Workbooks or Repositories.\nSubmit the Code Repository to the Knowledge Store where it will then be visible to anyone who has Enclave access.\nPublish the Code Repository to a public Github where the code will then become accessible to anyone outside the Enclave.\n\nMore information on Code Repositories can be found in the Foundry Documentation (Palantir, 2023).\n\n\n8.4.5 Fusion\nFoundry Documentation: Fusion Sheet Overview\n\nUseful for writing back datasets for use within the Enclave\nLeverage cell references and spreadsheet functions\nSync tables to a dataset to use in other Foundry applications\nCreate charts\nAllow customization and flexibility\n\n\n\n\nFigure 8.14: Example Fusion Sheet.\n\n\nFusion, shown in Figure 8.14, is a spreadsheet application within the Enclave analogous to Microsoft Excel or Google Sheets. Palantir provides extensive documentation. Fusion allows you to sync specific cell ranges within a spreadsheet to Spark datasets, which can subsequently be imported into any other Enclave application. Fusion is an excellent option for use cases which require manual data entry, such as curating lists of concept sets (see Chapter 7) to configure the Logic Liaison Fact Tables and Templates. Unlike many other Enclave applications, Fusion is not suitable for large datasets; each document has a maximum size of 50 MB. Similar to Google Sheets, multiple users can simultaneously view and edit the same document.\nFusion provides many features familiar to other spreadsheet applications such as cell-referencing formulas, formatting, and a charting library to name a few. While you cannot directly import external .xls/.xlsx formatted files into the Enclave, you can copy/paste external dataset values into Fusion. For example, you can copy and paste concept IDs from ATLAS to use in an analysis or in metadata manually curated for a dataset. To import larger external datasets into the Enclave see Chapter 7, and N3C procedures and protocols around importing large external datasets. In addition to standard spreadsheet functionality, Fusion has additional features which allow it to integrate with the rest of your Enclave environment. Objects created within Fusion, such as formatted tables, can be embedded in Notepad. Finally, Fusion sheets can be templatized to facilitate replication of similar functionality.\n\n\n8.4.6 Notepad\nFoundry Documentation: Notepad Overview\n\nNote-taking with ability to add embeds of the workflow\nDocumentation of pipelines or datasets\nCreate report templates\nMonthly status reports using template function\nUnable to create dashboard or complex page-based text editing\n\n\n\n\nFigure 8.15: Example Notepad Document.\n\n\nMany research projects in the Enclave are complex, involving multiple summary datasets, statistical analyses, and visualizations scattered across multiple applications and documents. Notepad is a tool that is often used for consolidating various research artifacts from multiple sources within the Enclave into a single coherent document as shown in Figure 8.15. Formatted Fusion tables, Contour charts, Python/R-generated images from Code Workbooks, and more are all embeddable in a Notepad document, with the option to add a title and caption for each artifact. Users can also create sections and provide narrative structure to their documents using Markdown. A Notepad document can be arranged and configured using a point-and-click interface.\nAll embedded objects can be configured to remain static or refresh automatically when the underlying data sources update. Notepad is also useful for annotating documents, presenting an executive summary of results for internal stakeholders, or external presentations after being approved for download request and export as PDF. A Logic Liaison Template in the Knowledge Store generally includes a README which is created using Notepad. The tool does have limitations in that it cannot be used to create dashboards that include chart-to-chart filtering; however, a Contour Dashboard can provide this feature for tabular data and a Quiver Dashboard can provide this feature for object or time series data. Palantir has curated documentation for creating and editing Notepad documents. Palantir also has documentation for their application known as Reports that was previously used in a similar fashion though with less functionality compared to Notepad.\n\n\n8.4.7 Data Lineage (aka Monocle)\nFoundry Documentation: Data Lineage Overview\n\nFind datasets\nExpand or hide a dataset’s ancestors and descendants\nVisualize a data pipeline and its details\nFacilitate dataset build scheduling\n\n\n\n\nFigure 8.16: Example Data Lineage Visualization.\n\n\nWhether you’re creating a data pipeline for your research project or investigating one from the Knowledge Store, you’ll likely want to holistically assess the dataset’s origins with the Data Lineage tool shown in Figure 8.16. The data pipeline flows from left to right, which is an intuitive way to visualize the relationships between datasets and their ancestors or descendants. Views are enhanced with color coding and grouping. The Data Lineage tool allows you to view details such as a dataset’s schema, last build datetime, and the code that generated the dataset. You can use this lineage tracing tool to understand and verify the data curation methods when using Knowledge Objects and other shared datasets as part of their study analysis. The Data Lineage tool also allows you to see upstream dataset(s) aka potential triggers and downstream dataset(s) aka potential targets for setting up dataset build schedules. Foundry Documentation provides additional instructions and descriptions of Data Lineage.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nCharlson, M. E., Pompei, P., Ales, K. L., & MacKenzie, C. R. (1987). A new method of classifying prognostic comorbidity in longitudinal studies: Development and validation. Journal of Chronic Diseases, 40(5), 373–383. https://doi.org/10.1016/0021-9681(87)90171-8\n\n\nPalantir. (2023). Documentation: Code repositories overview. https://www.palantir.com/docs/foundry/code-repositories/overview/.\n\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., et al. (2016). The FAIR guiding principles for scientific data management and stewardship. Scientific Data, 3. https://doi.org/10.1038/sdata.2016.18"
  },
  {
    "objectID": "chapters/practices.html",
    "href": "chapters/practices.html",
    "title": "9  Best Practices for the Research Life Cycle",
    "section": "",
    "text": "Chapter Leads: Ken Wilkins, Harold Lehmann\n\n\n\n\n\n\nNote\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1ExkYChsnO3hYZk6HCI5cEfQdQJ9F-ynw\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-committee",
    "href": "chapters/publishing.html#sec-publishing-committee",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.1 Publication Committee Process",
    "text": "10.1 Publication Committee Process\nThe Publication Committee is a multidisciplinary team that meets on a weekly basis to evaluate abstracts and manuscripts prior to submission for peer review (and / or preprint). The committee’s primary goal is to ensure that authors comply with N3C policies and procedures. This chapter describes attribution and publication principles regarding community dissemination of research resulting from N3C.\nThe guiding principle of the Publication Committee is to be a steward of the following critical components of the N3C:\n\nData partners and individual patients: Assure that the publication is:\n\nConsistent with the approved Data Use Request (DUR)\nConsistent with policies to protect the identity of individuals, data partners, and marginalized groups\n\nIndividual consortial contributors: Assure inclusive and appropriate attribution of the work that went into the data elements used for a given analysis or publication. Many individuals contribute time and expertise into building the N3C, and it is important to acknowledge their work. Thus, a tiered approach to authorship has been developed (see Table 10.1).\nN3C collaborative: Assure that information about the N3C is represented accurately\nAuthors: Serve as a resource for authors\n\n\n\nTable 10.1: Authorship tiers designed to accurately represent publication contributions\n\n\n\n\n\n\n\nTier 1:Masthead Authorship\nTier 2:Consortial Collaborators\nTier 3:In Text Acknowledgements\n\n\n\n\nSubstantial intellectually important contributions made directly to the manuscript\nLess substantial but still intellectually important contributions made directly to the study/manuscript or to knowledge artifacts used directly by the study, such as concept sets or code\nImportant contributions to the N3C infrastructure\n\n\n\n\nThe first two tiers are indexed in Medline, which is an important way for contributions to acknowledge those who made them. Tier 1 and 2 authors must satisfy ICMJE criteria:\n\nThe ICMJE recommends that authorship be based on the following 4 criteria:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or revising it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\n\nThe publication committee has a role in helping authors identify Tier 2 and Tier 3 contributors:\n\nNotify core contributors (named in covid.cd2h.org/acknowledgements) and relevant domain teams of manuscripts that might be using their work\nEnable core contributors and relevant domain teams to review the manuscript and either opt out of acknowledgment or opt into consortial contributorship. Opting into consortial contributorship requires meeting the four ICMJE criteria above\nAny other invited individuals (e.g., domain team) would opt into acknowledgment OR opt into consortial contributorship; the latter requires meeting the four ICMJE criteria above.\nThe publication committee then informs the author of any responses to consortial-level contributions or changes to default acknowledgments.\n\nThe publications committee helps to maintain the list of core contributors, and progress is underway to automatically track contributions directly within the N3C Data Enclave, which aids this process.\nThe publication committee also maintains the content on https://covid.cd2h.org/acknowledgements to be included in the acknowledgments section of each manuscript. An abridged version of the acknowledgments is available for use in abstracts, posters, and slides."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-drr",
    "href": "chapters/publishing.html#sec-publishing-drr",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.2 Result Download (aka export) Request Process",
    "text": "10.2 Result Download (aka export) Request Process\n\n10.2.1 Policy summary\nIn compliance with the NCATS Data Transfer and Data Use Agreements that all N3C Data Enclave users agree to, under no circumstances are tables, figures, parameter estimates, or aggregated statistics to leave the Enclave until download (aka export) approval is obtained by the ResultDownload Committee. Prior approval is required for export regardless of the result format (e.g., tables, figures) and regardless of the target venue (manuscripts, posters, presentations, supplementary material, or even your own private hard drive). Prior approval is also required regardless of the export mechanism (screenshot, copy/paste, or download).\n\n\n10.2.2 Constraints with aggregated data pertaining to small groups of patients (aka small ‘cell sizes’)\nTo protect patient’s data confidentiality, aggregations pertaining to fewer than 20 persons, unless the value is zero, must be identified with a symbol (best represented as &lt; 20 with a corresponding statement that group size below 20 has been obscured in accordance with the N3C Data Use Agreement). Users may apply for an appeal to reduce the reportable threshold down to 10 (via the Enclave-external help desk, see Chapter 11). A strong scientific rationale for this must be provided. NCATS makes the final decision about whether to approve the appeal. In no cases may group sizes below 10 be considered. Authors must include an asterisk or footnote in their manuscript to indicate that the values as low as 10 are reported with approval from the N3C Download Committee.\nExample: Consider the following fictional table, which reports several aggregations of fewer than 20 patients in the ‘other’ gender category, race (AI/AN, black), and in comorbidities (Dementia, HIV). In their original form (left) these would violate the policy, as there are several values reported below 20. Even if these values are simply replaced with &lt; 20 (middle), the masking is still insufficient, because marginal totals can be used to calculate some of their contents. A fully-masked example is shown on the right, modifying the reported values to satisfy the policy constraints, and including a note of the changes made.\n\n\n\nFigure 10.1: An example of summary results that require masking prior to export for publication. To comply with N3C policy, counts below 20 are displayed as &lt; 20, and in this case, additional values must be skewed by up to 5 in order to render it impossible to back-calculate precise counts fewer than 20 in the ‘other gender’ and HIV categories.\n\n\n\n\n10.2.3 Constraints related to Native Populations\nNCATS has engaged in extensive discussion with the NIH Tribal Health Research Office and developed data use plans based on Tribal input following formal Tribal Consultations. As of September 2022, data from individuals who self-identify as American Indian (AI) or Alaska Native (AN) are indicated in race and ethnicity distribution.\n\nAll zip codes of regions representing rural populations of 20,000 persons or less are fully obscured to ‘00000’\nAll zip codes of regions of more than 20,000 persons that overlap with Tribal communities or where the majority of residents identify as AI/AN are identified by the first 3 digits. For example, if a ZIP code of “01234” represents a predominantly AI/AN community, the user will see only a partial ZIP code of “012”.\n\nThe N3C Data User Code of Conduct was revised to clarify that assumptions about Tribal affiliation are not valid or appropriate. For more information, consult NCATS Tribal Consultations.\n\n\n10.2.4 Constraints related to Data Partner IDs\nFor any exports that include Data Partner IDs, these IDs must be masked before being published and a statement that these have been masked must also be included. You may consider assigning random labels to data partners, such as four-digit codes which do not occur in the partner IDs."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-tech",
    "href": "chapters/publishing.html#sec-publishing-tech",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.3 Technical Considerations",
    "text": "10.3 Technical Considerations\nIn addition to the processes and policies described above, there are technical requirements for exporting summary data and figures from the N3C Data Enclave, and features you should be aware of as your project nears publication.\n\n10.3.1 Pinning to a Release\nAll datasets in the Enclave are capable of being versioned with branches, meaning that previous versions of a dataset remain available under a different branch name. N3C uses this mechanism to provide researchers access to the most recent version of the data via the ‘master’ branch, which is selected by default when importing datasets to Code Workbooks or Code Repositories. For example, consider the following Code Workbook, into which we’ve imported the Synthea notional condition_era table.\n\n\n\nFigure 10.2: Selecting the branch for a dataset imported into a code workbook. Branches allow researchers to ensure that the same data is used for further analysis.\n\n\nClicking on the “Branch” tab of the dataset node shows that the master branch is currently being used in this workbook. Instead of master, we could use the dropdown to select another branch–on real N3C data, these branches will be named like Release-v98-2022-10-27. (We call them releases because new versions are ‘released’ periodically after quality checks and harmonization.) Selecting such a branch will import the release of that table as of the date selected, effectively ‘pinning’ the dataset to a point in time. The master branch is configured to always match the latest release, and thus changes over time.\nWhen working with a Code Repository, the branch can be selected as a parameter to the Input entry in the @transform decorator, for example:\nsource_df = Input(\"/UNITE/LDS Release/datasets/condition_era\", branch = \"Release-v98-2022-10-27\")\n\n\n\n\n\n\nNote\n\n\n\nAfter changing the branch of an imported dataset, you will need to re-compute downstream results for their output to reflect the new input.\nAdditionally, all primary N3C tables are versioned with similar release names; if you are using both the condition_era and drug_exposure tables, you should select the same branch for each. It is not recommended to mix data from different releases (or a pinned release with the master branch), because they may contain incompatible data–each N3C release is designed to be a self-contained set of datasets.\nIn fact, because downstream results are only updated when they are explicitly run, you may wish to avoid using the master branch altogether, or run the risk of one set of different sets of results being based on different releases, depending on when they were run.\n\n\nWhy is pinning to a release helpful? Because the default master branch is continuously being updated, analysis results based on it will change over time along with the underlying data (if they are re-run). This becomes cumbersome when writing about results!\n\n\n10.3.2 Download Request Process\nAll research results derived from N3C data–including summary tables, figures, and logs–must be reviewed to ensure they don’t inadvertently leak any patient-level data. The review will check for the policy requirements described above (such as cell sizes less than 20 being masked), and when approved you will be able to download tabular data as a comma-separated-values (CSV) file, images as PDF or PNG, and logs as plain text.\nThe submission and export process is described in detail in the How to download results outside the Enclave training module , but we’ll provide a brief overview here.\n\n10.3.2.1 Submitting a Download Request\nThe download request dashboard may be accessed from the Enclave homepage under “Download Dashboard.” This interface shows your previous requests, and you can submit a new one via the “Submit a new Download Request” button.\n\n\n\nFigure 10.3: The download request dashboard, where researchers can submit requests to export summary data and figures and see the status of those requests.\n\n\nWhen submitting a new download request, you will first be asked to complete a short quiz to ensure you understand the requirements. Subsequently, you will be prompted to enter information about the request, and select one or more “Resources” that you wish to export. A single download request may include multiple resources, such as multiple summary tables saved as datasets. To export figures you should include them in a Report, which can be done from either the Code Workbook or Contour interfaces. Desired log information (such as textual results from statistical tests) should also be copied and pasted into the report so that it can be reviewed.\n\n\n\nFigure 10.4: The download review request form.\n\n\nIf you need the review request to be expedited, you have the option of entering a Need By Date and providing a justification.\n\n\n10.3.2.2 Downloading an Approved Request\nWhen a request is approved, behind the scenes a read-only copy of the materials you requested is created in a location that you will have access to download from. This is to prevent modification of export materials post-review.\nWhen your request is approved, you will see it listed as Approved in the Download Dashboard (see above). Each approved request is provided with a DRR ID (which stands for Data Review Request), for example “DRR-E5C4B6C” shown above. If we scroll down further in the specific request, we can use the “View all…” link to see the individual approved “Resource(s) To Download”:\n\n\n\nFigure 10.5: A view of exportable datasets and a report from an approved download request.\n\n\nTo export one of the datasets as CSV, we can open it by clicking on it, and in the resulting new tab select “Download as CSV” from the “Actions” menu - this will prompt you to enter the corresponding DRR ID, after which the file will be downloaded to your local computer.\nExporting figures and logs stored in Reports can be done in multiple ways. After opening the approved report from the list of resources to download, you can select either “Export to PDF” or “Export to PowerPoint” from the “Actions” menu. Again you will be prompted to enter the DRR ID before the file download begins. A common issue with “Download as PDF” is that figures produced in Code Workbooks may be lower resolution than expected.\n\n\n\n10.3.3 Exporting Code to a Git Repo\nWhile code written inside the Enclave is generally not executable in other contexts (a result of the proprietary execution environment), exporting code for review and publication can be an important part of computational science. Fortunately, both Code Workbooks and Code Repositories support cloning as git repositories, which can then be mirrored to public git hosting like GitHub or BitBucket. This publication step is completely optional and may not even be applicable to all work.\n\n\n\n\n\n\nNote\n\n\n\nGit repositories cloned from the Enclave are read-only; changes made to the locally cloned copy cannot be pushed back to the Enclave. You can however pull new changes made in the Enclave, see below.\nUnlike summary tables, figures, or other results derived from N3C data, N3C does not require review of code prior to export. However, you should be careful to ensure that the code does not include any sensitive information added by hand. The most common use case for such hard-coded data is data_partner_id values added to filter a data partner’s data. These should be removed or masked to comply with N3C policy.\n\n\nCode Workbook code can be exported via the “Gear” icon in the interface, under “Export git repository.” Selecting this item will open the workbook in a new browser tab, with a new “Export Code Workbook” panel on the right, providing the git command to use to clone the repo to your local computer. As described in the panel text, the link contains an authentication token that you should keep private.\n\n\n\nFigure 10.6: Code workbooks can be exported as Git repositories for external publishing. Researchers must ensure that no identifying information, including pseudonymous identifiers like data_partner_id values, are present in the code prior to export.\n\n\nThe resulting git repo will contain three files, pipeline.R, pipeline.py, and pipeline.sql, containing all of the workbooks’ R, Python, and SQL transform code respectively. These files are not very reader-friendly, and as of this writing, N3C is working on parsing tools to help researchers publish their work in an accessible manner.\n\n10.3.3.1 Pulling Changes, Pushing to GitHub, Branches, and Code Repositories\nWhile it is not possible to push changes made locally up to the Enclave, it is possible to pull updates made in the Enclave with a simple git pull. However, updates are not pulled unless some action is taken within the workbook to commit them prior to pulling. Simply editing the code for a transform is not enough, but executing a transform or adding a new transform will commit the current state of the workbook so that it can be pulled.\nOnce you’ve pulled the latest version of your workbook locally, you likely will want to push a copy up to a public repository such as GitHub. The recommended way to do this is to create a new empty repository in GitHub (or your git hosting service of choice) with the same name as the workbook, and set it up as a push-only remote repository. For example, for a workbook named example_workbook, one would create a new GitHub repository named example_workbook, and in the local copy run:\ngit remote set-url --add --push origin https://github.com/&lt;username&gt;/example_workbook.git\nA subsequent git push will sync the state of the remote repository. You will of course need the appropriate permissions to push a repository to GitHub, via a Personal Access Token or some other means.\nIf you have created one or more branches in your code workbook, these are treated as regular git branches and so can be synced locally with git fetch and git switch &lt;branch-name&gt;. A subsequent git push will push the new branch to GitHub as well.\nCode Repositories work much the same way. The primary differences are that\n\ncommits to a repo in the Enclave are made manually, rather than automatically when a transform is run or created, and\nthe Clone button provides only the git URL with the embedded token, you will need to use this URL in combination with git clone or other git commands as usual. Code Repos similarly support branching.\n\n\n\n\nFigure 10.7: Code workbooks can also be exported as Git repositories for external publishing. Researchers must ensure that no identifying information, including pseudonymous identifiers like data_partner_id values, are present in the code prior to export.\n\n\n\n\n10.3.3.2 Python Libraries and Code Repositories\nCode Repositories are less frequently submitted to the Knowledge Store, but they provide a feature for sharing code that Code Workbooks don’t: it is possible to author a Python library in a Code Repository that can then be imported for use in either another Code Repository or a Code Workbook by any N3C researcher. One such example is the Semantic Similarity Python Library . Authoring Python libraries is different from authoring Code Repositories that transform data, and is covered in the official documentation, as is utilizing such libraries in Code Repositories. Using them in Code Workbooks requires updating the workbook environment to include the library, just like with any other Python or R library you might like to use.\nIn fact, Python libraries published this way are automatically usable by others in N3C without submission to the Knowledge Store, but you should still submit the repository itself along with documentation as a Knowledge Object for discoverability in N3C. If you run into issues or have questions, be sure to submit an Enclave-internal support ticket or visit office hours.\n\n\n\n10.3.4 Exporting Concept Sets\nAs discussed in Chapter 7, concept sets are sets of OMOP concept_ids representing clinical concepts. These are organized and curated as shared resources in N3C, see the section on the Concept Set Browser for details.\nWhen publishing work utilizing a concept set, you will likely want to download the concept set in a tabular format for inclusion in supplemental materials. Fortunately, the concept set browser supports this functionality–when viewing a specific version of a concept set, the “OMOP Concepts” tab lists the individual concepts and provides an “Export list of Concepts as Excel.” The provided Excel export includes the concept IDs and other relevant information.\n\n\n\nFigure 10.8: The N3C Concept Set Browser supports exporting a concept set version as an Excel spreadsheet for inclusions in supplementary materials. Concept sets are referenceable via DOI (see text).\n\n\nSome concept sets, notably those properly reviewed and marked N3C Recommended, are already published externally and referenceable via a DOI at the N3C Zenodo community."
  },
  {
    "objectID": "chapters/publishing.html#sec-publishing-ks",
    "href": "chapters/publishing.html#sec-publishing-ks",
    "title": "10  Publishing and Sharing Your Work",
    "section": "10.4 Submitting to the Knowledge Store",
    "text": "10.4 Submitting to the Knowledge Store\nFor security reasons the permissions around project workspaces are very tight; it is not possible for a researcher to share data, code, or other resources with others outside of the project workspace. While necessary, this prevents code sharing and re-use amongst the many analysts working in the N3C Data Enclave. The N3C Knowledge Store provides an authorized mechanism for researchers to share code, and datasets derived via code, with other researchers (provided they also have access to the correct input datasets in the case of shared datasets). Because the Knowledge Store is internal to the Enclave and protected to those who have the authorization to view row-level data, the publication is not involved in monitoring or approving the contents.\nIf you’ve written an analysis pipeline that would be of benefit to others, you should consider submitting it to the Knowledge Store for others to find and use. Details on this process are covered in the Knowledge Store Guide  training module, but we’ll give an overview here. Note that the process can be cumbersome and sometimes requires administrator help–be sure to submit an Enclave-internal support ticket or visit office hours if you run into any issues or have questions.\nCode sharing within N3C is usually accomplished by sharing Code Workbook “templates” in the knowledge store, which provide a set of workbook nodes that can be imported into a workbook and configured via parameters for the end user. Authoring workbook templates is covered in the official documentation. Templates are typically accompanied by a README (as a Report) with documentation, an example workbook using the template on N3C data, and potentially the derived dataset created by the example workbook. We suggest placing these resources in a single folder prior to submitting them to the Knowledge Store.\nTo submit your new “knowledge object” to the store, first find the “Publish New Knowledge Object” item under the … menu in the upper right of the Knowledge Store:\n\n\n\nFigure 10.9: The N3C Knowledge Store is a repository of community-developed code and derived datasets for use by other researchers with sufficient access.\n\n\nThis will open a form where you are able to select your knowledge object (you should select the folder containing your template and other materials) and add details such as a title and description.\n\n\n\nFigure 10.10: Knowledge Store contribution form.\n\n\nOnce the form is completed the submission will be sent to a queue for system administrators to finalize the creation of the knowledge object (which requires creating read-only copies in a place where other researchers can access them) and listing them in the store. Depending on the complexity and goals of your template, administrators may help create multiple versions of your example workbook with pre-configured access to different levels of N3C data and set up a schedule so that they produce new derived datasets whenever a new release of N3C data is available. This allows other researchers to use the computed datasets directly, or import the template for customization.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page."
  },
  {
    "objectID": "chapters/support.html#sec-support-internal",
    "href": "chapters/support.html#sec-support-internal",
    "title": "11  Help and Support",
    "section": "11.1 Support Tickets: Enclave-Internal",
    "text": "11.1 Support Tickets: Enclave-Internal\nWhile live-support options are available, submitting questions via “tickets” (also known as “issues” in the Enclave) helps ensure they reach the right person and that questions are logged and tracked. Given the sensitive nature of N3C data, questions that pertain to patients or data partners should be asked within the Enclave itself.\nThe within-Enclave support ticket system is also a good avenue for technical questions, including about platform features, performance, permissions, and tooling. In fact, when submitting a ticket in the Enclave, the ticket itself will automatically track the resource being viewed when the ticket is submitted.\nTo illustrate an example, we first navigate to the Synthea Notional Data entry in the Data Catalog (under “Projects & files” in the left navigation menu).\n\n\n\nFigure 11.1: Navigating to Synthea.\n\n\nNext, we’ll open the condition_era table which displays a preview in the Dataset Preview application. Let’s suppose we have a question about this data, or perhaps have discovered a potential data quality issue.\n\n\n\nFigure 11.2: Synthea Preview.\n\n\nTo submit a ticket about the currently opened dataset, we’ll open the Help menu near the top, and select “Report Issue.”\n\n\n\nFigure 11.3: New Issue.\n\n\nThis opens a dialog requisition information about the ticket. Notice that the RESOURCE is identified as the condition_era table we had opened. Since we are asking a question about the data, we’ll select “Data quality.”\n\n\n\nFigure 11.4: New Issue: What kind of help do you need?\n\n\nOnce we click Next, we’ll be prompted to change the resource of interest or application being used (if desired). Since we are reporting an issue on a dataset, we even have the option of selecting the specific column we are interested in. We’ll just click Next here.\n\n\n\nFigure 11.5: Share some details.\n\n\n\n\n\n\n\n\nReporting Issues via Help Center\n\n\n\nUsing “Report Issue” from the Help menu of an Enclave application is the preferred way to submit a ticket, as this option keeps the best track of the resource being reported from. While most Enclave applications have a Help menu near the top left, not all do. In these cases you can alternatively submit an issue by finding the “Help & support” option in the lower part of the left navigation bar and choosing the “Help Center”. This will open a sidebar to the right, with a large blue button at the bottom for “Report an Issue.”\n\n\nFinally, we are prompted to submit our issue, including a title and description with pre-filled questions depending on the issue type selected. Answering all of these is not required, but any information you can add that speaks to them is helpful. This section also allows you to upload a screenshot if desired. Even though these issues are protected in Enclave, you should not screenshot any data (or results like summary tables or figures), as that would result in your local computer storing, even if temporarily, unapproved patient-level information. Nevertheless, when excluding patient data is possible, a screenshot may help diagnose the problem, and the support personnel who respond to the issue may request a screenshot during follow-up.\n\n\n\nFigure 11.6: More details.\n\n\nWe can scroll down in this panel to see more advanced information pertaining to the ticket. Priority should generally be left to “Medium,” since “High” priority is used to alert infrastructure support of system-wide issues or outages likely to affect a majority of users. The default assignee is the “N3C: Issues Triage Team,” who will further route the ticket to the appropriate support group (issues are triaged most business days, but follow-up from support may take longer). Followers allow you to specify other users who will receive alerts about this issue. Adding labels to the ticket is optional as well, since the triage team usually applies relevant labels for tracking purposes.\n\n\n\nFigure 11.7: Submit.\n\n\nOnce we click Submit and refresh the browser page, we’ll see that a new “warning” icon has been added to the interface indicating that the resource now has one or more open issues relating to it, and it can be clicked on to open a menu with details. This warning will also show for other users who open the resource, and it will show in the file browser for this dataset. Reporting issues about datasets from the datasets themselves is thus a mechanism for alerting support teams and other N3C researchers about potential data quality issues. The same principle applies to other resource types like Code Workbooks, in cases where multiple researchers are working with them.\n\n\n\nFigure 11.8: Recent Issues.\n\n\n\n11.1.1 Issue followup\nAfter your ticket is submitted, it will be routed to a triage team who will decide which support group is best able to address it. These include groups with admin-level access and general knowledge of the Enclave (at least one of which is familiar N3C-specific tools or workflows), experts in N3C data ingestion and harmonization processes, as well as individuals with expertise in OMOP and other N3C technologies.\nWhen activity occurs on your ticket you will see a small orange dot appear on the Notifications navigation menu item indicating you have a new notification, clicking this will show this notification (and any others you may have received). By default, you will also receive email notifications for ticket activity. This can be configured under your account preferences (the Account item in the left navigation menu). Finally, you can review and respond to tickets via the Issues application (which may be hidden for you in the left navigation bar under “View all apps”)."
  },
  {
    "objectID": "chapters/support.html#sec-support-external",
    "href": "chapters/support.html#sec-support-external",
    "title": "11  Help and Support",
    "section": "11.2 Support Tickets: Enclave-External",
    "text": "11.2 Support Tickets: Enclave-External\nWhile the Enclave-internal ticket system is a good avenue for more technical questions about data analysis or the data itself, most other questions should be directed to an Enclave-external ticket system (sometimes called the “support desk”). At the very least, if your issue is you cannot login to the Enclave, that cannot be reported via the Enclave-internal ticketing system!\n\n\n\nFigure 11.9: Starting a ticket.\n\n\nThe external help desk can be found at https://covid.cd2h.org/support. Here you will find a link to “Submit a Support Request” that directs you to select the kind of support you need.\n\n\n\nFigure 11.10: Ticket details.\n\n\nEach of the options is described, and range from Enclave access support (commonly used for login issues), Domain Team creation or support, questions about Data Use Requests or the Data Access Committee (commonly used to check on DUR review status), PPRL data, and “everything else.” In general, this help desk is staffed by a broader range of core N3C administrators, and so is generally the best option outside of technical or data questions.\nAfter selecting a support area, you will be given the option to select sub-categorizations, enter a description of the issue or question, provide a summary title for tracking and select the user (usually you) submitting the request. The list of users is pre-populated based on N3C data, but you can also type an email address in the same field.\nOnce submitted, you will receive an email with a link to the ticket. You can use this link to make further comments, or do so by replying to the email directly."
  },
  {
    "objectID": "chapters/support.html#sec-support-office",
    "href": "chapters/support.html#sec-support-office",
    "title": "11  Help and Support",
    "section": "11.3 Office Hours",
    "text": "11.3 Office Hours\nN3C hosts office hours on Tuesdays and Thursdays of most weeks, at 10a PT/1p ET. The join link can be found at https://covid.cd2h.org/support. All are welcome to join, from experienced N3C analysts looking for help with complex machine learning implementations, to brand new researchers needing help finding their project workspace. Experienced N3C volunteers are on hand and able to help with most questions. They can also refer you to external resources, or suggest submitting a ticket when appropriate. Some researchers join just to watch and learn. To satisfy N3C data privacy rules, N3C staff utilize Zoom breakout rooms allowing a researcher to share their screen only with others who have the same level of access."
  },
  {
    "objectID": "chapters/support.html#sec-support-training",
    "href": "chapters/support.html#sec-support-training",
    "title": "11  Help and Support",
    "section": "11.4 Training Resources",
    "text": "11.4 Training Resources\nMany training and educational resources are available within the Enclave where we can readily organize and link them to relevant resources. The “Training Material” button on the Enclave homepage displays several categories of training materials:\n\n\n\nFigure 11.11: Training Material.\n\n\nWhile the documentation and self-guided tours provide information about the cloud-based Enclave platform, they don’t provide any information specific to N3C. The Training Portal is the primary location for N3C-related training materials, while N3C Community Notes allow researchers to post short articles/guides for others to use. The Support option will redirect to a page linking to the two ticket systems described above.\n\n11.4.1 Training (Training Portal)\nThe N3C Training Portal hosts training “modules.” The list of training modules is roughly sorted by researchers’ N3C journey–those new to N3C will likely find the first modules of most interest, while those preparing to publish their results should scroll to the end.\nModules are searchable by keyword (from their title and description), and a brief list of Suggested Modules can be found in the orange button in the upper-right, though browsing through the full list is recommended.\n\n\n\nFigure 11.12: Training Modules.\n\n\nThe Training Portal also has a Paths View, which shows potential learning paths of interest. These links are not formally assigned, and act more like a recommendation system to help navigate and find modules and resources of interest. This interface is limited in the number of items it can display, so you may want to filter using the “Starting Module Category” dropdown.\n\n\n\nFigure 11.13: Learning Paths.\n\n\nOpening a module from the main list view reveals an overview of the module, including title, description, topics, learning objectives, suggested background, and estimated time to complete. Immediately below the title is a link whose URL points to this specific module in the portal for sharing.\n\n\n\nFigure 11.14: Overview of a Training Module.\n\n\nTo the right is a list of resources comprising the materials of the module; these may be videos or documents, example Enclave resources like code workbooks, or in some cases links to relevant external resources. The small search box allows you to filter the list, and is especially useful for modules with many resources such as our Enclave Users’ Group series discussed below.\nN3C community members are welcome to suggest or develop new training modules for inclusion in the portal. Several have been developed this way, and each module tracks authorship information. To contribute to the training portal or other N3C-related education and training efforts, just contact the Education & Training Domain Team at https://covid.cd2h.org/ET-DT.\n\n\n11.4.2 Self Guided Tours (Academy)\nThis platform feature provides step-by-step walkthroughs of individual tools like Contour and Code Workbooks. The Foundry 10X and 20X series are recommended and cover the basic tools researchers will encounter. Along the right individual steps walk you through an example workflow or analysis. Note that because these tours are not developed by N3C, the example analyses and data will not be N3C-relevant. You may also be prompted to create files or work in a “home folder” (which N3C has disabled) or a project workspace you don’t have write permissions to. Instead, you can utilize the N3C Training Area (see below).\n\n\n\nFigure 11.15: Academy walk-through.\n\n\n\n\n11.4.3 N3C Community Notes\nN3C Community Notes is a within-Enclave application where researchers can author and share short articles, code snippets, or FAQ items. The application supports a rich tagging system, and notes can be linked to other N3C resources like training modules, knowledge objects, and concept sets. The note overview contains a link whose URL points to this specific note in the application for sharing.\n\n\n\nFigure 11.16: N3C Community Notes.\n\n\n\n\n11.4.4 Documentation\nThe official platform documentation is a rich resource for details on applications, and includes many guides and how-tos. If you don’t desire to read all of the documentation in detail, you should at least skim sections relevant to applications you use. The search function can find articles relevant to specific application features or techniques.\n\n\n\nFigure 11.17: Foundry Documentation.\n\n\n\n\n11.4.5 Having Trouble? (Support)\nThis last entry in the Training Resources page simply redirects to a page describing, and linking to, the two ticket systems described earlier in this chapter."
  },
  {
    "objectID": "chapters/support.html#sec-support-area",
    "href": "chapters/support.html#sec-support-area",
    "title": "11  Help and Support",
    "section": "11.5 N3C Training Area",
    "text": "11.5 N3C Training Area\nThe N3C Training Area is a project workspace where all N3C users can practice and learn using notional datasets (described below). This workspace is also used to organize other training resources (like the Training Portal).\n\n\n\nFigure 11.18: Training Area.\n\n\nIf you wish to create a practice folder, you are free to do so inside the “Practice Area - Public and Example Data.” Simply open it up, and using the green +New button create a new subfolder with a unique name (many use shortened usernames, e.g. “oneils”). Within this folder you will be able to create new analyses, and these will have access to the notional datasets described next."
  },
  {
    "objectID": "chapters/support.html#sec-support-notional",
    "href": "chapters/support.html#sec-support-notional",
    "title": "11  Help and Support",
    "section": "11.6 Notional Datasets",
    "text": "11.6 Notional Datasets\nOMOP-formatted N3C patient data are protected by a Data Use Request process, but researchers may wish to explore OMOP tables and Enclave tools prior to completing a DUR. The N3C Training Area is the place to do such practice, and N3C provides two notional (i.e. fake) datasets formatted similarly to the Level 2 and Level 3 data that do not require a DUR to access. They are both available via the data catalog under “Synpuf Synthetic Data” and “Synthea Notional Data”.1 The data they contain differ in some important ways, described next.\n\n\n\nFigure 11.19: Two sets of synthetic data.\n\n\n\n11.6.1 SynPuf Synthetic Data\nSynPuf is short for “Synthetic Public Use Files,” or EHR records that have been scrubbed of personally identifiable information and released for public educational use. These SynPuf files originate from SynPuf Medicare Claims data and have been converted to OMOP format by the OHDSI community. The content of these data differs from N3C data in many ways (e.g. records prior to Jan. 1, 2018 are included), and they represent a distinctive population of Medicare-eligible patients. Lastly, the data are not recent, and so contain no COVID-19-related records such as diagnoses, lab tests, or vaccine records. The SynPuf data do not contain some N3C customizations to the OMOP data model, for example the manifest table used in N3C data to describe metadata about contributing data partners.\nCompared to the Synthea data however, SynPuf data better represent real EHR data, including the potential for data entry errors, diversity in medical codes used, and missing data. We thus recommend that researchers interested in trying statistical or machine learning models (or other applications better suited for realistic data) use the SynPuf notional data.\n\n\n11.6.2 Synthea Notional Data\nIn contrast to the SynPuf data, the Synthea notional data are derived from a probabilistic model of early-pandemic COVID-19 patient trajectories published by Walonoski et al. (2020) converted to OMOP. These data include COVID-19 diagnoses and lab tests for a subset of patients. The main limitation of this notional data is its model-generated cleanliness. Pneumonia in the Synthea dataset, for example, is always represented with the same concept ID, while in real data a variety of pneumonia sub-type concept IDs are represented. Real EHR data also contain missing, erroneous, or inconsistent information. With regard to COVID-19, N3C has modified the original data published by Walonoski et al. (2020) to include more diversity and realism in COVID-19 diagnoses and lab tests; a README file in the data catalog describes the modifications in detail.\nThe Synthea data have an additional benefit of being slightly more aligned with real N3C data for additions beyond the OMOP standard. For example, while SynPuf data tables include data partner IDs, Synthea also includes a manifest table with mock data partner metadata. The Synthea data also include constructed macrovisit information."
  },
  {
    "objectID": "chapters/support.html#sec-support-ohdsi",
    "href": "chapters/support.html#sec-support-ohdsi",
    "title": "11  Help and Support",
    "section": "11.7 OHDSI Resources",
    "text": "11.7 OHDSI Resources\nN3C relies heavily on the OMOP common data model, developed by an international group of researchers comprising the Observational Health Data Sciences and Informatics consortium, or OHDSI. OHDSI provides a wealth of training and support resources, the most significant of which are the Book of OHDSI (the inspiration for this book), EHDEN Academy (online video-based courses and lectures), and the OHDSI forums. These cover basic and advanced usage of OMOP data as well as techniques and good practices for working with observational EHR data.\n\n\n\nFigure 11.20: The Book of OHDSI is a great starting place for learning OMOP."
  },
  {
    "objectID": "chapters/support.html#sec-support-community",
    "href": "chapters/support.html#sec-support-community",
    "title": "11  Help and Support",
    "section": "11.8 Community Resources",
    "text": "11.8 Community Resources\nIn addition to Community Notes mentioned above, several venues are available to get help and support from the broad community. N3C researchers include statisticians and data scientists of all stripes, clinicians, and even industry and government representatives. More than a few new collaborations have resulted from peer-to-peer support in N3C!\n\n11.8.1 Enclave Users’ Group\nThe Enclave Users Group (EUG) is a community-focused forum where analysts can share practical information on techniques, tips, and methods in the N3C Data Enclave. Each session one or more presenters share a topic, emphasizing live Q&A, discussions, and meeting new people. Topics range from statistical techniques like propensity score matching, scaling machine learning algorithms for use on billion-row datasets, tips for scientific software development, and introductions of new N3C resources and initiatives. EUG sessions do not present protected data, so sessions are recorded and example resources are available in the N3C Training Area. For more information and an index of recorded sessions see the Enclave Users’ Group module  in the Training Portal.\n\n\n\nFigure 11.21: Enclave Users’ Group.\n\n\n\n\n11.8.2 Slack\nSlack is commonly used for team communication in N3C, and several widely-subscribed channels are great support resources. These include #n3c-analytics where researchers ask general questions about methods or data (with 390+ members), #n3c-training where training-related announcements are posted, and a variety of topic-focused channels such as #n3c-ml for machine learning. N3C uses the Slack organization of the National Center for Data To Health at https://cd2h.slack.com. Access however is managed via the N3C onboarding process, where Slack-preferred emails are collected.\n\n\n11.8.3 Domain Teams\nDomain Teams, covered in more detail in other parts of this book, are excellent support and training resources for their members. Not only can Domain Teams answer common questions of new N3C researchers, they can answer questions that pertain to their area of expertise. The pregnancy domain team, for example, is the best source of knowledge for locating pregnancy-related records in EHR data.2"
  },
  {
    "objectID": "chapters/support.html#sec-support-liaisons",
    "href": "chapters/support.html#sec-support-liaisons",
    "title": "11  Help and Support",
    "section": "11.9 Data and Logic Liaisons",
    "text": "11.9 Data and Logic Liaisons\nN3C Logic and Data Liaisons are teams contributing to the N3C mission through software development and user support, prioritizing the needs of Domain Teams and their members. In order to perform research, users need to identify key variables for analysis. These key variables are generated through Code Workbooks and Templates that utilize specific Concept Sets (lists of key variables from constituent vocabularies), that identify and extract data to answer research questions. Through interaction with Domain Teams, the Data and Logic Liaisons continually develop and refine a core set of N3C Recommended concept sets and code templates that generate commonly used variables and support efficient customization by research teams.\nYou may have noticed the data & logic liaisons have appeared several times in this book, including:\n\nFact Tables and Templates (Section 8.3.3),\nCurated Concept Sets (Section 8.2),\nPublished Concepts Sets (Section 7.3.3.1), and\nData Quality (Pfaff et al., 2022).\n\nThey also provide support services as described below.\n\n11.9.1 Data Liaison Services\nEHR data are complex, more so when they cover data contributed by 75+ sites. The Data Liaisons group consists of those most familiar with N3C data, including members of the phenotype and ingestion and harmonization teams. Data Liaisons are subject matter experts in biomedical, translational, clinical data standards, and Real-World data utilization to support program investigator analyses. Data Liaisons curate and review N3C-recommended concept sets for researcher use, and can field data-related questions, which should be submitted via the Enclave-internal ticket system. Potential data quality issues should also be submitted via Enclave-internal ticket system for routing to the Data Liaisons for review.\nFor basic questions about the OMOP common data model, refer to the OHDSI resources, and training portal modules for getting started with OMOP. Personalized assistance is provided during N3C Office Hours. Support for Concept Set consultation can be received by submitting a help desk technical support ticket in the N3C Enclave. The Data Liaisons team will send a representative to your domain team meetings on an as-needed basis for general consultation.\n\n\n11.9.2 Logic Liaison Services\nLogic Liaisons consist of analysts with significant technical expertise for research with N3C data. Although they do not develop project-specific research code as a service, they do create Knowledge Objects such as reusable code templates and convenient derived datasets. Logic Liaison members provide technical support during office hours, and many are active in the #n3c-analytics Slack channel.\nLogic Liaisons support N3C researchers who are learning to use and adapt the Logic Liaison code fact tables and templates. They also help researchers assess the feasibility of the project design with regard to data availability and data limitations. This team helps researchers assess and clean their project-specific fact tables using Logic Liaison Data Quality templates, which help research teams decide which sites to include in the analysis.\nLogic Liaison Code Fact Tables and Templates can be accessed by searching the Knowledge Store for “Logic Liaison Template”. Recorded trainings are provided in the “Logic Liaison Templates” module of the N3C Training Portal. Personalized help is provided during N3C Office Hours. Support for issues and errors encountered when using a Logic Liaison Template can be received by submitting a technical support ticket in the Enclave. Team members are also active in the #n3c-analytics Slack channel. The Logic Liaison team will send a representative to your domain team meetings on an as-needed basis for general consultation.\n\n\n\n\n\n\nAdditional Chapter Details\n\n\n\nThis chapter was first published May 2023. If you have suggested modifications or additions, please see How to Contribute on the book’s initial page.\n\n\n\n\n\n\nPfaff, E. R., Girvin, A. T., Gabriel, D. L., Kostka, K., Morris, M., Palchuk, M. B., Lehmann, H. P., Amor, B., Bissell, M., Bradwell, K. R., et al. (2022). Synergies between centralized and federated approaches to data quality: A report from the national COVID cohort collaborative. Journal of the American Medical Informatics Association, 29(4), 609–618. https://doi.org/10.1093/jamia/ocab217\n\n\nWalonoski, J., Klaus, S., Granger, E., Hall, D., Gregorowicz, A., Neyarapally, G., Watson, A., & Eastman, J. (2020). Synthea™ novel coronavirus (COVID-19) model and synthetic data set. Intelligence-Based Medicine, 1-2, 100007. https://doi.org/doi.org/10.1016/j.ibmed.2020.100007"
  },
  {
    "objectID": "chapters/support.html#footnotes",
    "href": "chapters/support.html#footnotes",
    "title": "11  Help and Support",
    "section": "",
    "text": "Note that these should not be confused with the Level 1 Synthetic Data, which are derived from N3C patient data and protected by a Data Use Request.↩︎\nThis is not as trivial as it sounds!↩︎"
  },
  {
    "objectID": "chapters/ml.html",
    "href": "chapters/ml.html",
    "title": "12  Machine Learning",
    "section": "",
    "text": "Chapter Leads: Peter Robinson, Justin Reese, Blessy Antony, T.M. Murali, Elena Casiraghi, Hannah Blau\n\n\n\n\n\n\nNote\n\n\n\nThis chapter is being drafted in Google Docs at https://drive.google.com/drive/u/0/folders/1HZ3IGv17zUl9t8RxZSl4uOq_FRzrgTp_\nSee a draft of the chapter outline at https://docs.google.com/document/d/1ttUKgwVcIZHM87elrlUNV6Qi9thzOwKBg8GegKObEtg/\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAt this point, any edits to this chapter should be made in Google Docs. The current Markdown is for testing only. It is NOT the source of truth (yet)."
  },
  {
    "objectID": "chapters/funding.html",
    "href": "chapters/funding.html",
    "title": "13  Funding and Institutional Support",
    "section": "",
    "text": "A. Jerrod Anzalone: University of Nebraska Medical Center, Department of Neurological Sciences.  Great Plains IDeA-CTR, NIH/NIGMS 5 U54 GM115458.\nWilliam H. Beasley: University of Oklahoma Health Science Center, Biomedical and Behavioral Methodology Core.  Oklahoma Shared Clinical and Translational Resources, NIH/NIGMS 5 U54 GM104938\nKaren M. Crowley: Brown University, Brown Center for Biomedical Informatics, RI Advance-CTR Biomedical Informatics, Bioinformatics, and Cyberinfrastructure Enhancement Core.  NIH/NIGMS 5 U54 GM115677.\nLisa C. Eskenazi: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science.  NCATS-P00438-B.\nSigfried Gold: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science.  NCATS-P00438-B.\nStephanie S. Hong: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science.  NCATS-P00438-B.\nBryan J Laraway: University of Colorado, Department of Biomedical Informatics, Translational and Integrative Biology Lab.  NCATS U24 TR002306.\nHarold P. Lehmann: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science.  NCATS-P00438-B.\nMary H. Mays: University of Puerto Rico Medical Sciences Campus, Hispanic Alliance for Clinical and Translational Research.  NIH/NIGMS 5 U54 GM133807.\nJulie A McMurry: University of Colorado, Anschutz Medical Campus.  NCATS U24 TR002306, Axel Informatics Subcontract NCATS-P00438-B\nShawn T. O’Neil: University of Colorado, Department of Biomedical Informatics, Translational and Integrative Biology Lab.  NCATS U24 TR002306.\nSharon J. Patrick: West Virginia Clinical and Translational Science Institute, Biostatistics, Epidemiology, and Research Design Core.  NIH/NIGMS 5 U54 GM104942.\nChristine Suver: Sage Bionetworks, Research Governance and Ethics.  NCATS U24 TR002306, Axel Informatics Subcontract NCATS-P00438-B\nKenneth J. Wilkins: National Institutes of Health, NIDDK, Office of the Director, Biostatistics Program / Office of Clinical Research Support.\nXiaohan Tanner Zhang: Johns Hopkins University, School of Medicine, Biomedical Informatics & Data Science.  NCATS-P00438-B."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "14  References",
    "section": "",
    "text": "Anzalone, Alfred Jerrod, Ronald Horswell, Brian M Hendricks, San Chu,\nWilliam B Hillegass, William H Beasley, Jeremy R Harper, et al. 2023.\n“Higher Hospitalization and Mortality Rates Among\nSARS-CoV-2-Infected Persons in Rural America.” The Journal of\nRural Health 39 (1): 39–54. https://doi.org/10.1111/jrh.12689.\n\n\nBradwell, Katie R, Jacob T Wooldridge, Benjamin Amor, Tellen D Bennett,\nAdit Anand, Carolyn Bremer, Yun Jae Yoo, et al. 2022. “Harmonizing\nUnits and Values of Quantitative Data Elements in a Very Large\nNationally Pooled Electronic Health Record (EHR) Dataset.”\nJournal of the American Medical Informatics Association 29 (7):\n1172–82. https://doi.org/10.1093/jamia/ocac054.\n\n\nCharlson, Mary E., Peter Pompei, Kathy L. Ales, and C.Ronald MacKenzie.\n1987. “A New Method of Classifying Prognostic Comorbidity in\nLongitudinal Studies: Development and Validation.” Journal of\nChronic Diseases 40 (5): 373–83. https://doi.org/10.1016/0021-9681(87)90171-8.\n\n\nDong, Xiao, Jianfu Li, Ekin Soysal, Jiang Bian, Scott L DuVall,\nElizabeth Hanchrow, Hongfang Liu, et al. 2020. “COVID-19 TestNorm:\nA Tool to Normalize COVID-19 Testing Names to LOINC Codes.”\nJournal of the American Medical Informatics Association 27 (9):\n1437–42. https://doi.org/10.1093/jamia/ocaa145.\n\n\nGold, Sigfried, Andrea Batch, Robert McClure, Guoqian Jiang, Hadi\nKharrazi, Rishi Saripalle, Vojtech Huser, et al. 2018. “Clinical\nConcept Value Sets and Interoperability in Health Data\nAnalytics.” In AMIA Annual Symposium Proceedings,\n2018:480. American Medical Informatics Association.\n\n\nGold, Sigfried, Harold Lehmann, Lisa Schilling, and Wayne Lutters. 2021.\n“Practices, Norms, and Aspirations Regarding the Construction,\nValidation, and Reuse of Code Sets in the Analysis of Real-World\nData.” medRxiv, 2021–10.\n\n\nHaendel, Melissa A, Christopher G Chute, Tellen D Bennett, David A\nEichmann, Justin Guinney, Warren A Kibbe, Philip R O Payne, et al. 2020.\n“The National COVID Cohort Collaborative\n(N3C): Rationale, design, infrastructure, and deployment.”\nJournal of the American Medical Informatics Association 28 (3):\n427–43. https://doi.org/10.1093/jamia/ocaa196.\n\n\nKharrazi, Hadi, Winnie Chi, Hsien-Yen Chang, Thomas M Richards, Jason M\nGallagher, Susan M Knudson, and Jonathan P Weiner. 2017.\n“Comparing Population-Based Risk-Stratification Model Performance\nUsing Demographic, Diagnosis and Medication Data Extracted from\nOutpatient Electronic Health Records Versus Administrative\nClaims.” Medical Care 55 (8): 789–96. https://doi.org/10.1097/MLR.0000000000000754.\n\n\nMehta, Hemalkumar B., Huijun An, Kathleen M. Andersen, Omar Mansour,\nVithal Madhira, Emaan S. Rashidi, Benjamin Bates, et al. 2021.\n“Use of Hydroxychloroquine, Remdesivir, and Dexamethasone Among\nAdults Hospitalized with Covid-19 in the United States: A Retrospective\nCohort Study.” Annals of Internal Medicine 174 (10):\n1395–1403. https://doi.org/10.7326/M21-0857.\n\n\nOHDSI. 2019. The Book of OHDSI: Observational Health Data Sciences\nand Informatics. United States: OHDSI. https://ohdsi.github.io/TheBookOfOhdsi/.\n\n\nPalantir. 2023. “Documentation: Code Repositories\nOverview.” https://www.palantir.com/docs/foundry/code-repositories/overview/.\n\n\nPfaff, E. R., A. T. Girvin, T. D. Bennett, A. Bhatia, I. M. Brooks, R.\nR. Deer, J. P. Dekermanjian, et al. 2022. “Identifying Who Has\nLong COVID in the USA: A Machine Learning Approach Using N3C\nData.” Lancet Digit Health 4 (7): e532–41. https://doi.org/10.1016/S2589-7500(22)00048-6.\n\n\nPfaff, Emily R, Andrew T Girvin, Davera L Gabriel, Kristin Kostka,\nMichele Morris, Matvey B Palchuk, Harold P Lehmann, et al. 2022.\n“Synergies Between Centralized and Federated Approaches to Data\nQuality: A Report from the National COVID Cohort Collaborative.”\nJournal of the American Medical Informatics Association 29 (4):\n609–18. https://doi.org/10.1093/jamia/ocab217.\n\n\nReese, Justin T, Hannah Blau, Elena Casiraghi, Timothy Bergquist,\nJohanna J Loomba, Tiffany J Callahan, Bryan Laraway, et al. 2023.\n“Generalisable Long COVID Subtypes: Findings from the NIH N3C and\nRECOVER Programmes.” EBioMedicine 87. https://doi.org/10.1016/j.ebiom.2022.104413.\n\n\nRichesson, Rachel L, W Ed Hammond, Meredith Nahm, Douglas Wixted,\nGregory E Simon, Jennifer G Robinson, Alan E Bauck, et al. 2013.\n“Electronic Health Records Based Phenotyping in Next-Generation\nClinical Trials: A Perspective from the NIH Health Care Systems\nCollaboratory.” Journal of the American Medical Informatics\nAssociation 20 (e2): e226–31. https://doi.org/10.1136/amiajnl-2013-001926.\n\n\nSharafeldin, Noha, Benjamin Bates, Qianqian Song, Vithal Madhira, Yao\nYan, Sharlene Dong, Eileen Lee, et al. 2021. “Outcomes of COVID-19\nin Patients with Cancer: Report from the National COVID Cohort\nCollaborative (N3C).” Journal of Clinical Oncology 39\n(20): 2232–46. https://doi.org/10.1200/JCO.21.01074.\n\n\nSun, Jing, Qulu Zheng, Vithal Madhira, Amy L. Olex, Alfred J. Anzalone,\nAmanda Vinson, Jasvinder A. Singh, et al. 2022. “Association\nBetween Immune Dysfunction and COVID-19 Breakthrough Infection After\nSARS-CoV-2 Vaccination in the US.” Archives of Internal\nMedicine (Chicago, Ill. : 1908) 182 (2): 153–62. https://doi.org/10.1001/jamainternmed.2021.7024.\n\n\nWalonoski, Jason, Sybil Klaus, Eldesia Granger, Dylan Hall, Andrew\nGregorowicz, George Neyarapally, Abigail Watson, and Jeff Eastman. 2020.\n“Synthea™ Novel Coronavirus (COVID-19) Model and Synthetic Data\nSet.” Intelligence-Based Medicine 1-2: 100007. https://doi.org/doi.org/10.1016/j.ibmed.2020.100007.\n\n\nWilkinson, Mark D, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3. https://doi.org/10.1038/sdata.2016.18.\n\n\nYang, Xueying, Jing Sun, Rena C Patel, Jiajia Zhang, Siyuan Guo, Qulu\nZheng, Amy L Olex, et al. 2021. “Associations Between HIV\nInfection and Clinical Spectrum of COVID-19: A Population Level Analysis\nBased on US National COVID Cohort Collaborative (N3C) Data.”\nThe Lancet HIV 8 (11): 690–700. https://doi.org/10.1016/S2352-3018(21)00239-3."
  }
]